<!DOCTYPE html>
<!-- saved from url=(0056)./kersting/kersting.html -->
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>?Your Name</title>


    <link href="./../../css/bootstrap.min.css" rel="stylesheet" media="screen">
    <link href="./../../css/jquery.dataTables.min.css" rel="stylesheet" media="screen">
    <link href="./../../css/dataTables.bootstrap4.min.css" rel="stylesheet" media="screen">

    <link href="./../../css/bootstrap-social.css" rel="stylesheet" media="screen">

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

    <link href="./../../css/style.css" rel="stylesheet">

    <script src="./../../build/jquery.min.js"></script>




        <script type='text/javascript'>//<![CDATA[
      $(document).ready(function(){
        // Add smooth scrolling to all links
        $("a").on('click', function(event) {

          // Make sure this.hash has a value before overriding default behavior
          if (this.hash !== "") {
            // Prevent default anchor click behavior
            event.preventDefault();

            // Store hash
            var hash = this.hash;

            // Using jQuery's animate() method to add smooth page scroll
            // The optional number (800) specifies the number of milliseconds it takes to scroll to the specified area
            $('html, body').animate({
              scrollTop: $(hash).offset().top
            }, 800, function(){

              // Add hash (#) to URL when done scrolling (default click behavior)
              window.location.hash = hash;
            });
          } // End if
        });
      });
      </script>

      <script src="./../../build/jquery.min.js"></script>
      <!--    <script>
          $(function(){
            $("#includedContent").load("myBib.bib");
          });
        </script>-->


</head>



<body onload="start()" data-spy="scroll" data-target=".navbar" data-offset="65">
 <header>
       <nav class="navbar navbar-expand-md navbar-dark fixed-top bg-dark">

         <a class="navbar-brand" href="#">?Your Name</a>

         <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarCollapse">
           <span class="navbar-toggler-icon"></span>
         </button>

         <div class="collapse navbar-collapse" id="navbarCollapse">

           <ul class="navbar-nav mr-auto">
             <li class="nav-item">
               <a class="nav-link" href="#mission">Mission and Bio</a>
             </li>
             <li class="nav-item">
               <a class="nav-link" href="#publications">Publications</a>
             </li>
             <li class="nav-item">
               <a class="nav-link" href="#activities">Activities</a>
             </li>
             <li class="nav-item">
               <a class="nav-link" href="../../index.html">ML@TUDA</a>
             </li>
             <li class="nav-item">
               <a class="nav-link" href="#legal">Legal Info</a>
             </li>
           </ul>

         </div>
       </nav>
 </header>


<main role="main">

<div class="container" id="banner">
    <br><br><br>
    <img class="img-fluid" src="./../../images/banner.png" style="float: left; margin: 0px 0px 10px 10px;">
      <a href="https://www.tu-darmstadt.de" target="_blank"> <img class="img-fluid" src="./../../images/tud_weblogo.svg" style="margin: 19px 10px 10px 10px;height:75px"></a>
    </p>
</div>
 <div class="clearfix"></div>
<div class="container" id="mission">

<div class="row">
        <div class="col-md-2">

        <img src="profile-thumbnail.jpg" class=" rounded-circle imgme" style="height:120px">
     </div>

      <div class="col-md-6">
        <div id="htname">Name </div>
        <div id="htem">Computer Science Department, TU Darmstadt, Altes Hauptgebäude, Room ?Number, Hochschulstrasse 1, 64289 Darmstadt, Germany
          <br><img src="./../../images/phone.png" style="height:15px"> ?Phone Number <img src="./../../images/mail.jpg" style="height:15px"> ?Email Adress</div>
      </div>
      <div class="col-md-4">
        <img id="logo" class="imgsm" src="./../../images/FB20TUDAlogo.png" style="text-align: right; margin: 10px 5px 5px 3px;height:55px"></a>
      </div>
</div>

<br> <b> Meetings by appointment, general consultation: ?when</b>

<br><br>

<b>Mission.</b> My research interests are ....
<br><br>


<b>Bio.</b> Name is a ?PostDoc / ? PhD student at the
<a href="https://www.informatik.tu-darmstadt.de/en/department/" target="_blank">Computer Science Department</a>
of the <a href="https://www.tu-darmstadt.de" target="_blank">TU Darmstadt University</a>, Germany.
After receiving ?her/?his ?Ph.D/?M.Sc. from the University of ... in 20.., (s)he was with ... .<br><br>


<b>Timeline.</b>
<TABLE BORDER="0" width="100%" id = "awards">
<col width="120px">
<TR>
<TD><span class="t2when">   2017 - now:</span></TD>
<TD><span class="t2where"> Ph.D. student at the Machine Learning Lab, <a href="https://www.informatik.tu-darmstadt.de/en/department/" target="_blank">CS Department, TU Darmstadt</a>, Germany</span></TD>
</TR>
<TR>
<TD><span class="t2when">  201? - 2017:</span></TD>
<TD><span class="t2where">Studying computer science at the ... </span></TD>
</TR>
</TABLE>

<br><br>

<b>Awards.</b> If you have some Awards

<div id="timeline" style="margin-top:1px;margin-bottom:20px;height:400px;width:auto; overflow-y:scroll;list-style-position: outside">



<div class="timelineitem">

<div class="tdate">2017</div>

<div class="ttitle"><span class="t2where"> Turing Award.</div>

</div>


<div class="timelineitem">

<div class="tdate">2017</div>

<div class="ttitle"><span class="t2where">Nobel Prize</div>

</div>



</div>


<hr class="soft">


<div class="container" id="publications">

<h2>Publications</h2>

<br>

Please change you DBLP etc. links correspondingly. Also please put your bibtext entries correspondinly.


<div class="hht" style="font-size: 18px"> Publications can be found at <a href="https://dblp.uni-trier.de/pers/hy/k/Kersting:Kristian.html" target="_blank">DBLP</a>, <a href="https://www.semanticscholar.org/author/Kristian-Kersting/1746871" target="_blank">SemanticScholar</a>, and <a href="http://scholar.google.com/citations?user=QY-earAAAAAJ&amp;hl=en" target="_blank">GOOGLE Scholar Citations</a>.
</div>

<br>

 <noscript>
      <!-- bibtex source hidden by default, show it if JS disabled -->
      <style>
      #bibtex { display: block;}
      </style>
</noscript>

<table id="pubTable" class="table style="width:100%"" ></table>

<br>

<pro id="bibtex" style="display:none;">

  @incollection{vergari2018tpm,
    Anote = {./../../images/vergari2018tpm.png},
    Author = {Antonio Vergari and Alejandro Molina and Robert Peharz and Zoubin Ghahramani and Kristian Kersting and Isabel Valera},
    Booktitle = {Working Notes of the ICML 2018 Workshop on Tractable Probabilistic Models (TPM)},
    Note = {Making sense of a dataset in an automatic, unsupervised fashion is a challenging problem in statistics
      and AI. For example, classical approaches for density estimation do not naturally deal with heterogeneous
      statistical data types, do not automatically select suitable parametric forms for the likelihood models, and,
      in basic formulations, are sensitive to corrupted data and outliers. To overcome this, we propose to extend
      density estimation to Automatic Bayesian Density Analysis (ABDA), casting both data modeling and selection of adequate likelihood models (statistical data types) into a joint inference problem. Specifically, we advocate a hierarchically structured mixture model, which explicitly incorporates arbitrarily rich collections of likelihood models and corresponding latent selection variables, and captures variable interactions by a latent hierarchical structure obtained from data-type agnostic structure learning. To account for prediction uncertainty, selection of parametric likelihood models and
      statistical data types, we employ Bayesian inference over a model formulated as a sum-product network,
      naturally providing the aforementioned expressiveness and flexibility, while facilitating exact and tractable
      inference.},
      Keywords = {Sum-Product Networks, Deep Learning, Automatic Statistician, Density Estimation, Density Analysis, AutoML},
    Pages = {},
    Title = {Automatic Bayesian Density Analysis},
    Url = {},
    Year = {2018}}


  @incollection{sommer2018tpm,
    Anote = {./../../images/sommer2018tpm.png},
    Author = {Lukas Sommer and Julian Oppermann and Alejandro Molina and Carsten Binnig and Kristian Kersting and Andreas Koch},
    Booktitle = {Working Notes of the ICML 2018 Workshop on Tractable Probabilistic Models (TPM)},
    Note = {FPGAs have recently proven to be ideally suited for the implementation of efficient accelerators for a wide
      range of machine learning tasks. Here, we consider probabilistic models, specifically, (Mixed) Sum-Product Networks
      (SPN), a deep architecture that can provide tractable inference for multivariate distributions over mixed data-sources.
We show how to construct an FPGA-based accelerator for the inference in (mixed) SPNs. Starting from an input description of
the network, we develop a fully automatic synthesis flow to a custom FPGA-accelerator. The synthesized accelerator and its
interface to the external memory on the FPGA are fully pipelined, and computations are conducted using double-precision floating
point arithmetic.  To the best of our knowledge, this work is the first approach to offload the (mixed) SPN inference problem to FPGA-based
accelerators. Our evaluation shows that the SPN inference problem can profit from offloading to an FPGA accelerator.},
      Keywords = {FPGA, Sum-Product Networks, Deep Learning, Hardware implementation },
    Pages = {},
    Title = {Automatic Synthesis of FPGA-based Accelerators for the Sum-Product Network Inference Problem},
    Url = {},
    Year = {2018}}

  @inproceedings{kordjamshidi2018ijcaiecai_systemsai,
    Anote = {./../../images/kordjamshidi2018ijcaiecai_systemsai.png},
    Author = {Parisa Kordjamshidi and Dan Roth and Kristian Kersting},
    Booktitle = {Proceedings of the 27th International Joint Conference on Artificial Intelligence and the 23rd European Conference on Artificial Intelligence (IJCAI-ECAI)},
    Note = {Data-driven approaches are becoming dominant problem-solving techniques in many areas of
      research and industry. Unfortunately, current technologies do not make it easy to use them for
      application experts that are not fluent in machine learning technologies.
We review key efforts made by various AI communities to provide languages for high-level abstractions
over learning and reasoning techniques needed for designing complex AI systems. We classify the existing
frameworks based on the techniques as well as the data and knowledge representations they use, provide a
comparative study of the way they address the challenges of programming real-world applications, and highlight
some shortcomings and future directions.  },
      Keywords = {Systems AI, Learning based programming, Statistical relational learning, Complex AI systems, Survey},
    Pages = {},
    Title = {Systems AI: A Declarative Learning Based Programming Perspective},
    Url = {./../../papers/kordjamshidi2018ijcaiecai_systemsai.pdf},
    Year = {2018}}

  @incollection{gries2018esof,
    Anote = {./../../images/esof2018.png},
    Author = {Lucas Gries and Edith Luschmann and Laurent Gautier and Lars Koppers and Kristian Kersting and Jörg Rahnenführer and Julia Serong and Holger Wormer },
    Booktitle = {Poster Proceedings of the Euroscience Open Forum (ESOF) Conference},
    Note = {It’s surprising how often scientists are surprised by their findings: This observation was
described in Nature by the Polish Researcher Michael Jasienski more than ten years ago. In
2015 Christiaan Vinkers from the Netherlands compared the use of and semantic valence
between positive and negative words of PubMed abstracts from 1974 to 2014 and found signs
for increasing sensationalism. However, systematic tools to evaluate "sensationalism" and
similar aspects concerning the wording in pieces of science communication are still missing.
Furthermore, comparisons between European countries are difficult, as systematic access
(e.g., to scientific press releases) differs from one country to another.
Our interdisciplinary group will present approaches to compare press releases from different
disciplines, institutions and countries by means of database examples from Germany
(providing an already analysed database extracted from the “Informationsdienst
Wissenschaft” of more than 300 000 press releases from about 1000 institutions) and France
(using a newly constructed corpus from more than 100 institutions). Considering different
perspectives from communication science, informatics and statistics we discuss to what extent
indexes based on a set of "sensational words" and other forms of linguistic analysis may deliver
hints for unethical exaggerations in different fields of science communication.},
      Keywords = {Poster, Sensationalism, Communication Science, Linguistic Analysis},
    Pages = {},
    Title = {Science, Surprise and Sensation in Science Communication: Towards a Concept for Cross-Country Comparisons of Press Releases from Research Institutions},
    Url = {./../../papers/gries2018ESOF_poster.pdf},
    Year = {2018}}


    @incollection{bauer2018ecda,
      Anote = {./../../images/ecda2018.png},
      Author = {Nadja Bauer and Malte Jastrow and Daniel Horn and Lukas Stankiewicz and Kristian Kersting and Jochen Deuse and Claus Weihs},
      Booktitle = {Abstract Proceedings of the European Conference on Data Analysis (ECDA)},
      Note = {The advent of industry 4.0 and the availability of large data storage systems lead to an increasing demand
for specially educated data-oriented professionals in industrial production. The education of such specialists
is supposed to combine elements from the three fields of engineering, data analysis and data administration.
Data administration skills are demanded to handle big data in diverse storage structures in data bases.
In order to extract knowledge from the stored data the proficient handling of data analysis tools - especially
machine learning - is essential. Finally, industrial domain knowledge is important to identify possible applications
for machine learning algorithms (and to interpret the results). However, to the best knowledge of the authors, an education program incorporating
elements of all three fields has not yet been established (in Germany).

In the context of the newly acquired project "Industrial Data Science" (InDaS) we aim to develop a qualification
concept for machine learning in industrial production targeted at two different groups. On the one hand advanced
students from any of the three fields mentioned above and on the other hand experienced professionals working in
industrial production. For the first group a one term lecture is going to be designed while coping with different
levels of knowledge in the inhomogeneous audience. It will be followed by a seminar with focus on use cases delivered
by partners from industrial production. Separately a workshop concept for the second target group will be developed
taking into account the strong domain knowledge of the participants.

The contents of the qualification concept should be selected according to the needs of industrial companies.
Therefore a survey was created to inquire the use and potentials of machine learning and the requirements for future
employees in industrial production. The evaluation of the survey and the resulting conclusions affecting the qualification
concept are going to be presented in this talk.},
        Keywords = {Abstract, machine learning, engineering, production, qualification concept},
      Pages = {},
      Title = {Industrial Data Science: developing a qualification concept for machine learning in industrial production},
      Url = {},
      Year = {2018}}


  @inproceedings{kolb2018ijcaiecai_xadds,
    Anote = {./../../images/kolb2018ijcaiecai_xadds.png},
    Author = {Samuel Kolb and Martin Mladenov and Scott Sanner and Vaishak Belle and Kristian Kersting},
    Booktitle = {Proceedings of the 27th International Joint Conference on Artificial Intelligence and the 23rd European Conference on Artificial Intelligence (IJCAI-ECAI)},
    Note = {In recent years, weighted model counting (WMC) has become the de facto work-horse for logically-structured inference tasks.
      Problems ranging over probabilistic inference, planning, game theory, and numerical optimization have been tackled successfully using WMC technology.
      Weighted model integration (WMI) is a recent formalism generalizing WMC to the integration of functions over models of mixed discrete-continuous theories.
      WMI has already shown tremendous promise for solving inference problems in graphical models and probabilistic programs.
      Yet, state of the art tools for WMI are nowhere as versatile as those for WMC--solvers are generally limited either by the range of amenable theories, or in terms of performance.
      Moreover, the data structures to represent, compose, and transform formulas over mixed theories have not reached the efficiency of their Boolean counterparts.
      Arguably, the latter is one of the prime reasons why WMC is so widely applicable.
      To address both limitations, we propose the use of extended algebraic decision diagrams (XADDs) as a compilation language for WMI.
      Aside from tackling typical WMI problems XADDs also enable partial WMI yielding parametrized solutions.
      To overcome the main roadblock of XADDs---the computational cost of variable elimination---we formulate a novel and powerful exact symbolic dynamic programming (SDP)
      algorithm that unlike its predecessor is able to effectively cache partial computations and seamlessly handle Boolean, integer-valued, and real variables.
      Our empirical results demonstrate that the improved algorithm can lead to an exponential to linear computational reduction in the best case and that it exceeds or
      matches state-of-the-art WMI solvers in terms of performance.
},
      Keywords = {Variable elimination for WMI, XADDs, SMT, Weighted Model Integration (WMI), Probabilsitic Inference, Smybolic Integration},
    Pages = {},
    Title = {An Efficient Symbolic Partial Integration Operator for Probabilistic Inference},
    Url = {./../../papers/kolb2018ijcaiecai_xadds.pdf},
    Year = {2018}}


    @inproceedings{luedtke2018ijcaiecai_liftedFiltering,
      Anote = {./../../images/liftedFiltering.png},
      Author = {Stefan Lüdtke and Max Schröder and Sebastian Bader and Kristian Kersting and Thomas Kirste},
      Booktitle = {Proceedings of the 27th International Joint Conference on Artificial Intelligence and the 23rd European Conference on Artificial Intelligence (IJCAI-ECAI)},
      Keywords = {Lifted Inference, Filtering, Planning, Symmetries},
      Note = {We present a model for recursive Bayesian filtering based on lifted multiset states.
        Combining multisets with lifting makes it possible to simultaneously exploit multiple strategies
        for reducing inference complexity when compared to list-based grounded state representations. The
        core idea is to borrow the concept of Maximally Parallel Multiset Rewriting Systems and to enhance
        it by concepts from Rao-Blackwellisation and Lifted Inference, giving a representation of state
        distributions that enables efficient inference. In worlds where the random variables that define
        the system state are exchangeable - where the identity of entities does not matter - it automatically
        uses a representation that abstracts from ordering (achieving an exponential reduction
        in complexity) and it automatically adapts when observations or system dynamics destroy
        exchangeability by breaking symmetry.},
      Title = {Lifted Filtering via Exchangeable Decomposition},
      Url = {https://arxiv.org/pdf/1801.10495.pdf},
      Year = {2018},
      Bdsk-Url-1 = {https://arxiv.org/pdf/1801.10495.pdf}}


    @article{kersting2018ki,
      Anote = {./../../images/ki2018.jpg},
      Author = {Kristian Kersting and Ulrich Meyer},
      Journal = {Künstliche Intelligenz (KI)},
      Keywords = {Artificial Intelligence, Big Data, Algorithmic Challenges, Editorial},
      Note = {Big Data is no fad. The world is growing at an exponential rate, and so is the size of data
        collected across the globe. The data is becoming more meaningful and contextually relevant, breaks
        new ground for machine learning and artificial intelligence (AI), and even moves them from research
        labs to production. That is, the problem has shifted from collecting massive amounts of data to
        understanding it, i.e., turning data into knowledge, conclusions, and actions. This Big AI,
        however, often faces poor scale-up behaviour from algorithms that have been designed based on
        models of computation that are no longer realistic for Big Data. This special issue constitutes
        an attempt to highlight the algorithmic challenges and opportunities but also the social and
        ethical issues of Big Data. Of specific interest and focus have been computation- and
        resource-efficient algorithms when searching through data to find and mine relevant or
        pertinent information.},
      Pages = {3--8},
      Publisher = {Springer},
      Title = {From Big Data to Big Artificial Intelligence? - Algorithmic Challenges and Opportunities of Big Data},
      Url = {https://link.springer.com/content/pdf/10.1007%2Fs13218-017-0523-7.pdf},
      Volume = {32},
      number = {1},
      Year = {2018},
  }

  @misc{teso2018explanatory,
    Anote = {./../../images/teso2018explanatory.png},
    Author = {Stefano Teso and Kristian Kersting},
    Howpublished = {arXiv preprint arXiv:1805.08578; also abstract at the 14th Biannual Conference of the German Society for Cognitive Science (KogWis) 2018},
    Keywords = {trust, explanations, active learning, explantory interactive learning},
    Note = {Although interactive learning puts the user into the loop, the learner
      remains mostly a black box for the user. Understanding the reasons behind queries
      and predictions is important when assessing how the learner works and, in turn, trust.
      Consequently, we propose the novel framework of explanatory interactive learning:
      in each step, the learner explains its interactive query to the user, and she
      queries of any active classifier for visualizing explanations of the corresponding
      predictions. We demonstrate that this can boost the predictive and explanatory powers of and the trust into the learned model, using text (e.g. SVMs) and image classification (e.g. neural networks) experiments as well as a user study.},
    Title = {"Why Should I Trust Interactive Learners?" Explaining Interactive Queries of Classifiers to Users},
    Url = {https://arxiv.org/abs/1805.08578.pdf},
    Year = {2018},
    Bdsk-Url-1 = {https://arxiv.org/abs/1805.08578.pdf}}


      @misc{peharz2018ratspns,
        Anote = {./../../images/peharz2018ratspns.png},
        Author = {Robert Peharz and Antonio Vergari and Karl Stelzner and Alejandro Molina and Martin Trapp and Kristian Kersting and Zoubin Ghahramani},
        Howpublished = {arXiv preprint arXiv:1806.01910},
        Keywords = {deep learning, probabilistic deep learning, sum product networks, random models, generative model},
        Note = {Probabilistic deep learning currently receives an increased interest, as
          consistent treatment of uncertainty is one of the most important goals in machine
          learning and AI. Most current approaches, however, have severe limitations concerning
          inference. Sum-Product networks (SPNs), although having excellent properties in that regard,
          have so far not been explored as serious deep learning models, likely due to their special
          structural requirements. In this paper, we make a drastic simplification and use a random
          structure which is trained in a "classical deep learning manner" such as automatic
          differentiation, SGD, and GPU support. The resulting models, called RAT-SPNs, yield
          comparable prediction results to deep neural networks, but maintain well-calibrated
          uncertainty estimates which makes them highly robust against missing data. Furthermore,
          they successfully capture uncertainty over their inputs in a convincing manner, yielding
          robust outlier and peculiarity detection.},
        Title = {Probabilistic Deep Learning using Random Sum-Product Networks},
        Url = {https://arxiv.org/abs/1806.01910.pdf},
        Year = {2018},
        Bdsk-Url-1 = {https://arxiv.org/abs/1806.01910.pdf}}



  @misc{schramowski2018neuralfw,
  	Anote = {./../../images/neuralfw.png},
  	Author = {Patrick Schramowski and Christian Bauckhage and Kristian Kersting},
  	Howpublished = {arXiv preprint arXiv:1803.04300},
  	Keywords = {Frank Wolfe, Deep Learning, Neural Networks, Learnign to learn, meta learning},
  	Note = {The move from hand-designed to learned optimizers
in machine learning has been quite successful
for gradient-based and -free optimizers. When
facing a constrained problem, however, maintaining
feasibility typically requires a projection step,
which might be computationally expensive and
not differentiable. We show how the design of
projection-free convex optimization algorithms
can be cast as a learning problem based on FrankWolfe
Networks: recurrent networks implementing
the Frank-Wolfe algorithm aka. conditional
gradients. This allows them to learn to exploit
structure when, e.g., optimizing over rank-1 matrices.
Our LSTM-learned optimizers outperform
hand-designed as well learned but unconstrained
ones. We demonstrate this for training support
vector machines and softmax classifiers},
  	Title = {Neural Conditional Gradients},
  	Url = {https://arxiv.org/pdf/1803.04300.pdf},
  	Year = {2018},
  	Bdsk-Url-1 = {https://arxiv.org/pdf/1803.04300.pdf}}



    @misc{luedtke2018liftedFiltering,
      Anote = {./../../images/liftedFiltering.png},
      Author = {Stefan Lüdtke and Max Schröder and Sebastian Bader and Kristian Kersting and Thomas Kirste},
      Howpublished = {arXiv preprint arXiv:1801.10495},
      Keywords = {Lifted Inference, Filtering, Planning, Symmetries},
      Note = {We present a model for recursive Bayesian filtering based on lifted multiset states.
        Combining multisets with lifting makes it possible to simultaneously exploit multiple strategies
        for reducing inference complexity when compared to list-based grounded state representations. The
        core idea is to borrow the concept of Maximally Parallel Multiset Rewriting Systems and to enhance
        it by concepts from Rao-Blackwellisation and Lifted Inference, giving a representation of state
        distributions that enables efficient inference. In worlds where the random variables that define
        the system state are exchangeable - where the identity of entities does not matter - it automatically
        uses a representation that abstracts from ordering (achieving an exponential reduction
        in complexity) and it automatically adapts when observations or system dynamics destroy
        exchangeability by breaking symmetry.},
      Title = {Lifted Filtering via Exchangeable Decomposition},
      Url = {https://arxiv.org/pdf/1801.10495.pdf},
      Year = {2018},
      Bdsk-Url-1 = {https://arxiv.org/pdf/1801.10495.pdf}}

  @inproceedings{lioutikov2018icra_probGramMove,
    Anote = {./../../images/lioutikov2018icra_probGramMove.png},
    Author = {Rudolf Lioutikov and Guilherme Maeda and Filipe Veiga and Kristian Kersting and Jan Peters},
    Booktitle = {Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)},
    Note = {Movement Primitives are a well studied and
widely applied concept in modern robotics. Composing primitives
out of an existing library, however, has shown to be
a challenging problem. We propose the use of probabilistic
context-free grammars to sequence a series of primitives to
generate complex robot policies from a given library of primitives.
The rule-based nature of formal grammars allows an
intuitive encoding of hierarchically and recursively structured
tasks. This hierarchical concept strongly connects with the way
robot policies can be learned, organized, and re-used. However,
the induction of context-free grammars has proven to be a
complicated and yet unsolved challenge. In this work, we exploit
the physical nature of robot movement primitives to restrict
and efficiently search the grammar space. The grammar is
learned with Markov Chain Monte Carlo optimization over the
posteriors of the grammars given the observations. Restrictions
over operators connecting the search define the corresponding
proposal distributions and, therefore, guide the optimization
additionally. In experiments, we validate our method on a
redundant 7 degree-of-freedom lightweight robotic arm on tasks
that require the generation of complex sequences of motions out
of simple primitives.},
      Keywords = {Robotics, Movement Primitives, Probabilistic Grammar, Bayesian Grammar Induction, Grammar Prior},
    Pages = {},
    Title = {Inducing Probabilistic Context-Free Grammars for the Sequencing of Robot Movement Primitives},
    Url = {./../../papers/lioutikov2018icra_probGramMove.pdf},
    Year = {2018}}

  @inproceedings{binnig2018sysml_deepVizdom,
    Anote = {./../../images/binnig2018sysml_deepVizdom.png},
    Author = {Carsten Binnig and Kristian Kersting and Alejandro Molina and Emanuel Zgraggen},
    Booktitle = {Proceedings of the Inaugural Systems and Machine Learning Conference (SysML)},
    Note = {We make the case for a new generation of interactive data exploration systems
      that seamlessly integrate deep models as first class citizens into the data exploration stack.
      Based on three case studies, we argue that this not only enables users to gain a much
      deeper insights into a broader range of data sets but also
      helps to improvethe performance and quality of existing data exploration systems.},
      Keywords = {Databases, Interactive ML, Automatic Statistician, ML Systems, Deep Learning, Generative Model, Sum Product Networks},
    Pages = {},
    Title = {DeepVizdom: Deep Interactive Data Exploration},
    Url = {./../../papers/binnig2018sysml_deepVizdom.pdf},
    Year = {2018}}

  @inproceedings{molina2018aaai_mspn,
    Anote = {./../../images/molina2018aaai_mspn.png},
    Author = {Alejandro  Molina and Antonio Vergari and Nicola Di Mauro and Floriana Esposito and Siraam Natarajan and Kristian Kersting},
    Booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)},
    Keywords = {Hybrid Domains,Deep Learning, Generative Model, Sum Product Networks, Hirschfeld-Gebelein-Rényi, Automatic Statistician, Structure Learning},
    Note = {While all kinds of mixed data — from personal data, over panel and scientific data,
      to public and commercial data — are collected and stored, building probabilistic graphical models for
      these hybrid domains becomes more difficult. Users spend significant amounts of time in identifying the
      parametric form of the random variables (Gaussian, Poisson, Logit, etc.) involved and learning the mixed
      models. To make this difficult task easier, we propose the first trainable probabilistic deep
      architecture for hybrid domains that features tractable queries. It is based on Sum-Product Networks (SPNs)
      with piecewise polynomial leave distributions together with novel nonparametric decomposition and
      conditioning steps using the Hirschfeld-Gebelein-Rényi Maximum Correlation Coefficient. This relieves
      the user from deciding a-priori the parametric form of the random variables but is still expressive
      enough to effectively approximate any continuous distribution and permits efficient learning and inference.
      Our empirical evidence shows that the architecture, called Mixed SPNs, can indeed capture complex distributions
      across a wide range of hybrid domains.},
    Pages = {},
    Title = {Mixed Sum-Product Networks: A Deep Architecture for Hybrid Domains},
    Url = {./../../papers/molina2018aaai_mspns.pdf},
    Year = {2018}}

    @inproceedings{molina2018aaai_cdn,
      Anote = {./../../images/molina2018aaai_cdn.png},
      Author = {Alejandro  Molina and Alexander Munteanu and Kristian Kersting},
      Booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)},
      Keywords = {Core Sets, eps-subspace embedding, Dependency Networks, Graphical Models, Generative Model, Structure Learning, Gaussian, Poisson},
      Note = {Many applications infer the structure of a probabilistic graphical model from data to elucidate the
        relationships between variables. But how can we train graphical models on a massive data set? In this paper,
        we show how to construct coresets — compressed data sets which can be used as proxy for the original data and
        have provably bounded worst case error — for Gaussian dependency networks (DNs), i.e., cyclic directed graphical
        models over Gaussians, where the parents of each variable are its Markov blanket. Specifically, we prove that
        Gaussian DNs admit coresets of size independent of the size of the data set. Unfortunately, this does not
        extend to DNs over members of the exponential family in general. As we will prove, Poisson DNs do not admit
        small coresets. Despite this worst-case result, we will provide an argument why our coreset construction for
        DNs can still work well in practice on count data. To corroborate our theoretical results, we empirically
        evaluated the resulting Core DNs on real data sets. The results demonstrate significant gains over no or naive
        sub-sampling, even in the case of count data.},
      Pages = {},
      Title = {Core Dependency Networks},  <hr class="soft">


        <div class="container" id="publications">

            <h3>Publications</h3>
            <br>
                    <noscript>
                <style>
                    #bibtex {
                        display: block;
                    }
                </style>
            </noscript>

            <table id="pubTable" class="table" style="width:100%" data-paging="false" data-searching="false" data-info="false">
    	</table>

            <br>

            <pre id="bibtex" style="display:none;">


            </pre>
            <div class="hht" style="font-size: 18px"> Publications can be found at <a
                    href="https://dblp.uni-trier.de/pers/hd/v/Ventola:Fabrizio_G=" target="_blank">DBLP</a>, <a
                    href="https://www.semanticscholar.org/author/Fabrizio-G.-Ventola/8051672"
                    target="_blank">SemanticScholar</a>
            </div>

            <br>

        </div>


<br>


<hr class="soft">

<div class="container" id="activities">

<h2>Activities</h2>

<h3>Conference and Event Organizations:</h3> <br>



<div class="row">

  <div class="col-md-6">
  <div class="t2where">Co-Chair of the ICML 2018 Workshop Program</div>
        <a href="https://2017.icml.cc/Conferences/2018/CallForPapers" target="_blank">
        <img src="./../../images/icml2018.png" style="border-radius:3px;width:240px">
        </a><br>
  </div>


<div class="col-md-6">
<div class="t2where">General Co-Chair of UAI 2018</div>
      <a href="http://auai.org/uai2018/" target="_blank">
      <img src="./../../images/uai2018.png" style="border-radius:3px;width:240px">
      </a><br>
</div>



</div>
<br>

<br>

<div class="t2what">PC Co-Chair of
    <a href="https://mltrain.cc/events/enabling-reproducibility-in-machine-learning-mltrainrml-icml-2018/" target = "_blank">  ICML 2018 Enabling Reproducibility in Machine Learning MLTrain@RML</a> Workshop,
  KDML 2018, DeLBP 2018, <a href="https://mltrain.cc/events/nips-highlights-learn-how-to-code-a-paper-with-state-of-the-art-frameworks/" target = "_blank"> NIPS 2017 Highlights</a> Workshop,
  DS+J 2017, DeLBP 2017, SymInfOpt 2017, BeyondLabeler 2016, StarAI 2014, Buda 2014, SRL 2012, StarAI 2012, CoLISD 2012, CMPL 2011, MLG 2011, StarAI 2010, SRL 2009, MLG 2007, Dagstuhl Seminar 07161</div>

<br>

<h3>Selected Program Committees/Reviewer:</h3><br>
<div class="t2what">NIPS 2018, ... </div>
<br>


<br>



<hr class="soft">

<div class="container" id="talks">



<h2>Invited Talks, Panels and Training</h2>

<h3>Talks and Panels</h3><br>
<br>
<div class="row">
<div class="col-md-3">
  <a href="http://cpaior2017.dei.unipd.it/" target="_blank">
<img src="./../../images/cpaior2017.png" style="border-radius:3px;height:150px;width:230px">
</a>
</div>
<div class="col-md-3">
<div class="t2what">"Relational Quadratic Programming": <span class="t2where">14th International Conference on Integration of Artificial Intelligence and Operations Research Techniques in Constraint Programming (CPAIOR 2017)</span></div>
</div>

<div class="col-md-3">
  <a href="https://www.kuleuven-kulak.be/benelearn/" target="_blank">
<img src="./../../images/benelearn2016.png" style="border-radius:3px;height:150px;width:230px">
</a>
</div>
<div class="col-md-3">
<div class="t2what">"Declarative Data Science Programming": <span class="t2where">25th Annual Machine Learning Conference of Belgium and The Netherlands (BeneLearn 2016)</span></div>
</div>
</div>

</main>


<footer>
    <hr class="soft" id="legal">


    <div class="container" id="sitefooter">

        <a href="https://www.informatik.tu-darmstadt.de/impressum.en.jsp">Legal info / Impressum</a>
        <br><br>

        Technische Universität Darmstadt <br>
        Fachbereich Informatik <br>
        Hochschulstr. 1 <br>
        64289 Darmstadt<br>
        Germany<br><br>


    </div>

</footer>


<!-- place js at end for faster loading -->
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.1/js/bootstrap.bundle.min.js"></script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/datatables/1.10.19/js/jquery.dataTables.min.js"></script>

<script type="text/javascript" src="./../../build/bib-list.js"></script>

<script type="text/javascript">
    $(document).ready(function () {

        $.get('https://ml-research.github.io/references.bib', function (data) {

            function containsName(bib) {
                return bib.toLowerCase().includes("?your name")
            }

            function replaceImagePath(bib) {
                return bib.replace("./images/", "../../images/");
            }

            bibtex = data.split(/(?=@\w+)/u);
            my_bibtex = bibtex.filter(containsName).map(replaceImagePath);
            $("#bibtex").html(my_bibtex.join());
            bibtexify("#bibtex", "pubTable");
        });


    });
</script>

<script>

    var more_bio_shown = false;

    var more_projects_shown = false;

    var more_pubs_shown = false;


    $(document).ready(function () {


        $("#showmorebio").click(function () {

            if (!more_bio_shown) {

                $("#morebio").slideDown('fast', function () {

                    $("#showmorebio").text('hide more bio');

                });

                more_bio_shown = true;

            } else {

                $("#morebio").slideUp('fast', function () {

                    $("#showmorebio").text('show more bio');

                });

                more_bio_shown = false;

            }

        });


        $("#showmoreprojects").click(function () {

            if (!more_projects_shown) {

                $("#moreprojects").slideDown('fast', function () {

                    $("#showmoreprojects").text('hide');

                });

                more_projects_shown = true;

            } else {

                $("#moreprojects").slideUp('fast', function () {

                    $("#showmoreprojects").text('show more projects');

                });

                more_projects_shown = false;

            }

        });


        $("#showmorepubs").click(function () {

            if (!more_pubs_shown) {

                $("#morepubs").slideDown('fast', function () {

                    $("#showmorepubs").text('hide older publications');

                });

                more_pubs_shown = true;

            } else {

                $("#morepubs").slideUp('fast', function () {

                    $("#showmorepubs").text('show older publications');

                });

                more_pubs_shown = false;

            }

        });


    });

</script>

<script>
    $(document).ready(function () {
        $("#nav a").on('click', function (event) {

            // Make sure this.hash has a value before overriding default behavior
            if (this.hash !== "") {

                // Prevent default anchor click behavior
                event.preventDefault();

                // Store hash
                var hash = this.hash;

                // Using jQuery's animate() method to add smooth page scroll
                // The optional number (800) specifies the number of milliseconds it takes to scroll to the specified area
                $('html, body').animate({
                    scrollTop: $(hash).offset().top
                }, 800, function () {

                    // Add hash (#) to URL when done scrolling (default click behavior)
                    window.location.hash = hash;
                });

            } // End if

        });
    });
</script>


</body>
</html>
