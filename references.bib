@misc{brack2024unleashing,
	title        = {Unleashing Creativity: Generalizing Semantic Control for Text-to-Image Diffusion Models},
	author       = {Manuel Brack and Marlon May and Linoy Tsaban and Felix Friedrich and Patrick Schramowski and Apolinaros Passos and Kristian Kersting},
	year         = 2024,
	url          = {https://www.aiml.informatik.tu-darmstadt.de/papers/brack2024unleashing.pdf},
	note         = {The recent surge in popularity of text-to-image diffusion models (DMs) can largely be attributed to the versatile, expressive, and intuitive user interfaces provided through textual prompts. These models enable inexperienced people to explore artistic ventures easily and provide exciting new opportunities to experienced artists. However, the semantic control offered through text prompts alone is limited and rather fragile, and overall lacks the fine granularity necessary for creative applications. The majority of methods addressing this issue are restricted to specific DM architectures, severely limiting the creative workflow instead of generalizing it to arbitrary models. In contrast, we demonstrate that semantic guidance (SEGA) generalizes to any DM architecture. Importantly, SEGA is natively compatible with state-of-the-art diffusion transformers. Our empirical results show strong model-agnostic performance, and we highlight new creative possibilities enabled by SEGA, such as enhanced typographic manipulations. This work underscores SEGA’s potential to provide consistent, high-quality semantic guidance in a rapidly evolving generative model landscape.},
	howpublished = {preprint},
	anote        = {./images/brack2024unleashing.png}
}
@misc{deiseroth2024tfree,
	title        = {T-FREE: Tokenizer-Free Generative LLMs via Sparse Representations for Memory-Efficient Embeddings},
	author       = {Björn Deiseroth and Manuel Brack and Patrick Schramowski and Kristian Kersting and Samuel Weinbach},
	year         = 2024,
	url          = {https://arxiv.org/abs/2406.19223},
	note         = {Tokenizers are crucial for encoding information in Large Language Models, but their development has recently stagnated, and they contain inherent weaknesses. Major limitations include computational overhead, ineffective vocabulary use, and unnecessarily large embedding and head layers. Additionally, their performance is biased towards a reference corpus, leading to reduced effectiveness for underrepresented languages. To remedy these issues, we propose T-FREE, which directly embeds words through sparse activation patterns over character triplets, and does not require a reference corpus. T-FREE inherently exploits morphological similarities and allows for strong compression of embedding layers. In our exhaustive experimental evaluation, we achieve competitive downstream performance with a parameter reduction of more than 85% on these layers. Further, T-FREE shows significant improvements in cross-lingual transfer learning.},
	howpublished = {arXiv preprint arXiv:2406.19223},
	anote        = {./images/deiseroth2024tfree.png}
}
@article{friedrich2024fair,
	title        = {Auditing and Instructing Text-to-Image Generation Models on Fairness},
	author       = {Felix Friedrich and Manuel Brack and Dominik Hintersdorf and Lukas Struppek and Patrick Schramowski and Sasha Luccioni and Kristian Kersting},
	year         = 2024,
	journal      = {AI and Ethics},
	publisher    = {Springer},
	doi          = {https://doi.org/10.1007/s43681-024-00531-5},
	url          = {https://link.springer.com/content/pdf/10.1007/s43681-024-00531-5.pdf},
	note         = {Generative AI models have recently achieved astonishing results in quality and are consequently employed in a fast-growing number of applications. However, since they are highly data-driven, relying on billion-sized datasets randomly scraped from the internet, they also suffer from degenerated and biased human behavior, as we demonstrate. In fact, they may even reinforce such biases. To not only uncover but also combat these undesired effects, we present a novel strategy, called Fair Diffusion, to attenuate biases after the deployment of generative text-to-image models. Specifically, we demonstrate shifting a bias, based on human instructions, in any direction yielding arbitrarily new proportions for, e.g., identity groups. As our empirical evaluation demonstrates, this introduced control enables instructing generative image models on fairness, with no data filtering and additional training required.},
	anote        = {./images/ffriedrich_fair_2023.png},
	keywords     = {Fairness, Text-to-Image Synthesis, Text-Guided Image Generation, Stable Diffusion, AI Ethics}
}
@incollection{struppeknemofmw,
	title        = {Finding NeMo: Localizing Neurons Responsible For Memorization in Diffusion Models},
	author       = {Lukas Struppek and Dominik Hintersdorf and Kristian Kersting and Adam Dziedzic and Franziska Boenisch},
	year         = 2024,
	booktitle    = {Working Notes of the ICML 2024 Workshop on Foundation Models in the Wild},
	pages        = {},
	url          = {https://openreview.net/pdf?id=5wOrSneuwe},
	note         = {Diffusion models (DMs) produce very detailed and high-quality images, achieved through rigorous training on huge datasets. Unfortunately, this practice raises privacy and intellectual property concerns, as DMs can memorize and later reproduce their potentially sensitive or copyrighted training images at inference time. Prior efforts to prevent this issue are viable when the DM is developed and deployed in a secure and constantly monitored environment. However, they hold the risk of adversaries circumventing the safeguards and are not effective when the DM itself is publicly released. To solve the problem, we introduce NeMo, the first method to localize memorization of individual data samples down to the level of neurons in DMs' cross-attention layers. Through our experiments, we make the intriguing finding that in many cases, single neurons are responsible for memorizing particular training samples. By deactivating these memorization neurons, we avoid replication of training data at inference time, increase the diversity in the generated outputs, and mitigate the leakage of sensitive data.},
	anote        = {./images/hintersdorf2024nemo.png},
	keywords     = {Memorization, Diffusion Models, Stable Diffusion}
}
@misc{solaiman2024evaluatingsocialimpactgenerative,
	title        = {Evaluating the Social Impact of Generative AI Systems in Systems and Society},
	author       = {Irene Solaiman and Zeerak Talat and William Agnew and Lama Ahmad and Dylan Baker and Su Lin Blodgett and Canyu Chen and Hal Daumé III au2 and Jesse Dodge and Isabella Duan and Ellie Evans and Felix Friedrich and Avijit Ghosh and Usman Gohar and Sara Hooker and Yacine Jernite and Ria Kalluri and Alberto Lusoli and Alina Leidinger and Michelle Lin and Xiuzhu Lin and Sasha Luccioni and Jennifer Mickel and Margaret Mitchell and Jessica Newman and Anaelia Ovalle and Marie-Therese Png and Shubham Singh and Andrew Strait and Lukas Struppek and Arjun Subramonian},
	year         = 2024,
	url          = {https://arxiv.org/abs/2306.05949},
	note         = {Generative AI systems across modalities, ranging from text (including code), image, audio, and video, have broad social impacts, but there is no official standard for means of evaluating those impacts or for which impacts should be evaluated. In this paper, we present a guide that moves toward a standard approach in evaluating a base generative AI system for any modality in two overarching categories: what can be evaluated in a base system independent of context and what can be evaluated in a societal context. Importantly, this refers to base systems that have no predetermined application or deployment context, including a model itself, as well as system components, such as training data. Our framework for a base system defines seven categories of social impact: bias, stereotypes, and representational harms; cultural values and sensitive content; disparate performance; privacy and data protection; financial costs; environmental costs; and data and content moderation labor costs. Suggested methods for evaluation apply to listed generative modalities and analyses of the limitations of existing evaluations serve as a starting point for necessary investment in future evaluations. We offer five overarching categories for what can be evaluated in a broader societal context, each with its own subcategories: trustworthiness and autonomy; inequality, marginalization, and violence; concentration of authority; labor and creativity; and ecosystem and environment. Each subcategory includes recommendations for mitigating harm.},
	anote        = {./images/defending_with_backdoors.png},
	howpublished = {arXiv preprint arXiv:2306.05949 and to appear in Hacker, Engel, Hammer, Mittelstadt (eds), Oxford Handbook on the Foundations and Regulation of Generative AI. Oxford University Press.},
	keywords     = {Generative AI, Social Impact, Ethical AI, Fairness, Accountability, Transparency}
}
@incollection{delfosse2024ocalm,
	title        = {OCALM: Object-Centric Assessment with Language Models},
	author       = {Timo Kaufmann and Jannis Blüml and Antonia Wüst and Quentin Delfosse and Kristian Kersting and Eyke Hüllermeier},
	year         = 2024,
	booktitle    = {Working Notes of the RLC 2024 Workshop on Reinforcement Learning Beyond Rewards},
	url          = {https://rlbrew-workshop.github.io/papers/40_ocalm_object_centric_assessmen.pdf},
	note         = {Properly defining a reward signal to efficiently train a reinforcement learning (RL) agent is a challenging task. Designing balanced objective functions from which a desired behavior can emerge requires expert knowledge, especially for complex environments. Learning rewards from human feedback or using large language models (LLMs) to directly provide rewards are promising alternatives, allowing non-experts to specify goals for the agent. However, black-box reward models make it difficult to debug the reward. In this work, we propose Object-Centric Assessment with Language Models (OCALM) to derive inherently interpretable reward functions for RL agents from natural language task descriptions. OCALM uses the extensive world-knowledge of LLMs while leveraging the object-centric nature common to many environments to derive reward functions focused on relational concepts, providing RL agents with the ability to derive policies from task descriptions.},
	anote        = {./images/delfosse2024ocalm.png},
	keywords     = {Deep Reinforcement Learning, LLM, Atari, Arcade Games, Reward Modification}
}
@incollection{delfosse2024hackatari,
	title        = {HackAtari: Atari Learning Environments for Robust and Continual Reinforcement Learning},
	author       = {Quentin Delfosse and Jannis Blüml and Bjarne Gregori and Kristian Kersting},
	year         = 2024,
	booktitle    = {Working Notes of the RLC 2024 Worskhop on Interpretable Policies in Reinforcement Learning},
	url          = {https://openreview.net/pdf?id=Th5OOmiHVo},
	note         = {Artificial agents' adaptability to novelty and alignment with intended behavior is crucial for their effective deployment. Reinforcement learning (RL) leverages novelty as a means of exploration, yet agents often struggle to handle novel situations, hindering generalization. To address these issues, we propose HackAtari, a framework introducing controlled novelty to the most common RL benchmark, the Atari Learning Environment. HackAtari allows us to create novel game scenarios (including simplification for curriculum learning), to swap the game elements' colors, as well as to introduce different reward signals for the agent. We demonstrate that current agents trained on the original environments include robustness failures, and evaluate HackAtari's efficacy in enhancing RL agents' robustness and aligning behavior through experiments using C51 and PPO. Overall, HackAtari can be used to improve the robustness of current and future RL algorithms, allowing Neuro-Symbolic RL, curriculum RL, causal RL, as well as LLM-driven RL. Our work underscores the significance of developing interpretable in RL agents.},
	anote        = {./images/delfosse2024hackatari.png},
	crossref     = {https://github.com/k4ntz/HackAtari},
	keywords     = {Deep Reinforcement Learning, Object-centric Deep Learning, Atari, Arcade Games, Novelty, Continual Learning, Robustness}
}
@inproceedings{hintersdorf24defending,
	title        = {Defending Our Privacy With Backdoors},
	author       = {Dominik Hintersdorf and Lukas Struppek and Daniel Neider and Kristian Kersting},
	year         = 2024,
	booktitle    = {Proceedings of the 27th European Conference on Artificial Intelligence (ECAI)},
	url          = {https://arxiv.org/pdf/2310.08320.pdf},
	note         = {The proliferation of large AI models trained on uncurated, often sensitive web-scraped data has raised significant privacy concerns. One of the concerns is that adversaries can extract information about the training data using privacy attacks. Unfortunately, the task of removing specific information from the models without sacrificing performance is not straightforward and has proven to be challenging. We propose a rather easy yet effective defense based on backdoor attacks to remove private information such as names and faces of individuals from vision-language models by fine-tuning them for only a few minutes instead of re-training them from scratch. Specifically, through strategic insertion of backdoors into text encoders, we align the embeddings of sensitive phrases with those of neutral terms-"a person" instead of the person's actual name. For image encoders, we map embeddings of individuals to be removed from the model to a universal, anonymous embedding. Our empirical results demonstrate the effectiveness of our backdoor-based defense on CLIP by assessing its performance using a specialized privacy attack for zero-shot classifiers. Our approach provides not only a new "dual-use" perspective on backdoor attacks, but also presents a promising avenue to enhance the privacy of individuals within models trained on uncurated web-scraped data.},
	anote        = {./images/defending_with_backdoors.png},
	keywords     = {Security, Privacy, Backdoor Attacks, CLIP, Identity Inference Attacks}
}
@inproceedings{czech24representation,
	title        = {Representation Matters for Mastering Chess: Improved Feature Representation in AlphaZero Outperforms Changing to Transformers},
	author       = {Johannes Czech and Jannis Blüml and Kristian Kersting and Hedinn Steingrimsson},
	year         = 2024,
	booktitle    = {Proceedings of the 27th European Conference on Artificial Intelligence (ECAI)},
	url          = {},
	note         = {While transformers have gained recognition as a versatile tool for artificial intelligence(AI), an unexplored challenge arises in the context of chess - a classical AI benchmark. Here, incorporating Vision Transformers (ViTs) into AlphaZero is insufficient for chess mastery, mainly due to ViTs' computational limitations. The attempt to optimize their efficiency by combining MobileNet and NextViT could not outperform AlphaZero. Instead, we propose a practical improvement that involves a simple change in the input representation and value loss functions. As a result, we achieve a significant performance boost of up to 180 Elo points beyond what is currently achievable with AlphaZero in chess and chess variants. In addition to these improvements, our experimental results using the Integrated Gradient technique confirm the effectiveness of the newly introduced features.},
	anote        = {./images/czech24representation.png},
	keywords     = {Chess, Decision Transformer, MCTS, Input Representation}
}
@misc{hintersdorf2024nemo,
	title        = {Finding NeMo: Localizing Neurons Responsible For Memorization in Diffusion Models},
	author       = {Dominik Hintersdorf and Lukas Struppek and Kristian Kersting and Adam Dziedzic and Franziska Boenisch},
	year         = 2024,
	url          = {https://arxiv.org/pdf/2406.02366.pdf},
	note         = {Diffusion models (DMs) produce very detailed and high-quality images. Their power results from extensive training on large amounts of data, usually scraped from the internet without proper attribution or consent from content creators. Unfortunately, this practice raises privacy and intellectual property concerns, as DMs can memorize and later reproduce their potentially sensitive or copyrighted training images at inference time. Prior efforts prevent this issue by either changing the input to the diffusion process, thereby preventing the DM from generating memorized samples during inference, or removing the memorized data from training altogether. While those are viable solutions when the DM is developed and deployed in a secure and constantly monitored environment, they hold the risk of adversaries circumventing the safeguards and are not effective when the DM itself is publicly released. To solve the problem, we introduce NeMo, the first method to localize memorization of individual data samples down to the level of neurons in DMs' cross-attention layers. Through our experiments, we make the intriguing finding that in many cases, single neurons are responsible for memorizing particular training samples. By deactivating these memorization neurons, we can avoid the replication of training data at inference time, increase the diversity in the generated outputs, and mitigate the leakage of private and copyrighted data. In this way, our NeMo contributes to a more responsible deployment of DMs.},
	anote        = {./images/hintersdorf2024nemo.png},
	howpublished = {arXiv preprint arXiv:2406.02366},
	keywords     = {Memorization, Diffusion Models, Stable Diffusion}
