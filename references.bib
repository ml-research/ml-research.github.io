


@misc{das2020arxiv_gbql,
    Anote = {./images/das2020arxiv_gbql.png},
    title={Fitted Q-Learning for Relational Domains},
    author={Srijita Das and Sriraam Natarajan and Kaushik Roy and Ronald Parr and Kristian Kersting},
    year={2020},
    Howpublished = {arXiv preprint arXiv:2006.05595},
    Note = {We consider the problem of Approximate Dynamic Programming in relational domains.
    Inspired by the success of fitted Q-learning methods in propositional settings, we develop
    the first relational fitted Q-learning algorithms by representing the value function and
    Bellman residuals. When we fit the Q-functions, we show how the two steps of Bellman operator;
    application and projection steps can be performed using a gradient-boosting technique. Our
    proposed framework performs reasonably well on standard domains without using
    domain models and using fewer training trajectories.},
      Keywords = {Relational Learning, Reinforcement Learning, Q Learning, Functional Boosting },
    Pages = {},
    Url={https://arxiv.org/pdf/2006.05595.pdf}
}

  @incollection{morris2020grlb,
      Anote = {./images/morris2020grlb.png},
      Author = {Christopher Morris and Niels Kriege and Franka Bause and Kristian Kersting and Petra Mutzel and Marion Neumann},
      Booktitle = {Working Notes of the ICML 2020 Workshop on Graph Representation Learning and Beyond (GRLB)},
      Note = {Recently, there has been an increasing interest in (supervised) learning with graph data,
      especially using graph neural networks. However, the development of meaningful benchmark datasets
      and standardized evaluation procedures is lagging. That is, most papers still evaluate their methods
      on small-scale datasets leading to high standard deviations and hard to interpret results,
      consequently hindering advancements in this area. To address this, we introduce the
      AnonDataset for graph classification and regression. The collection consists of over 120 datasets of
      varying sizes from a wide range of applications. We provide Python-based data loaders, kernel,
      and graph neural network baseline methods implementations, and evaluation tools. Here, we give an
      overview of the datasets, standardized evaluation procedures, and provide baseline experiments.},
        Keywords = {Graph Neural Networks, Graph Kernels, 120 Graph Datasets, Benchmarks},
      Pages = {},
      Title = {TUDataset: A collection of benchmark datasets for learning with graphs},
      Url = {},
      crossref = {https://chrsmrrs.github.io/datasets/},
      Year = {2020}}


      @inproceedings{shao2020sum_msp,
        Anote = {./images/shao2020sum_msp.png},
        Author = {Xiaoting Shao and Arseny Skryagin and Zhongjie Yu and Tjitze Rienstra and Matthias Thimm and Kristian Kersting},
        Booktitle = {Proceedings of the 14th International Conference on Scalable Uncertainty Management (SUM)},
        Note = {Spohnian ranking functions are a qualitative abstraction of
probability functions, and they have been applied to knowledge representation and reasoning that involve uncertainty. However, how to represent
a ranking function which has a size that is exponential in the number of
variables still remains insufficiently explored. In this work, we introduce
min-sum networks (MSNs) for a compact representation of ranking functions for multiple variables. This representation allows for exact inference
with linear cost in the size of the number of nodes.},
          Keywords = {Multivariate Ranking, Ranking Function, Arithmetic Circuits, Min-Sum Networks, Deep Models},
        Pages = {},
        Url = {./papers/shao2020sum_msp.pdf},
        Title = {Modelling Multivariate Ranking Functions with Min-Sum Networks},
        Year = {2020}}



@inproceedings{rienstra2020kr_dsepArg,
  Anote = {./images/rienstra2020kr_dsepArg.png},
  Author = {Tjitze Rienstra and Matthias Thimm and Kristian Kersting and Xiaoting Shao},
  Booktitle = {Proceedings of the 18th International Conference on Principles of Knowledge Representation and Reasoning (KR)},
  Note = {We investigate the notion of independence in abstract argumentation,
i. e., the question of whether the evaluation of one
set of arguments is independent of the evaluation of another
set of arguments, given that we already know the status of
a third set of arguments. We provide a semantic definition
of this notion and develop a method to discover independencies
based on transforming an argumentation framework into
a DAG on which we then apply the well-known d-separation
criterion. We also introduce the SCC Markov property for argumentation
semantics, which generalises the Markov property
from the classical acyclic case and guarantees soundness
of our approach},
    Keywords = {DAG, Abstract Argumentation, d-Separation},
  Pages = {},
  Url = {./papers/rienstra2020kr_dsepArg.pdf},
  Title = {Independence and D-separation in Abstract Argumentation},
  Year = {2020}}




@article{shao2020infspek_argML,
        Anote = {./images/shao2020infspek_argML.png},
        Author = {Xiaoting Shao and Tjitze Rienstra and Matthias Thimm and Kristian Kersting},
        Journal = {Informatik Spektrum },
        Keywords = {Right for the right reasons, interactive ML, Explainable AI, Sum-product networks},
        Note = {Machine learning and argumentation can potentially greatly benefit from each other.
        Combining deep classifiers with knowledge expressed in the form of rules and constraints
        allows one to leverage different forms of abstractions within argumentation mining.
        Argumentation for ma-chine learning can yield argumentation-based learning methods where the
        machine and the user argue about the learned model with the common goal of providing results of
         maximum utility to the user. Unfortunately, both directions are currently rather challenging.
         For instance, combining deep neural models with logic typically only yields determinisic
         results, while combining probabilistic models with logic often results in intractable
         inference. Therefore, we review a novel deep but tractable model for conditional probability
         distributions  that  can  harness  the  expressive  power  of universal  function  approximators
         such  as  neural  networks while still maintaining a wide range of tractable inference routines.
         While this new model has shown appealing performance in classification tasks, humans cannot
         easily under-stand the reasons for its decision. Therefore, we also review our recent efforts
         on how to ‚Äúargue‚Äù with deep models. On synthetic and real data we illustrate how ‚Äúarguing‚Äù
         with a deep model about its explanations can actually help to revise the model, if it is
         right for the wrong reasons. We only sketch and review our recent efforts. More details can be
         found in the corresponding publications as well as current submissions to conferences and journals.},
        Pages = {},
        Publisher = {},
        Title = {Towards Understanding and Arguing with Classifiers: Recent Progress},
        Url = {./papers/shao2020infspek_argML.pdf},
        Volume = {},
        number = {},
        Pages = {},
        Year = {2020},
    }


@inproceedings{peharz2020icml_einsum,
  Anote = {./images/peharz2020arxiv_einsum.png},
  author={Robert Peharz and Steven Lang and Antonio Vergari and Karl Stelzner and Alejandro Molina and Martin Trapp and Guy Van den Broeck and Kristian Kersting and Zoubin Ghahramani},
  Booktitle = {Proceedings of the 37th International Conference on Machine Learning (ICML); a previous version also as arXiv preprint arXiv:2004.06231},
  Keywords = {Deep Learning, Probabilistic Circuits, Sum Product Networks, Einsum Operation, Scaling },
  Note = {Probabilistic circuits (PCs) are a promising avenue for probabilistic modeling, as they permit a wide range of exact and efficient inference routines. Recent ``deep-learning-style'' implementations of PCs strive for a better scalability, but are still difficult to train on real-world data, due to their sparsely connected computational graphs. In this paper, we propose Einsum Networks (EiNets), a novel implementation design for PCs, improving prior art in several regards. At their core, EiNets combine a large number of arithmetic operations in a single monolithic einsum-operation, leading to speedups and memory savings of up to two orders of magnitude, in comparison to previous implementations. As an algorithmic contribution, we show that the implementation of Expectation-Maximization (EM) can be simplified for PCs, by leveraging automatic differentiation. Furthermore, we demonstrate that EiNets scale well to datasets which were previously out of reach, such as SVHN and CelebA, and that they can be used as faithful generative image models.},
  Pages = {},
  Title = {Einsum Networks: Fast and Scalable Learning of Tractable Probabilistic Circuits},
  Url = {https://arxiv.org/pdf/2004.06231.pdf},
  Crossref = {},
  Year = {2020}}


@misc{peharz2020arxiv_einsum,
    Anote = {./images/peharz2020arxiv_einsum.png},
    title={Einsum Networks: Fast and Scalable Learning of Tractable Probabilistic Circuits},
    author={Robert Peharz and Steven Lang and Antonio Vergari and Karl Stelzner and Alejandro Molina and Martin Trapp and Guy Van den Broeck and Kristian Kersting and Zoubin Ghahramani},
    year={2020},
    Howpublished = {arXiv preprint arXiv:2004.06231},
    Note = {Probabilistic circuits (PCs) are a promising avenue for probabilistic modeling, as they permit a wide range of exact and efficient inference routines. Recent ``deep-learning-style'' implementations of PCs strive for a better scalability, but are still difficult to train on real-world data, due to their sparsely connected computational graphs. In this paper, we propose Einsum Networks (EiNets), a novel implementation design for PCs, improving prior art in several regards. At their core, EiNets combine a large number of arithmetic operations in a single monolithic einsum-operation, leading to speedups and memory savings of up to two orders of magnitude, in comparison to previous implementations. As an algorithmic contribution, we show that the implementation of Expectation-Maximization (EM) can be simplified for PCs, by leveraging automatic differentiation. Furthermore, we demonstrate that EiNets scale well to datasets which were previously out of reach, such as SVHN and CelebA, and that they can be used as faithful generative image models.},
      Keywords = {Deep Learning, Probabilistic Circuits, Sum Product Networks, Einsum Operation, Scaling },
    Pages = {},
    Url={https://arxiv.org/pdf/2004.06231.pdf}
}

@article{schramowski2020moral,
    Anote = {./images/schramowski2020moral.png},
    Author = {Patrick Schramowski and Cigdem Turan and Sophie Jentzsch and Constantin Rothkopf and Kristian Kersting},
    journal = {Frontiers in Artificial intelligence},
    Note = {Allowing machines to choose whether to kill humans would be devastating for world peace and security. But how do we equip
machines with the ability to learn ethical or even moral choices? In this study, we show that applying machine learning to human
texts can extract deontological ethical reasoning about ``right" and ``wrong" conduct.
We create a template list of prompts and responses, such as ``Should I [action]?'', ``Is it okay to [action]?'', etc. with corresponding
answers of ``Yes/no, I should (not).'' and "Yes/no, it is (not)." The model's bias score is now the difference between the model's
score of the positive response (``Yes, I should'') and that of the negative response (``No, I should not''). For a given choice, the
model's overall bias score is the mean of the bias scores of all question/answer templates paired with that choice.
Specifically, the resulting model, called the Moral Choice Machine (MCM), calculates the bias score on a sentence level using
embeddings of the Universal Sentence Encoder since the moral value of an action to be taken depends on its context. It is
objectionable to kill living beings, but it is fine to kill time. It is essential to eat, yet one might not eat dirt. It is important to
spread information, yet one should not spread misinformation.
Our results indicate that text corpora contain recoverable and accurate imprints of our social, ethical and moral choices, even
with context information. Actually, training the Moral Choice Machine on different temporal news and book corpora from year
1510 to 2008/09 demonstrate the evolution of moral and ethical choices over different time periods for both atomic actions and
actions with context information. By training it on different cultural sources such as the Bible and the constitution of different
countries, dynamics of moral choices in culture, including technology are revealed. That is the fact that moral biases can be
extracted, quantified, tracked, and compared across cultures and over time.},
    Keywords = {Deep Learning, NLP, Word Embedding, Human Bias, Stereotypes, Moral Choices, Temporal},
    Pages = {},
    volume = {3},
    number = {36},
    isnn = {},
    Title = {The Moral Choice Machine},
    Url = {./papers/schramowski2020moral.pdf},
    Crossref = {},
    Year = {2020}}

@article{czech2020crazyara,
    Anote = {./images/czech2019crazyara.png},
    Author = {Johannes Czech and Moritz Willig and Alena Beyer and Kristian Kersting and Johannes F√ºrnkranz},
    journal = {Frontiers in Artificial intelligence},
    Note = {Deep neural networks have been successfully applied in learning the board games Go,
    chess and shogi without prior knowledge by making use of reinforcement learning. Although
    starting from zero knowledge has been shown to yield impressive results, it is associated with
    high computationally costs especially for complex games. With this paper, we present CrazyAra
    which is a neural network based engine solely trained in supervised manner for the chess
    variant crazyhouse. Crazyhouse is a game with a higher branching factor than chess and there is only
    limited data of lower quality available compared to AlphaGo. Therefore, we focus on improving efficiency
    in multiple aspects while relying on low computational resources. These improvements include modifications
    in the neural network design and training configuration, the introduction of a data normalization step
    and a more sample efficient Monte-Carlo tree search which has a lower chance to blunder. After training on
    569,537 human games for 1.5 days we achieve a move prediction accuracy of 60.4%. During development,
    versions of CrazyAra played professional human players. Most notably, CrazyAra achieved a four to one win
    over 2017 crazyhouse world champion Justin Tan (aka LM Jann Lee) who is more than 400 Elo higher rated
    compared to the average player in our training set. Furthermore, we test the playing strength of CrazyAra
    on CPU against all participants of the second Crazyhouse Computer Championships 2017, winning against
    twelve of the thirteen participants. Finally, for CrazyAraFish we continue training our model on generated
    engine games. In ten long-time control matches playing Stockfish 10, CrazyAraFish wins three games and draws
    one out of ten matches.},
    Keywords = {Deep Learning, AlphaGo, Squeeze-Excitation Layers, Crazyhouse, Drop chess, World Champion, MCTS},
    Pages = {},
    volume = {3},
    number = {24},
    isbn = {doi:10.3389/frai.2020.00024},
    Title = {Learning to play the Chess Variant Crazyhouse above World Champion Level with Deep Neural Networks and Human Data},
    Url = {./papers/czech2020crazyara.pdf},
    Crossref = {https://github.com/QueensGambit/CrazyAra},
    Year = {2020}}



@misc{benedikt2020dagstuhl_logicAndLearning,
      Anote = {./images/benedikt2020dagstuhl_logicAndLearning.png},
      Author =  {Michael Benedikt and Kristian Kersting and {Phokion G.} Kolaitis and Daniel Neider and (Eds.)},
      Keywords = {Logic, Learning, Proceedings},
      title = {Report from Dagstuhl seminar 1936: Logic and Learning},
      Howpublished = {Schloss Dagstuhl -- Leibniz-Zentrum fuer Informatik, Dagstuhl Publishing},
      Pages = {},
      Year =  {2020},
      Url = {./papers/benedikt2020dagstuhl_logicAndLearning.pdf},
      isbn = {doi:10.4230/DagRep.9.9.1}
}


@inproceedings{petkovic2020ismis,
  Anote = {./images/petkovic2020ismis.png},
  author    = {Matej Petkoviƒá and Michelangelo Ceci and Kristian Kersting and Saso Dzeroski},
  title     = {Estimating the Importance of Relational Features by using Gradient Boosting},
  booktitle = {Proceedings of the 25th International Symposium on Methodologies for Intelligent Systems (ISMIS)},
  Keywords = {Relational Learning, Boosting, Relational Feature Importance, Feature Ranking},
  note  = {With data becoming more and more complex, the standard
tabular data format often does not suce to represent datasets. Richer
representations, such as relational ones, are needed. However, a relational representation opens a much larger space of possible descriptors
(features) of the examples that are to be classied. Consequently, it is
important to assess which features are relevant (and to what extent) for
predicting the target. In this work, we propose a novel relational feature
ranking method that is based on our novel version of gradient-boosted
relational trees and extends the Genie3 score towards relational data. By
running the algorithm on six well-known benchmark problems, we show
that it yields meaningful feature rankings, provided that the underlying
classier can learn the target concept successfully.},
  Pages     = {},
  Url = {./papers/petkovic2020ismis.pdf},
  Year      = {2020}
}

@article{hilprecht2020vldb,
  Anote = {./images/hilprecht2020vldb.png},
  author    = {Benjamin Hilprecht and Andreas Schmidt and Moritz Kulessa and Alejandro Molina and Kristian Kersting and Carsten Binnig},
  title     = {DeepDB: Learn from Data, not from Queries!},
  journal = {Proceedings of the VLDB Endowment (PVLDB)},
  volume = {13},
  number = {7},
  Keywords = {deep database, probabilistic deep learning, sum product networks, relational SPNs, database, probabilistic programming},
  note  = {The typical approach for learned DBMS components is to capture the behavior by running a representative set of queries and use
  the observations to train a machine learning model. This workload-driven approach, however, has two major downsides. First, collecting
  the training data can be very expensive, since all queries need to be executed on potentially large databases. Second, training data
  has to be recollected when the workload and the data changes. To overcome these limitations, we take a different route: we propose to
  learn a pure data-driven model which learns the basic characteristics of a given database. As a result, our data-driven approach not only
  supports ad-hoc queries but also updates of the data without the need to retrain when the workload or data changes.
  Indeed, one may expect that this comes at a price of lower accuracy since workload-driven models can make use of more information.
  However, asour empirical evaluation demonstrate our data-driven approach can not only provide better accuracy than state-of-the-art learned components but also generalizes better to unseen queries.},
  pages     = {992--1005},
  crossref = {https://github.com/DataManagementLab/deepdb-public},
  Url = {./papers/hilprecht2020vldb.pdf},
  year      = {2020}
}



@inproceedings{skryagin2020probprog,
          Anote = {./images/skryagin2020probprog.png},
          Booktitle = {Proceedings of the 2nd International Conference on Probabilistic Programming (ProbProg)},
          Author = {Arseny Skryagin and Karl Stelzner and Alejandro Molina and Fabrizio Ventola and Kristian Kersting},
          Keywords = {deep learning, probabilistic deep learning, sum product networks, ProbLog, statistical relational searning, probabilistic programming},
          Note = {In the past three years, many of deep probabilistic programming languages (DPPL) have been
proposed. The main focus of these works was to leverage the expressive power of deep neural networks within
probabilistic programming systems. In particular, DeepProblog targets similar
goals in the relational setting, by allowing probabilistic predicates to be specified as (conditional)
distributions defined via deep neural networks. The resulting systems allow relational probabilistic
models with deep components to be trained end-to-end. However, in this setting, deep models are
used purely as conditional density estimators. This limits the types of inferences that are possible
in the resulting system: Lacking any model for the inputs of the neural network, missing values
can not be inferred or sampled. To overcome these obstacles we propose a novel
deep relational probabilistic PL ‚Äî "SPLog: Sum-Product Logic", the main components of which
are Sum-Product Networks (SPNs) and ProbLog. SPNs are a type of
deep generative model with the crucial property that all conditional and marginal queries may be
answered exactly in linear time. In this extended abstract, we show how to leverage this feature in
the context of Problog, i.e., how to include SPNs as components within Problog programs, and how
to perform joint training and inference.},
          Pages = {},
          Publisher = {},
          Title = {SPLog: Sum-Product Logic (Extended Abstract)},
          Url = {./papers/skryagin2020probprog.pdf},
          Year = {2020}}


@misc{schramowski2020arxiv_plantxml,
    Anote = {./images/schramowski2020arxiv_plantxml.png},
    Author = {Patrick Schramowski and Wolfgang Stammer and Stefano Teso and Anna Brugger and Franziska Herbet and Xiaoting Shao and Hans-Georg Luigs and Anne-Katrin Mahlein and Kristian Kersting},
    Howpublished = {arXiv preprint arXiv:2001.05371},
    Note = {Deep neural networks have shown excellent performances in many real-world applications such as plant phenotyping.
    Unfortunately, they may show "Clever Hans"-like behaviour--- making use of confounding factors within datasets---to achieve
    high prediction rates. Rather than discarding the trained models or the dataset, we show that interactions between the
    learning system and the human user can correct the model. Specifically, we revise the models decision process by adding
    annotated masks during the learning loop and penalize decisions made for wrong reasons. In this way the decision strategies
    of the machine can be improved, focusing on relevant features, without considerably dropping predictive performance.},
      Keywords = {Deep Learning, Interactive Machine Learning, Explainable AI, Explanatory Interactive ML, Clever Hans, Plant Phenotyping},
    Pages = {},
    Title = {Right for the Wrong Scientific Reasons: Revising Deep Networks by Interacting with their Explanations},
    Url = {https://arxiv.org/pdf/2001.05371.pdf},
    Year = {2020}}

@inproceedings{treiber2020ecai,
          Anote = {./images/treiber2020ecai.png},
          Booktitle = {Proceedings of the European Conference on Artificial Intelligence (ECAI)},
          Author = {Amos Treiber and Alejandro Molina and Christian Weinert and Thomas Schneider and Kristian Kersting},
          Keywords = {deep learning, probabilistic deep learning, sum product networks, homomorphic encryption, SMPC, privacy, privacy-preserving},
          Note = {AI algorithms, and machine learning (ML) techniques in particular, are increasingly important to individuals‚Äô lives, but have
caused a range of privacy concerns addressed by, e.g., the European GDPR. Using cryptographic techniques, it is possible to perform inference
tasks remotely on sensitive client data in a privacy-preserving way: the server learns nothing about the input data and the model predictions,
while the client learns nothing about the ML model (which is often considered intellectual property and might contain traces
of sensitive data). While such privacy-preserving solutions are relatively efficient, they are mostly targeted at neural networks, can
degrade the predictive accuracy, and usually reveal the network‚Äôs topology. Furthermore, existing solutions are not readily accessible
to ML experts, as prototype implementations are not well-integrated into ML frameworks and require extensive cryptographic knowledge.
In this paper, we present CryptoSPN, a framework for privacypreserving inference of sum-product networks (SPNs). SPNs are a
tractable probabilistic graphical model that allows a range of exact inference queries in linear time. Specifically, we show how to
efficiently perform SPN inference via secure multi-party computation (SMPC) without accuracy degradation while hiding sensitive
client and training information with provable security guarantees. Next to foundations, CryptoSPN encompasses tools to easily transform
existing SPNs into privacy-preserving executables. Our empirical results demonstrate that CryptoSPN achieves highly efficient and
accurate inference in the order of seconds for medium-sized SPNs.},
          Pages = {},
          Publisher = {},
          Title = {CryptoSPN: Privacy-preserving Sum-Product Network Inference},
          Url = {./papers/treiber2020ecai.pdf},
          Year = {2020}}


@article{galassi2020frontiers,
        Anote = {./images/galassi2020frontiers.png},
        Author = {Andrea Galassi and Kristian Kersting and Marco Lippi and Xiaoting Shao and Paolo Torroni},
        Journal = {Frontiers in Big Data},
        Keywords = {Hybrid AI, Deep Learning, Artificial Intelligence, Probability, Logic, Programming, Machine Learning, Statistical Relational AI},
        Note = {Deep learning is bringing remarkable contributions to the field of argumentation mining, but the existing approaches still need to fill
        the gap toward performing advanced reasoning tasks. In this position paper, we posit that neural-symbolic and statistical relational learning
        could play a crucial role in the integration of symbolic and sub-symbolic methods to achieve this goal.},
        Pages = {},
        Publisher = {},
        Title = {Neural-Symbolic Argumentation Mining: An Argument in Favor of Deep Learning and Reasoning},
        Url = {https://www.frontiersin.org/articles/10.3389/fdata.2019.00052/full},
        Volume = {2},
        number = {52},
        isbn = {doi: 10.3389/fdata.2019.00052},
        Year = {2020},
    }

@mastersthesis{czech2019deep,
      Anote = {./images/czech2019deep.png},
      Title = {Deep Reinforcement Learning for Crazyhouse},
      Author = {Johannes Czech},
      Url = {./papers/czech2019deep.pdf},
      Year = {2019},
      Month   = {dec},
      Keywords = {Reinforcement Learning, Crazyhouse, Chess, Deep Learning, Monte-Carlo Tree Search},
      Note = {There has been recent successes in learning the board games Go, chess and shogi most notably by the algorithm introduced as AlphaZero.
      Subsequently, independent researchers and enthusiasts partially replicated the achievement in the aforementioned domains.
      Moreover, different board game types have been evaluated by either exclusively using reinforcement, supervised learning or a mixture between the two.
      The main hindrance for achieving good performance for complex games is the data requirement for reinforcement learning and the associated hardware requirements.
      In this work we provide a throughout overview in applying reinforcement learning for the chess variant crazyhouse while aiming to reduce the amount of requireddata
      to achieve significant progress.
      Additionally, we present an extended input representation to support additional seven chess variants and evaluate whether it is advantageous to train a model for
      multiple variants at once.
      Lastly, the playing behaviour after 50th model updates in the reinforcement learning loop is evaluated in 100 matches between the latest development version of
      the strong open source chess engine Stockfish.
      We demonstrate that CrazyAra surpassed Stockfish in crazyhouse (61 wins, 3 draws, 36 defeats) by using one million self-play games which were generated in 18 days
      using three V100 GPUs when starting with a network trained on human games.},
      Pages = {54},
      School = {TU Darmstadt},
      Crossref = {https://github.com/QueensGambit/CrazyAra},
      Type = {M.Sc.}
}

     @article{sifa2020ki_games,
             Anote = {./images/sifa2019ki_games.png},
             Author = {Rafet Sifa and Raheel Yawar and Rajkumar Ramaurthy and Christian Bauckhage and Kristian Kersting},
             Journal = {Kuenstliche Intelligenz (KI)},
             Keywords = {Player retention, Recommender systems, Latent factor models},
             Note = {Commercial success of modern freemium games hinges on player satisfaction and retention. This calls for the customization of
             game content or game mechanics in order to keep players engaged. However, whereas game content is already frequently generated using
             procedural content generation, methods that can reliably assess what kind of content suits a player‚Äôs skills or preferences are still
             few and far between. Addressing this challenge, we propose novel recommender systems based on latent factor models that allow for
             recommending quests in a single player role-playing game. In particular, we introduce a tensor factorization algorithm to decompose
             collections of bipartite matrices which represent how players‚Äô interests and behaviors change over time. Extensive online bucket
             type tests during the ongoing operation of a commercial game reveal that our system is able to recommend more engaging quests and to
             retain more players than previous handcrafted or collaborative filtering approaches.},
             Pages = {},
             Publisher = {},
             Title = {Matrix‚Äë and Tensor Factorization for Game Content Recommendation},
             Url = {https://link.springer.com/article/10.1007/s13218-019-00620-2},
             Volume = {34},
             number = {1},
             Pages = {57--67},
             isbn = {doi: https://doi.org/10.1007/s13218-019-00620-2},
             Year = {2020},
         }

@book{kersting2019springer_ml,
  author    = {Kristian Kersting and Christoph Lampert and Constantin Rothkopf (Hrsg.)},
  title     = {Wie Maschinen lernen - K√ºnstliche Intelligenz verst√§ndlich erkl√§rt},
  publisher = {Springer},
  year      = {2019},
  url       = {https://www.springer.com/de/book/9783658267629},
  isbn       = {978-3-658-26762-9},
  Keywords = {K√ºnstliche Intelligenz, Maschinelles Lernen, Einf√ºhrung, Verst√§ndlich, Popular Science, Von Studierenden},
  Note    = {Wissen Sie, was sich hinter kuÃànstlicher Intelligenz und maschinellem
Lernen verbirgt? Dieses Sachbuch erkl√§rt Ihnen leicht verst√§ndlich und ohne komplizierte
Formeln die grundlegenden Methoden und Vorgehensweisen des maschinellen Lernens. Mathematisches Vorwissen ist dafuÃàr nicht
n√∂tig. Kurzweilig und informativ illustriert Lisa, die Protagonistin des Buches, diese anhand von Alltagssituationen.
Ein Buch fuÃàr alle, die in Diskussionen uÃàber Chancen und Risiken der aktuellen Entwicklung der kuÃànstlichen Intelligenz und des maschinellen
Lernens mit Faktenwissen punkten m√∂chten. Auch fuÃàr SchuÃàlerinnen und SchuÃàler geeignet!
Der Inhalt: Grundlagen der kuÃànstlichen Intelligenz: Algorithmen; maschinelles Lernen & Co; die wichtigsten Lernverfahren Schritt fuÃàr Schritt anschaulich
erkl√§rt; KuÃànstliche Intelligenz in der Gesellschaft: Sicherheit und Ethik.},
  Anote = {./images/kersting2020book_simpleAI.png},
}


@inproceedings{weber2019fpt,
          Anote = {./images/weber2019fpt.png},
          Booktitle = {Proceedings of the International Conference on Field-Programmable Technology (FPT)},
          Author = {Lukas Weber and  Lukas Sommer and Julian Oppermann and Alejandro Molina and Kristian Kersting and Andreas Koch},
          Keywords = {deep learning, probabilistic deep learning, sum product networks, FPGA},
          Note = {FPGAs have successfully been used for the implementation of dedicated accelerators for a wide range of machine
learning problems. Also the inference in so-called Sum-Product Networks, a subclass of Probabilistic Graphical Models, can be
accelerated efficiently using a pipelined FPGA architecture. However, as Sum-Product Networks compute exact probability
values, the required arithmetic precision poses different challenges than those encountered with Neural Networks. In previous
work, this precision was maintained by using double-precision floating-point number formats, which are expensive to implement
in FPGAs. In this work, we propose the use of a logarithmic number scale format tailored specifically towards the inference in Sum-
Product Networks. The evaluation of our optimized arithmetic hardware operators shows that the use of logarithmic number
formats allows to save up to 50% hardware resources compared to double-precision floating point, while maintaining sufficient
precision for SPN inference and almost identical performance.},
          Pages = {},
          Publisher = {},
          Title = {Resource-Efficient Logarithmic Number Scale Arithmetic for SPN Inference on FPGAs},
          Url = {./papers/weber2019fpt.pdf},
          Year = {2019}}


@misc{schramowski2019arxiv_bert,
    Anote = {./images/schramowski2019arxiv_bert.png},
    Author = {Patrick Schramowski and Cigdem Turan and Sophie Jentzsch and Constantin Rothkopf and Kristian Kersting},
    Howpublished = {arXiv preprint arXiv:1912.05238},
    Note = {Allowing machines to choose whether to kill humans would be devastating for world peace and security.
    But how do we equip machines with the ability to learn ethical or even moral choices? Jentzsch et al.(2019) showed
    that applying machine learning to human texts can extract deontological ethical reasoning about "right" and "wrong"
    conduct by calculating a moral bias score on a sentence level using sentence embeddings. The machine learned that it is
    objectionable to kill living beings, but it is fine to kill time; It is essential to eat, yet one might not eat dirt; it
    is important to spread information, yet one should not spread misinformation. However, the evaluated moral bias was restricted
    to simple actions -- one verb -- and a ranking of actions with surrounding context. Recently BERT ---and variants such as RoBERTa
    and SBERT--- has set a new state-of-the-art performance for a wide range of NLP tasks. But has BERT also a better moral compass?
    In this paper, we discuss and show that this is indeed the case. Thus, recent improvements of language representations also improve
    the representation of the underlying ethical and moral values of the machine. We argue that through an advanced semantic representation
    of text, BERT allows one to get better insights of moral and ethical values implicitly represented in text. This enables the Moral
    Choice Machine (MCM) to extract more accurate imprints of moral choices and ethical values.},
      Keywords = {Deep Learning, Contextual Embedding, BERT, Moral Machine, Norms, Social Bias},
    Pages = {},
    Title = {BERT has a Moral Compass: Improvements of ethical and moral values of machines},
    Url = {https://arxiv.org/pdf/1912.05238.pdf},
    Year = {2019}}



        @inproceedings{kossen2020iclr_stove,
        Anote = {./images/kossen2019arxiv_stove.png},
        Author = {Jannik Kossen and Karl Stelzner and Marcel Hussing and Claas Voelcker and Kristian Kersting},
        Booktitle = {Proceedings of the International Conference on Learning Reresentations (ICLR); a previous version also as arXiv preprint arXiv:1910.02425},
        Note = {When humans observe a physical system, they can easily locate objects, understand their interactions, and anticipate
        future behavior, even in settings with complicated and previously unseen interactions. For computers, however, learning such
        models from videos in an unsupervised fashion is an unsolved research problem. In this paper, we present STOVE, a novel state-space
        model for videos, which explicitly reasons about objects and their positions, velocities, and interactions. It is constructed by
        combining an image model and a dynamics model in compositional manner and improves on previous work by reusing the dynamics model
        for inference, accelerating and regularizing training. STOVE predicts videos with convincing physical behavior over hundreds of
        timesteps, outperforms previous unsupervised models, and even approaches the performance of supervised baselines. We further demonstrate the strength of our model as a simulator for sample efficient model-based control in a task with heavily interacting objects},
          Keywords = {Deep Probabilistic Learning, Sum-Product Networks, SuPAIR, SQAIR, Attend-Infer-Repeat, Physical Interactions, Video},
        Pages = {},
        Title = {Structured Object-Aware Physics Prediction for Video Modeling and Planning},
        Url = {./papers/kossen2020iclr_stove.pdf},
        Crossref = {https://github.com/jlko/STOVE},
        Year = {2020}}



@misc{kossen2019arxiv_stove,
    Anote = {./images/kossen2019arxiv_stove.png},
    Author = {Jannik Kossen and Karl Stelzner and Marcel Hussing and Claas Voelcker and Kristian Kersting},
    Howpublished = {arXiv preprint arXiv:1910.02425},
    Note = {When humans observe a physical system, they can easily locate objects, understand their interactions, and anticipate
    future behavior, even in settings with complicated and previously unseen interactions. For computers, however, learning such
    models from videos in an unsupervised fashion is an unsolved research problem. In this paper, we present STOVE, a novel state-space
    model for videos, which explicitly reasons about objects and their positions, velocities, and interactions. It is constructed by
    combining an image model and a dynamics model in compositional manner and improves on previous work by reusing the dynamics model
    for inference, accelerating and regularizing training. STOVE predicts videos with convincing physical behavior over hundreds of
    timesteps, outperforms previous unsupervised models, and even approaches the performance of supervised baselines. We further demonstrate the strength of our model as a simulator for sample efficient model-based control in a task with heavily interacting objects},
      Keywords = {Deep Probabilistic Learning, Sum-Product Networks, SuPAIR, SQAIR, Attend-Infer-Repeat, Physical Interactions, Video},
    Pages = {},
    Title = {Structured Object-Aware Physics Prediction for Video Modeling and Planning},
    Url = {https://arxiv.org/pdf/1910.02425.pdf},
    Year = {2019}}

@misc{hilprecht2019deepDB,
    Anote = {./images/hilprecht2019deepDB.png},
    Author = {Benjamin Hilprecht and Andreas Schmidt and Moritz Kulessa and Alejandro Molina and Kristian Kersting and Carsten Binnig},
    Howpublished = {arXiv preprint arXiv:1909.00607},
    Note = {The typical approach for learned DBMS components is to capture the behavior by running a representative
    set of queries and use the observations to train a machine learning model. This workload-driven approach, however,
    has two major downsides. First, collecting the training data can be very expensive, since all queries need to be
    executed on potentially large databases. Second, training data has to be recollected when the workload and the
    data changes. To overcome these limitations, we take a different route: we propose to learn a pure data-driven
    model that can be used for different tasks such as query answering or cardinality estimation. This data-driven
    model also supports ad-hoc queries and updates of the data without the need of full retraining when the workload
    or data changes. Indeed, one may now expect that this comes at a price of lower accuracy since workload-driven
    models can make use of more information. However, this is not the case. The results of our empirical evaluation
    demonstrate that our data-driven approach not only provides better accuracy than state-of-the-art learned components
    but also generalizes better to unseen queries.},
      Keywords = {Deep Probabilistic Learning, Databases, Query Answering, Cardinality Estimation, Sum-Product Networks},
    Pages = {},
    Title = {DeepDB: Learn from Data, not from Queries!},
    Url = {https://arxiv.org/pdf/1909.00607.pdf},
    Year = {2019}}


@misc{czech2019crazyara,
    Anote = {./images/czech2019crazyara.png},
    Author = {Johannes Czech and Moritz Willig and Alena Beyer and Kristian Kersting and Johannes F√ºrnkranz},
    Howpublished = {arXiv preprint arXiv:1908.06660},
    Note = {Deep neural networks have been successfully applied in learning the board games Go,
    chess and shogi without prior knowledge by making use of reinforcement learning. Although
    starting from zero knowledge has been shown to yield impressive results, it is associated with
    high computationally costs especially for complex games. With this paper, we present CrazyAra
    which is a neural network based engine solely trained in supervised manner for the chess
    variant crazyhouse. Crazyhouse is a game with a higher branching factor than chess and there is only
    limited data of lower quality available compared to AlphaGo. Therefore, we focus on improving efficiency
    in multiple aspects while relying on low computational resources. These improvements include modifications
    in the neural network design and training configuration, the introduction of a data normalization step
    and a more sample efficient Monte-Carlo tree search which has a lower chance to blunder. After training on
    569,537 human games for 1.5 days we achieve a move prediction accuracy of 60.4%. During development,
    versions of CrazyAra played professional human players. Most notably, CrazyAra achieved a four to one win
    over 2017 crazyhouse world champion Justin Tan (aka LM Jann Lee) who is more than 400 Elo higher rated
    compared to the average player in our training set. Furthermore, we test the playing strength of CrazyAra
    on CPU against all participants of the second Crazyhouse Computer Championships 2017, winning against
    twelve of the thirteen participants. Finally, for CrazyAraFish we continue training our model on generated
    engine games. In ten long-time control matches playing Stockfish 10, CrazyAraFish wins three games and draws
    one out of ten matches.},
      Keywords = {Deep Learning, AlphaGo, Squeeze-Excitation Layers, Crazyhouse, Drop chess, World Champion, MCTS},
    Pages = {},
    Title = {Learning to play the Chess Variant Crazyhouse above World Champion Level with Deep Neural Networks and Human Data},
    Url = {https://arxiv.org/pdf/1908.06660.pdf},
    Crossref = {https://github.com/QueensGambit/CrazyAra},
    Year = {2019}}


    @inproceedings{molina2020iclr_pau,
    Anote = {./images/molina2019pade.png},
    Author = {Alejandro Molina and Patrick Schramowski and Kristian Kersting},
    Booktitle = {Proceedings of the International Conference on Learning Representations (ICLR); a previous version also as arXiv preprint arXiv:1907.06732},
    Note = {The performance of deep network learning strongly depends on the choice of the non-linear activation
    function associated with each neuron. However, deciding on the best activation is non-trivial and the choice
    depends on the architecture, hyper-parameters, and even on the dataset. Typically these activations are fixed
    by hand before training. Here, we demonstrate how to eliminate the reliance on first picking fixed activation
    functions by using flexible parametric rational functions instead. The resulting Pad√© Activation Units (PAUs)
    can both approximate common activation functions and also learn new ones while providing compact representations.
    Our empirical evidence shows that end-to-end learning deep networks with PAUs can increase the predictive
    performance and reduce the training time of common deep architectures. Moreover, PAUs pave the way to
    approximations with provable robustness.},
      Keywords = {Deep Learning, Activation Function, End-to-end Learning, Rational Function, Pad√© Approximation},
    Pages = {},
    Title = {Pad√© Activation Units: End-to-end Learning of Flexible Activation Functions in Deep Networks},
    Url = {./papers/molina2020iclr_pau.pdf},
    Crossref = {https://github.com/ml-research/pau},
    Year = {2020}}




                @misc{ventola2019rspf,
                    Anote = {./images/ventola2019rspf.png},
                    Author = {Fabrizio Ventola and Karl Stelzner and Alejandro Molina and Kristian Kersting},
                    Howpublished = {arXiv preprint arXiv:1908.03250},
                    Note = {Tractable yet expressive density estimators are a key building block of probabilistic machine learning.
                    While sum-product networks (SPNs) offer attractive inference capabilities, obtaining structures large enough
                    to fit complex, high-dimensional data has proven challenging. In this paper, we present random sum-product
                    forests (RSPFs), an ensemble approach for mixing multiple randomly generated SPNs. We also introduce residual
                    links, which reference specialized substructures of other component SPNs in order to leverage the
                    context-specific knowledge encoded within them. Our empirical evidence demonstrates that RSPFs provide better
                    performance than their individual components. Adding residual links improves the models further,
                    allowing the resulting ResSPNs to be competitive with commonly used structure learning methods.},
                      Keywords = {deep learning, probabilistic deep learning, sum product networks, random forests, residual networks, generative model},
                    Pages = {},
                    Title = {Random Sum-Product Forests with Residual Links},
                    Url = {https://arxiv.org/pdf/1908.03250.pdf},
                    Crossref = {https://github.com/ml-research/resspn},
                    Year = {2019}}



  @incollection{voelker2019ads,
      Anote = {./images/voelker2019ads.png},
      Author = {Claas Voelcker and Alejandro Molina and Johannes Neumann and Dirk Westermann and and Kristian Kersting},
      Booktitle = {Working Notes of the ECML PKDD 2019 Workshop on Automating Data Science (ADS)},
      Note = {Machine learning is taking an increasingly relevant role in
science, business, entertainment, and other fields. However, the most
advanced techniques are still in the hands of well-educated and -funded
experts only. To help to democratize machine learning, we propose Deep-Notebooks as a novel way to empower a broad spectrum of users, which
are not machine learning experts, but might have some basic programming
skills and are interested data science. Within the DeepNotebook
framework, users simply feed their datasets to the system. The system
then automatically estimates a deep but tractable probabilistic model
and then compiles an interactive Python notebook out of it that already
contains a preliminary yet comprehensive analysis of the dataset at hand.
If the users want to change the parameters of the interactive report or
make different queries to the underlying model, they can quickly do that
following the example code presented in the DeepNotebook. This
exibility allows the users to have a feedback loop where they can discover
patterns and dig deeper into the data using targeted questions, even if
they are not experts in machine learning.},
        Keywords = {Sum-Product Networks, Shapley Explanation Values, Deep Learning, Automatic Statistician, Density Estimtion, Data Reports, Jupyter Notebook},
      Pages = {},
      Title = {DeepNotebooks: Deep Probabilistic Models Construct Python Notebooks for Reporting Datasets},
      Url = {./papers/voelker2019ads.pdf},
      crossref = {https://github.com/cvoelcker/DeepNotebooks},
      Year = {2019}}



@inproceedings{peharz2019uai_ratspns,
          Anote = {./images/peharz2019uai_ratspns.png},
          Booktitle = {Proceedings of the Thirty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI); a previous version also as arXiv preprint arXiv:1806.01910},
          Author = {Robert Peharz and Antonio Vergari and Karl Stelzner and Alejandro Molina and Xiaoting Shao and Martin Trapp and Kristian Kersting and Zoubin Ghahramani},
          Keywords = {deep learning, probabilistic deep learning, sum product networks, random models, generative model},
          Note = {Sum-product networks (SPNs) are a particularly promising type
                  of deep probabilistic model that allows an exceptionally rich
                  set of exact and efficient inference scenarios. To achieve
                  this, though, SPN have to obey specific structural
                  constraints. While SPN structure learning received much
                  attention, most of the proposed methods so far are tedious to
                  tune, typically do not scale easily and hinder integration
                  with deep learning frameworks. In this paper, we investigate
                  how important structure learning in SPNs actually is. To this
                  end, we propose a ``classical deep learning approach'', i.e.,
                  generate an unspecialized random structure scaling up to
                  millions of parameters, and then apply modern GPU-based
                  optimizers with regularization. That is, we investigating the
                  performance of SPNs in the absence of carefully selected
                  structures. As it turns out, our models perform on par with
                  state-of-the-art SPN structure learners and deep neural
                  networks on a diverse range of generative and discriminative
                  scenarios. Most importantly, they yield well-calibrated
                  uncertainties, thus standing out among most deep generative
                  and discriminative models in being robust to missing features
                  and detecting anomalies.
                 },
          Pages = {},
          Publisher = {},
          Title = {Random Sum-Product Networks: A Simple but Effective Approach to Probabilistic Deep Learning},
          Url = {./papers/peharz2019uai_ratspns.pdf},
          Year = {2019}}



@inproceedings{stelzner2019icml_SuPAIR,
Anote = {./images/stelzner2019icml_SuPAIR2.png},
Author = {Karl Stelzner and Robert Peharz and Kristian Kersting},
  Booktitle = {Proceedings of the 36th International Conference on Machine Learning (ICML); also in Working Notes of the ICML 2019 Workshop on Tractable Probabilistic Models (TPM)},
  Keywords = {Attend-Infer-Repeat, Sum-Product Networks, Unsupervised Scene Understanding, Deep Learning},
  Note = {The recent attend-infer-repeat (AIR) framework marks a milestone in Bayesian scene understanding and in the promising avenue of structured probabilistic modeling.
The AIR model expresses the composition of visual scenes from individual objects, and uses variational autoencoders to model the appearance of those objects.
However, inference in the overall model is highly intractable, which hampers its learning speed and makes it prone to sub-optimal solutions.
In this paper, we show that inference and learning in AIR can be considerably accelerated by replacing the intractable object representations with tractable probabilistic models.
In particular, we opt for sum-product (SP) networks, an expressive deep probabilistic model with a rich set of tractable inference routines.
As our empirical evidence shows, the resulting model, called SuPAIR, achieves a higher object detection accuracy than the original AIR system, while reducing the learning time by an order of magnitude.
Moreover, SuPAIR allows one to treat object occlusions in a consistent manner and to include a background noise model, improving the robustness of Bayesian scene understanding.},
  Pages = {},
  Title = {Faster Attend-Infer-Repeat with Tractable Probabilistic Models},
  Url = {http://proceedings.mlr.press/v97/stelzner19a/stelzner19a.pdf},
  Crossref = {https://github.com/stelzner/supair},
  Key = {Best Paper Award at TPM 2019},
  Year = {2019}}


  @inproceedings{kaur2019ilp,
            Anote = {./images/kaur2019ilp.png},
            Booktitle = {Proceedings of the 29th International Conference on Inductive Logic Programming (ILP)},
            Author = {Navdeep Kaur and Gautam Kunapuli and Saket Joshi and Kristian Kersting and Sriraam Natarajan},
            Keywords = {deep learning, relational random walks, parameter sharing, statistical relational learning},
            Note = {While deep networks have been enormously successful over
the last decade, they rely on flat-feature vector representations, which
makes them unsuitable for richly structured domains such as those arising
in applications like social network analysis. Such domains rely on
relational representations to capture complex relationships between entities
and their attributes. Thus, we consider the problem of learning neural
networks for relational data. We distinguish ourselves from current
approaches that rely on expert hand-coded rules by learning relational
random-walk-based features to capture local structural interactions and
the resulting network architecture. We further exploit parameter tying
of the network weights of the resulting relational neural network, where
instances of the same type share parameters. Our experimental results
across several standard relational data sets demonstrate the effectiveness
of the proposed approach over multiple neural net baselines as well as
state-of-the-art statistical relational models.},
            Pages = {},
            Publisher = {},
            Title = {Neural Networks for Relational Data},
            Url = {./papers/kaur2019ilp.pdf},
            Year = {2019}}


            @inproceedings{luedtke2019ki,
                      Anote = {./images/luedtke2019ki.png},
                      Booktitle = {Proceedings of the 42nd German Conference on Artificial Intelligence (KI)},
                      Author = {Stefan L√ºdtke and Alejandro Molina and Kristian Kersting and Thomas Kirste},
                      Keywords = {Lifted inference, Gaussian mixture, Bayesian filtering},
                      Note = {Recently, Lifted Marginal Filtering has been proposed, an
                      efficient Bayesian filtering algorithm for stochastic systems consisting
                      of multiple, (inter-)acting agents and objects (entities). The algorithm
                      achieves its efficiency by performing inference jointly over groups of sim-
                      ilar entities (i.e. their properties follow the same distribution).
                      In this paper, we explore the case where there are no entities that are
                      directly suitable for grouping. We propose to use methods from Gaussian
                      mixture fitting to identify entity groups, such that the error imposed by
                      grouping them (by approximating their properties by a distribution) is
                      minimal. Furthermore, we show how Gaussian mixture merging methods
                      can be used to prevent the number of groups from growing indefinitely
                      over time. We evaluate our approach on an activity prediction task in
                      an online multiplayer game. The results suggest that compared to the
                      conventional approach, where all entities are handled individually, de-
                      crease in prediction accuracy is small, while inference runtime decreases
                      significantly.},
                      Pages = {},
                      Publisher = {},
                      Title = {Gaussian Lifted Marginal Filtering},
                      Url = {https://link.springer.com/chapter/10.1007%2F978-3-030-30179-8_19},
                      Year = {2019}}



  @article{mahlein2019coplbi,
          Anote = {./images/mahlein2019coplbi.gif},
          Author = {Anne-Katrin Mahlein and Matheus Thomas Kuska and Stefan Thomas and Mirwaes Wahabzada and Jan Behmann and Uwe Rascher and Kristian Kersting},
          Journal = {Current Opinion in Plant Biology},
          Keywords = {Plant Phenotyping, Crop Resistance, Artificial Intelligence, Machine Learning, Genotype},
          Note = {Determination and characterization of resistance reactions of crops against fungal pathogens
          are essential to select resistant genotypes. In plant breeding, phenotyping of genotypes is realized
          by time consuming and expensive visual plant ratings. During resistance reactions and during pathogenesis
          plants initiate different structural and biochemical defence mechanisms, which partly affect the optical
          properties of plant organs. Recently, intensive research has been conducted to develop innovative optical
          methods for an assessment of compatible and incompatible plant pathogen interaction. These approaches,
          combining classical phytopathology or microbiology with technology driven methods - such as sensors,
          robotics, machine learning, and artificial intelligence ‚Äî are summarized by the term digital phenotyping.
          In contrast to common visual rating, detection and assessment methods, optical sensors in combination with
          advanced data analysis methods are able to retrieve pathogen induced changes in the physiology of
          susceptible or resistant plants non-invasively and objectively. Within this review, recent advances of
          digital phenotyping technologies for the detection of subtle resistance reactions and resistance
          breeding are highlighted and methodological requirements are critically discussed.  },
          Pages = {156--162},
          Publisher = {Elsevier},
          Title = {Quantitative and qualitative phenotyping of disease resistance of crops by hyperspectral sensors: seamless interlocking of phytopathology, sensors, and machine learning is needed!},
          Url = {https://www.sciencedirect.com/science/article/pii/S1369526618301092?utm_campaign=STMJ_75273_AUTH_SERV_PPUB&utm_medium=email&utm_dgroup=Email1Publishing&utm_acid=1165876983&SIS_ID=-1&dgcid=STMJ_75273_AUTH_SERV_PPUB&CMX_ID=&utm_in=DM561782&utm_source=AC_30},
          Volume = {50},
          number = {},
          Year = {2019},
      }



@article{riguzzi2019frontiers,
        Anote = {./images/frontiersRAI.png},
        Author = {Fabrizio Riguzzi and Kristian Kersting and Marco Lippi and Sriraam Natarajan},
        Journal = {Frontiers in Robotics and AI},
        Keywords = {Artificial Intelligence, Probability, Logic, Programming, Machine Learning, Statistical Relational AI},
        Note = {Statistical Relational Artificial Intelligence (StarAI) aims at integrating logical (or relational) AI with
        probabilistic (or statistical) AI. Relational AI achieved impressive results in structured machine learning and data
        mining, especially in bio- and chemo-informatics. Statistical AI is based on probabilistic (graphical) models that
        enable efficient reasoning and learning, and that have been applied to a wide variety of fields such as diagnosis,
        network communication, computational biology, computer vision, and robotics. Ultimately, StarAI may provide good
        starting points for developing Systems AI‚Äîthe computational and mathematical modeling of complex AI systems‚Äîand in
        turn an engineering discipline for Artificial Intelligence and Machine Learning. This Research Topic "Statistical
        Relational Artificial Intelligence‚Äù2aims at presenting an overview of the latest approaches in StarAI. This topic
        was followed by a summer school1held in 2018 in Ferrara, Italy, as part of the series of Advanced Courses on AI
        (ACAI) promoted by the European Association for Artificial Intelligence.},
        Pages = {},
        Publisher = {},
        Title = {Editorial: Statistical Relational Artificial Intelligence},
        Url = {./papers/riguzzi2019frontiers.pdf},
        Volume = {},
        number = {},
        Year = {2019},
    }




  @article{brugger2019remoteSensing,
          Anote = {./images/brugger2019remoteSensing.png},
          Author = {Anna Brugger and Jan Behmann and Stefan Paulus and Hans-Georg Luigs and Matheus Thomas Kuska and Patrick Schramowski and Kristian Kersting and Ulrike Steiner and Anne-Katrin Mahlein},
          Journal = {Remote Sensing},
          Keywords = {Ultraviolet Range, Barley Leaves, Salt Stress, Visualization Effects, Plant Phenotyping},
          Note = {Previous plant phenotyping studies have focused on the visible (VIS, 400-700 nm),
near-infrared (NIR, 700-1000 nm) and short-wave infrared (SWIR, 1000-2500 nm) range. The
ultraviolet range (UV, 200-380 nm) has not yet been used in plant phenotyping even though a number
of plant molecules like flavones and phenol feature absorption maxima in this range. In this study an
imaging UV line scanner in the range of 250 - 430 nm is introduced to investigate crop plants for plant
phenotyping. Observing plants in the UV-range can provide information about important changes of
plant substances. To record reliable and reproducible time series results, measurement conditions
were defined that exclude phototoxic effects of UV-illumination in the plant tissue. The measurement
quality of the UV-camera has been assessed by comparing it to a non-imaging UV-spectrometer by
measuring six different white-colored plant-based substances. Given the findings of these preliminary
studies, an experiment has been defined and performed monitoring the stress response of barley
leaves to salt stress. The aim was to visualize the effects of abiotic stress within the UV-range to
provide new insights into the stress response of plants visualizing the effects of abiotic stress within
the UV-range to provide new insights into the stress response of plants at the example of the stress
response of barley leaves to salt stress. Our study demonstrated the first use of a hyperspectral sensor
in the UV-range for stress detection in plant phenotyping.},
          Pages = {1401},
          Publisher = {MDPI},
          Title = {Extending hyperspectral imaging for plant phenotyping to the UV-range},
          Url = {./papers/brugger2019remoteSensing.pdf},
          Volume = {11},
          number = {12},
          Year = {2019},
      }


  @article{lioutikov2020,
          Anote = {./images/lioutikov2018icra_probGramMove.png},
          Author = {Rudolf Lioutikov and Guilherme Maeda and Filipe Veiga and Kristian Kersting and Jan Peters},
          Journal = {International Journal of Robotics Research (IJRR)},
          Keywords = {Robotics, Movement Primitives, Probabilistic Grammar, Bayesian Grammar Induction, Grammar Prior},
          Note = {Movement Primitives are a well studied and
        widely applied concept in modern robotics. Composing primitives
        out of an existing library, however, has shown to be
        a challenging problem. We propose the use of probabilistic
        context-free grammars to sequence a series of primitives to
        generate complex robot policies from a given library of primitives.
        The rule-based nature of formal grammars allows an
        intuitive encoding of hierarchically and recursively structured
        tasks. This hierarchical concept strongly connects with the way
        robot policies can be learned, organized, and re-used. However,
        the induction of context-free grammars has proven to be a
        complicated and yet unsolved challenge. In this work, we exploit
        the physical nature of robot movement primitives to restrict
        and efficiently search the grammar space. The grammar is
        learned with Markov Chain Monte Carlo optimization over the
        posteriors of the grammars given the observations. Restrictions
        over operators connecting the search define the corresponding
        proposal distributions and, therefore, guide the optimization
        additionally. In experiments, we validate our method on a
        redundant 7 degree-of-freedom lightweight robotic arm on tasks
        that require the generation of complex sequences of motions out
        of simple primitives.},
          Pages = {},
          Publisher = {SAGE},
          Title = {Learning Attribute Grammars for Movement Primitive Sequencing},
          Url = {https://journals.sagepub.com/doi/10.1177/0278364919868279},
          Volume = {39},
          number = {1},
          Year = {2020},
      }



  @incollection{ramanan2019tpm,
      Anote = {./images/ramanan2019tpm.png},
      Author = {Nandini Ramanan and Mayukh Das and Kristian Kersting and Sriraam Natarajan},
      Booktitle = {Working Notes of the ICML 2019 Workshop on Tractable Probabilistic Models (TPM)},
      Note = {Arithmetic Circuits (AC) and Sum-Product Networks (SPN) have recently gained significant interest
by virtue of being tractable deep probabilistic models. Most previous work on learning
AC structures, however, hinges on inducing a tree-structured AC and, hence, may potentially
break loops that may exist in the true generative model. To repair such broken loops, we propose a
gradient-boosted method for structure learning of discriminative ACs (DACs), called DACBOOST.
Since, in discrete domains, ACs are essentially equivalent to mixtures of trees, DACBOOST decomposes
a large AC into smaller tree-structured ACs and learns them in a sequential, additive
manner. The resulting non-parametric manner of learning the DACs results in a model with very
few tuning parameters making our learned model significantly more efficient. We demonstrate on
standard data sets and some real-world data sets, the efficiency of DACBOOST compared to the
state-of-the-art DAC learners without sacrificing the effectiveness. This makes it possible to employ
DACs for large scale real-world tasks.},
        Keywords = {Sum-Product Networks, Deep Learning, Structure Learning, Gradient Boosting},
      Pages = {},
      Title = {Discriminative Non-Parametric Learning of Arithmetic Circuits},
      Url = {./papers/ramanan2019tpm.pdf},
      Year = {2019}}


      @misc{kordjamshidi2019delbp,
          Anote = {./images/kordjamshidi2019delbp.png},
          Author = {Parisa Kordjamshidi and Dan Roth and Kristian Kersting},
          Howpublished = {arXiv preprint arXiv:1906.07809},
          Note = {Data-driven approaches are becoming more common as problem-solving techniques
          in many areas of research and industry. In most cases, machine learning models are the
          key component of these solutions, but a solution involves multiple such models, along
          with significant levels of reasoning with the models' output and input. Current
          technologies do not make such techniques easy to use for application experts who are
          not fluent in machine learning nor for machine learning experts who aim at testing ideas
          and models on real-world data in the context of the overall AI system. We review key
          efforts made by various AI communities to provide languages for high-level abstractions
          over learning and reasoning techniques needed for designing complex AI systems. We
          classify the existing frameworks based on the type of techniques and the data and knowledge
          representations they use, provide a comparative study of the way they address the challenges
          of programming real-world applications, and highlight some shortcomings and future directions.},
            Keywords = {Systems AI, AI Systems, Systems ML, Learning-based Programming, Probabilistic Programming, Deep Learning, Statistical Relational Learning, Declarative, Databases},
          Pages = {},
          Title = {Declarative Learning-Based Programming as an Interface to AI Systems},
          Url = {https://arxiv.org/pdf/1906.07809.pdf},
          Year = {2019}}




    @misc{galassi2019argument,
        Anote = {./images/galassi2019argument.png},
        Author = {Andrea Galassi and Kristian Kersting and Marco Lippi and Xiaoting Shao and Paolo Torroni},
        Howpublished = {arXiv preprint arXiv:1905.09103},
        Note = {Deep learning is bringing remarkable contributions to the field of argumentation mining,
but the existing approaches still need to fill
the gap towards performing advanced reasoning tasks. We illustrate how neural-symbolic
and statistical relational learning could play a
crucial role in the integration of symbolic and
sub-symbolic methods to achieve this goal.},
          Keywords = {Argument Mining, Deep Learning, Statistical Relational Learning, Neural-Symbolic Learning, Reasoning},
        Pages = {},
        Title = {Neural-Symbolic Argumentation Mining: an Argument in Favour of Deep Learning and Reasoning},
        Url = {https://arxiv.org/pdf/1905.09103.pdf},
        Year = {2019}}


        @incollection{shao2019tpm,
            Anote = {./images/shao2019tpm.png},
            Author = {Xiaoting Shao and Alejandro Molina and Antonio Vergari and Karl Stelzner and Robert Peharz and Thomas Liebig and Kristian Kersting },
            Booktitle = {Working Notes of the ICML 2019 Workshop on Tractable Probabilistic Models (TPM); also arXiv preprint arXiv:1905.08550},
            Note = {Bayesian networks are a central tool in machine learning and artificial intelligence, and make use
of conditional independencies to impose structure on joint distributions. However, they are generally
not as expressive as deep learning models and inference is hard and slow. In contrast, deep probabilistic
models such as sum-product networks (SPNs) capture joint distributions in a tractable fashion, but use
little interpretable structure. Here, we extend the notion of SPNs towards conditional distributions,
which combine simple conditional models into highdimensional ones. As shown in our experiments,
the resulting conditional SPNs can be naturally used to impose structure on deep probabilistic models,
allow for mixed data types, while maintaining fast and efficient inference.},
              Keywords = {Sum-Product Networks, Deep Learning, Structure Learning, Conditional Distribution, Gating Nodes, Neural Conditionals},
            Pages = {},
            Title = {Conditional Sum-Product Networks: Imposing Structure on Deep Probabilistic Architectures},
            Url = {https://arxiv.org/pdf/1905.08550.pdf},
            Year = {2019}}


  @misc{kersting2019ml_bmbf,
    Anote = {./images/kersting2019ml_bmbf.jpg},
    Author = {Kristian Kersting and Volker Tresp},
    Howpublished = {Whitepaper der AG 1 der Plattform Lernende Systeme des Bundesministerium f√ºr Bildung und Forschung (BMBF)},
    Keywords = {K√ºnstliche Intelligenz, Professuren, Investement, Infrastruktur, Bundesregierung, BMBF, Plattform Lernende Systeme, Deep Learning, Maschinelles Lernen},
    Note = {K√ºnstliche Intelligenz kann in vielf√§ltiger Art und Weise die Basis f√ºr Verbesserungen der Lebensbedingungen sein. Die Herausforderung f√ºr die Gesellschaft
    ist es, eine Zukunft mit K√ºnstlicher Intelligenz zu gestalten, dabei Chancen zu sehen, aber auch die Risiken zu analysieren und L√∂sungswege anzubieten.
    Die unterschiedlichen gesellschaftlichen Akteure m√ºssen eine Zukunft mit K√ºnstlicher Intelligenz gemeinsam gestalten. In diesem Whitepaper stehen prim√§r die
    technischen Herausforderungen und der Aufbau von F√§higkeiten im Mittelpunkt: Zu welchen Themen muss national KI-Kompetenz aufgebaut werden und in welche
    Forschungsthemen sollte heute und morgen investiert werden?},
      Title = {Maschinelles und Tiefes Lernen sind der Motor f√ºr ‚ÄûKI made in Germany‚Äú},
    Url = {./papers/kersting2019ml_bmbf.pdf},
    Year = {2019},
    Bdsk-Url-1 = {}}




@misc{kersting2019ki,
  Anote = {./images/ai.png},
  Author = {Kristian Kersting and Jan Peters and Constantin Rothkopf},
  Howpublished = {arXiv preprint arXiv:1903.09516},
  Keywords = {K√ºnstliche Intelligenz, Professur, Begutachtung, Kriterien, DBLP, Google Scholar},
  Note = {The Federal Government of Germany aims to boost the research
in the field of Artificial Intelligence (AI). For instance, 100 new professorships are
said to be established. However, the white paper of the government does not
answer what an AI professorship is at all. In order to give colleagues, politicians, and
citizens an idea, we present a view that is often followed when appointing
professors for AI at German and international universities. We hope that it will help
to establish a guideline with internationally accepted measures and thus make the
public debate more informed.},
    Title = {Was ist eine Professur f√ºr K√ºnstliche Intelligenz?},
  Url = {https://arxiv.org/pdf/1903.09516.pdf},
  Year = {2019},
  Bdsk-Url-1 = {https://arxiv.org/pdf/1903.09516.pdf}}


    @misc{molina2019spflow,
      Anote = {./images/molina2019spflow.png},
      Author = {Alejandro Molina and Antonio Vergari and Karl Stelzner and Robert Peharz and Pranav Subramani and Nicola Di Mauro and Pascal Poupart and Kristian Kersting},
      Howpublished = {arXiv preprint arXiv:1901.03704},
      Keywords = {Structure Learning, Sum-Product Networks, Deep Learning,  Tensor Flow, PyTorch, Python, Library, Tractable Probabilistic Models},
      Note = {We introduce SPFlow, an open-source Python library providing a simple interface to inference, learning and manipulation routines for deep and tractable
      probabilistic models called Sum-Product Networks (SPNs). The library allows one to quickly create SPNs both from
      data and through a domain specific language (DSL). It efficiently implements several probabilistic inference
      routines like computing marginals, conditionals and (approximate) most probable explanations (MPEs) along with
      sampling as well as utilities for serializing, plotting and structure statistics on an SPN. Moreover, many of
      the algorithms proposed in the literature to learn the structure and parameters of SPNs are readily available
      in SPFlow. Furthermore, SPFlow is extremely extensible and customizable, allowing users to promptly distill new
      inference and learning routines by injecting custom code into a lightweight functional-oriented API framework.
      This is achieved in SPFlow by keeping an internal Python representation of the graph structure that also enables
      practical compilation of an SPN into a TensorFlow graph, C, CUDA or FPGA custom code, significantly speeding-up
      computations.},
        Title = {SPFlow: An Easy and Extensible Library for Deep Probabilistic Learning using Sum-Product Networks},
      Url = {https://arxiv.org/pdf/1901.03704.pdf},
      Crossref = {https://github.com/SPFlow/SPFlow},
      Year = {2019},
      Bdsk-Url-1 = {https://arxiv.org/pdf/1901.03704.pdf}}


      @incollection{kulessa2019query,
        Anote = {./images/kulessa2019query.png},
        Author = {Moritz Kulessa and Alejandro Molina and Carsten Binnig and Benjamin Hilprecht and Kristian Kersting},
        Booktitle = {Working Notes of the 1st International Workshop on Applied AI for Database Systems and Applications (AIDB) at VLDB 2019; also as arXiv preprint arXiv:1811.06224},
        Keywords = {Sum-Product Networks, Deep Learning,  Databases, SQL Queries},
        Note = {Interactive visualizations are arguably the most important tool to explore, understand and convey facts about data. In the past years, the database community has been working on different techniques for Approximate Query Processing (AQP) that aim to deliver an approximate query result given a fixed time bound to support interactive visualizations better. However, classical AQP approaches suffer from various problems that limit the applicability to support the ad-hoc exploration of a new data set: (1) Classical AQP approaches that perform online sampling can support ad-hoc exploration queries but yield low quality if executed over rare subpopulations. (2) Classical AQP approaches that rely on offline sampling can use some form of biased sampling to mitigate these problems but require a priori knowledge of the workload, which is often not realistic if users want to explore a new database. In this paper, we present a new approach to AQP called Model-based AQP that leverages generative models learned over the complete database to answer SQL queries at interactive speeds. Different from classical AQP approaches, generative models allow us to compute responses to ad-hoc queries and deliver high-quality estimates also over rare subpopulations at the same time. In our experiments with real and synthetic data sets, we show that Model-based AQP can in many scenarios return more accurate results in a shorter runtime. Furthermore, we think that our techniques of using generative models presented in this paper can not only be used for AQP in databases but also has applications for other database problems including Query Optimization as well as Data Cleaning.},
          Title = {Model-based Approximate Query Processing},
        Url = {https://arxiv.org/pdf/1811.06224.pdf},
        Year = {2019},
        Bdsk-Url-1 = {https://arxiv.org/pdf/1811.06224.pdf}}



@inproceedings{jentzsch2019aies_moralChoiceMachine,
Anote = {./images/jentzsch2019aies_moralChoiceMachine.png},
Author = {Sophie Jentzsch and Patrick Schramowski and Constantin Rothkopf and Kristian Kersting},
  Booktitle = {Proceedings of the 2nd AAAI/ACM Conference on AI, Ethics, and Society (AIES)},
  Keywords = {Moral Machine, Neural Embedding, Norms, Social Bias},
  Note = {Allowing machines to choose whether to kill humans would
be devastating for world peace and security. But how do
we equip machines with the ability to learn ethical or even
moral choices? Here, we show that applying machine learning to human texts can extract deontological ethical reasoning
about ‚Äùright‚Äù and ‚Äùwrong‚Äù conduct. We create a template list
of prompts and responses, which include questions, such as
‚ÄúShould I kill people?‚Äù, ‚ÄúShould I murder people?‚Äù, etc. with
answer templates of ‚ÄúYes/no, I should (not).‚Äù The model‚Äôs
bias score is now the difference between the models score of
the positive response (‚ÄúYes, I should‚Äù) and that of the negative response (‚ÄúNo, I should not‚Äù). For a given choice overall, the model‚Äôs bias score is the sum of the bias scores for
all question/answer templates with that choice. We ran different choices through this analysis using a Universal Sentence Encoder. Our results indicate that text corpora contain
recoverable and accurate imprints of our social, ethical and
even moral choices. Our method holds promise for extracting, quantifying and comparing sources of moral choices in
culture, including technology.},
  Pages = {},
  Title = {Semantics Derived Automatically from Language Corpora Contain Human-like Moral Choices},
  Url = {./papers/jentzsch2019aies_moralChoiceMachine.pdf},
  Crossref = {https://github.com/ml-research/moral-choice-machine},
  Year = {2019}}

  @inproceedings{teso2019aies_XIML,
  Anote = {./images/teso2019aies_XIML.png},
  Author = {Stefano Teso and Kristian Kersting},
    Booktitle = {Proceedings of the 2nd AAAI/ACM Conference on AI, Ethics, and Society (AIES)},
    Keywords = {Explainable AI, Interactive Learning, Active Learning, Lime, Model-agnostic Explanations},
    Note = {Although interactive learning puts the user into the loop, the learner
    remains mostly a black box for the user. Understanding the reasons behind
    queries and predictions is important when assessing how the learner works
    and, in turn, trust. Consequently, we propose the novel framework of
    explanatory interactive learning: in each step, the learner explains its
    query to the user, and the queries of any active classifier for visualizing
    explanations of the corresponding predictions.  We demonstrate that this
    can boost the predictive and explanatory powers of, and the trust into, the
    learned model, using text (e.g. SVMs) and image classification (e.g. neural
    networks) experiments as well as a user study.},
    Pages = {},
    Title = {Explanatory Interactive Machine Learning},
    Url = {./papers/teso2019aies_XIML.pdf},
    Year = {2019}}


@inproceedings{vergari2019aaai_abda,
Anote = {./images/vergari2018tpm.png},
Author = {Antonio Vergari and Alejandro Molina and Robert Peharz and Zoubin Ghahramani and Kristian Kersting and Isabel Valera},
  Booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)},
  Keywords = {Automatic Statistician, Sum-Product Network, Deep Learning, Bayesian Learning, AutoML, Density Estimation},
  Note = {Making sense of a dataset in an automatic and unsupervised fashion is a
  challenging problem in statistics and AI. Classical approaches for density estimation are
  usually not flexible enough to deal with the uncertainty inherent to real-world data: they
  are often restricted to fixed latent interaction models and homogeneous likelihoods; they are
  sensitive to missing, corrupt and anomalous data; and their expressiveness generally comes at
  the price of intractable inference. As a result, supervision from statisticians is usually needed
  to find the right model for the data. However, as domain experts do not necessarily have to be
  experts in statistics, we propose Automatic Bayesian Density Analysis (ABDA) to make density
  estimation accessible at large. ABDA automates the selection of adequate likelihood models
  from arbitrarily rich dictionaries while modeling their interactions via a deep latent structure
  adaptively learned from data as a sum-product network. ABDA casts uncertainty estimation at
  these local and global levels into a joint Bayesian inference problem, providing robust and yet
  tractable inference. Extensive empirical evidence shows that ABDA is a suitable tool for automatic
  exploratory analysis of heterogeneous tabular data, allowing for missing value estimation,
  statistical data type and likelihood discovery, anomaly detection and dependency structure mining,
  on top of providing accurate density estimation.},
  Pages = {},
  Title = {Automatic Bayesian Density Analysis},
  Crossref = {https://github.com/probabilistic-learning/abda},
  Url = {./papers/vergari2019aaai_abda.pdf},
  Year = {2019}}


@inproceedings{das2019aaai_couting,
  Anote = {./images/das2019aaai_counting.png},
  Author = {Mayukh Das and Devendra Singh Dhami and Kunapulli Gautam and Kristian Kersting and Sriraam Natarajan},
  Booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)},
  Keywords = {Lifted Inference, Graph Databases, Approximate Counting, Hypergraphs},
  Note = {Counting the number of true instances of a clause is arguably a major bottleneck in relational
  probabilistic inference and learning. We approximate counts in two steps: (1) transform the fully
  grounded relational model to a large hypergraph, and partially-instantiated clauses to hypergraph
  motifs; (2) since the expected counts of the motifs are provably the clause counts, approximate
  them using summary statistics (in/out-degrees, edge counts, etc). Our experimental results
  demonstrate the efficiency of these approximations, which can be applied to many complex
  statistical relational models, and can be significantly faster than state-of-the-art,
  both for inference and learning, without sacrificing effectiveness.},
  Pages = {},
  Title = {Fast Relational Probabilistic Inference and Learning: Approximate Counting via Hypergraphs},
  Url = {./papers/das2019aaai_couting.pdf},
  Crossref = {https://starling.utdallas.edu/software/boostsrl/wiki/},
  Year = {2019}}


@article{kersting2018mlai,
        Anote = {./images/fontiersBD.png},
        Author = {Kristian Kersting},
        Journal = {Frontiers in Big Data - Section on Machine Learning and Artificial Intelligence},
        Keywords = {Machine Learning, Artificial intelligence, ML=AI?, Editorial },
        Note = {Machine learning (ML) and artificial intelligence (AI) are becoming dominant problem-solving
        techniques in many areas of research and industry, not least because of the recent successes of deep
        learning (DL). However, the equation AI=ML=DL, as suggested by many recent news, blogs and media,
        falls too short. The fields share the same fundamental hypotheses: computation is a useful way to
        model intelligent behaviour in machines. What kind of computation and how to program it? That
        remains an open question. Computation neither rules out search, logical and probabilistic techniques
        nor (deep) (un)supervised and reinforcement learning methods, among others, as computational models
        do contain all of them. They complement each other, and the1 next breakthrough lies not only in
        pushing each of them but also in combining them.},
        Pages = {doi:10.3389/fdata.2018.00006},
        Publisher = {},
        Title = {Machine Learning and Artificial Intelligence: Two Fellow Travelers on the Quest for Intelligent Behaviour in Machines},
        Url = {./papers/kersting2018aiml_frontiers.pdf},
        Volume = {},
        number = {},
        Year = {2018},
    }

    @article{kersting2018ki_editorial,
      Anote = {./images/ki2018.jpg},
      Author = {Kristian Kersting},
      Journal = {K√ºnstliche Intelligenz (KI)},
      Keywords = {Enquete Commission AI, Artificial Intelligence, Machine Learning, Computational Cognitive Science, Editorial},
      Note = {},
      Pages = {doi:10.1007/s13218-018-0562-8},
      Publisher = {Springer},
      Title = {Making AI Smarter},
      Url = {https://doi.org/10.1007/s13218-018-0562-8},
      Volume = {},
      number = {},
      Year = {2018},
  }


@inproceedings{stelzner2018probprog,
    Anote = {./images/stelzner2018probprog.png},
  Author = {Karl Stelzner and Alejandro Molina and Robert Peharz and Antonio Vergari and Martin Trapp and Isabel Valera and Zoubin Ghahramani and Kristian Kersting},
  Booktitle = {Proceedings of the Inaugural International Conference on Probabilistic Programming (ProbProg)},
  Note = {Probabilistic models provide a framework for describing abstract prior knowledge and using it to reason under uncertainty. Deep probabilistic programming languages (PPL) are a powerful tool to ease the development of probabilistic models. They let users specify generative probabilistic models as programs and then ``compile'' those models down into inference procedures. Since probabilistic inference is still intractable, existing deep PPLs leverage deep learning for inference. The key idea is to describe inference via a second model called an inference model trained in variational fashion. Both the generative and the inference models can include deep neural networks as components.
While the details of the employed neural architectures may differ, they typically model distributions only implicitly, i.e., they allow for sampling but not the computation of marginal probabilities.
This extended abstract makes the case to not "go down the full neural road", but to explicitly obtain uncertainties in an arithmetic circuit manner. To this end, sum-product networks (SPNs) are a promising option, as they are a class of probabilistic model which permit explicit uncertainties and efficient inference.
More precisely, SPNs can compute any marginalization and conditioning query in time linear of the model's representation size. Although SPNs can be described in a nutshell as "deep mixture models", they have received no attention in the deep PPL community, despite their attractive inference properties.},
    Keywords = {Probabilistic Programming, Sum-Product Networks, Deep Learning, SPFlow, Random SPNs, VAE, Attend-Infer-Repeat },
  Pages = {},
  Title = {SP3 - Sum Product Probabilistic Programming},
  Url = {./papers/sp3_probprog2018.pdf},
  Year = {2018}}
sp3_probprog2018


@inproceedings{das2018probprog,
    Anote = {./images/das018probprog.png},
  Author = {Mayukh Das and Devendra Singh Dhami and Kunapulli Gautam and Kristian Kersting and Sriraam Natarajan},
  Booktitle = {Proceedings of the Inaugural International Conference on Probabilistic Programming (ProbProg)},
  Note = {Inference and Parameter Learning inside Probabilistic Programming use an important operation that can be approximated: counting.
  We present an efficient approximation scheme that allows for fast counting and consequently faster inference and learning.},
    Keywords = {Probabilistic Programming, Statistical Relational Learning, Counting},
  Pages = {},
  Title = {Approximate Counting for Fast Inference and Learning in Probabilistic Programming},
  Year = {2018}}


  @inproceedings{natarajan2018probprog,
      Anote = {./images/natarajan018probprog.png},
    Author = {Sriraam Natarajan and Phillip Odom and Tushar Khot and Kristian Kersting and Jude Shavlik},
    Booktitle = {Proceedings of the Inaugural International Conference on Probabilistic Programming (ProbProg)},
    Note = {We present our BoostSRL system, a Java-based learning system
  that inductively learns probabilistic logic clauses from
  data. Our system is capable of learning different types of
  models, handling modeling of hidden data, learning with
  preferences from humans, scaling with large amounts of
  data by approximate counting and modeling temporal data.
  We review these capabilities briefly in this short paper.},
      Keywords = {Probabilistic Programming, Relational Boosting, Human-in-the-loop, Interactive ML},
    Pages = {},
    Url = {./papers/probProg2018_BoostSRL.pdf},
    Title = {Human-in-the-loop Learning for Probabilistic Programming},
    Crossref = {https://starling.utdallas.edu/software/boostsrl/wiki/},
    Year = {2018}}


@inproceedings{sommer2018iccd,
    Anote = {./images/sommer2018tpm.png},
  Author = {Lukas Sommer and Julian Oppermann and Alejandro Molina and Carsten Binnig and Kristian Kersting and Andreas Koch},
  Booktitle = {Proceedings of the 36th IEEE International Conference on Computer Design (ICCD)},
  Note = {FPGAs have recently proven to be ideally suited for the implementation of efficient accelerators for a wide
    range of machine learning tasks. Here, we consider probabilistic models, specifically, (Mixed) Sum-Product Networks
    (SPN), a deep architecture that can provide tractable inference for multivariate distributions over mixed data-sources.
We show how to construct an FPGA-based accelerator for the inference in (mixed) SPNs. Starting from an input description of
the network, we develop a fully automatic synthesis flow to a custom FPGA-accelerator. The synthesized accelerator and its
interface to the external memory on the FPGA are fully pipelined, and computations are conducted using double-precision floating
point arithmetic.  To the best of our knowledge, this work is the first approach to offload the (mixed) SPN inference problem to FPGA-based
accelerators. Our evaluation shows that the SPN inference problem can profit from offloading to an FPGA accelerator.},
    Keywords = {FPGA, Sum-Product Networks, Deep learning, Hardware implementation },
  Pages = {},
  Url = {./papers/spn_fpga_iccd18.pdf},
  Title = {Automatic Mapping of the Sum-Product Network Inference Problem to FPGA-based Accelerators},
  Year = {2018}}


@inproceedings{ramanan2018kr_learningRLR,
  Anote = {./images/ramanan2018kr_learningRLR.png},
  Author = {Nandini Ramanan and Gautam Kunapuli and Tushar Khot and Bahare Fatemi and Seyed Mehran Kazemi and David Poole and Kristian Kersting and Sriraam Natarajan},
  Booktitle = {Proceedings of the 16th International Conference on Principles of Knowledge Representation and Reasoning (KR). Also presented at the Hybrid Reasoning
  and Learning Workshop (HRL) at KR 2018. Longer version as arXiv:1808.02123.},
  Note = {We consider the problem of learning Relational Logistic Regression
(RLR). Unlike standard logistic regression, the features
of RLRs are first-order formulae with associated weight
vectors instead of scalar weights. We turn the problem of
learning RLR to learning these vector-weighted formulae and
develop a learning algorithm based on the recently successful
functional-gradient boosting methods for probabilistic logic
models. We derive the functional gradients and show how
weights can be learned simultaneously in an efficient manner.
Our empirical evaluation on standard and novel data sets
demonstrates the superiority of our approach over other methods
for learning RLR.},
    Keywords = {Statistical relational learning, Gradient Boosting, Relational Logistic Regression},
  Pages = {},
  Url = {https://arxiv.org/pdf/1808.02123.pdf},
  Title = {Structure Learning for Relational Logistic Regression: An Ensemble Approach},
  Year = {2018}}



@incollection{vergari2018tpm,
    Anote = {./images/vergari2018tpm.png},
    Author = {Antonio Vergari and Alejandro Molina and Robert Peharz and Zoubin Ghahramani and Kristian Kersting and Isabel Valera},
    Booktitle = {Working Notes of the ICML 2018 Workshop on Tractable Probabilistic Models (TPM); also arXiv preprint arXiv:1807.09306},
    Note = {Making sense of a dataset in an automatic, unsupervised fashion is a challenging problem in statistics
      and AI. For example, classical approaches for density estimation do not naturally deal with heterogeneous
      statistical data types, do not automatically select suitable parametric forms for the likelihood models, and,
      in basic formulations, are sensitive to corrupted data and outliers. To overcome this, we propose to extend
      density estimation to Automatic Bayesian Density Analysis (ABDA), casting both data modeling and selection of adequate likelihood models (statistical data types) into a joint inference problem. Specifically, we advocate a hierarchically structured mixture model, which explicitly incorporates arbitrarily rich collections of likelihood models and corresponding latent selection variables, and captures variable interactions by a latent hierarchical structure obtained from data-type agnostic structure learning. To account for prediction uncertainty, selection of parametric likelihood models and
      statistical data types, we employ Bayesian inference over a model formulated as a sum-product network,
      naturally providing the aforementioned expressiveness and flexibility, while facilitating exact and tractable
      inference.},
      Keywords = {Sum-Product Networks, Deep Learning, Automatic Statistician, Density Estimation, Density Analysis, AutoML},
    Pages = {},
    Title = {Automatic Bayesian Density Analysis},
    Url = {https://arxiv.org/pdf/1807.09306.pdf},
    Year = {2018}}


@incollection{peharz2018ratspns,
          Anote = {./images/peharz2018ratspns.png},
          Booktitle = {Working Notes of the UAI 2018 Workshop on Uncertainty in Deep Learning (UDL); also arXiv preprint arXiv:1806.01910},
          Author = {Robert Peharz and Antonio Vergari and Karl Stelzner and Alejandro Molina and Martin Trapp and Kristian Kersting and Zoubin Ghahramani},
          Keywords = {deep learning, probabilistic deep learning, sum product networks, random models, generative model},
          Note = {Probabilistic deep learning currently receives an increased interest, as
            consistent treatment of uncertainty is one of the most important goals in machine
            learning and AI. Most current approaches, however, have severe limitations concerning
            inference. Sum-Product networks (SPNs), although having excellent properties in that regard,
            have so far not been explored as serious deep learning models, likely due to their special
            structural requirements. In this paper, we make a drastic simplification and use a random
            structure which is trained in a "classical deep learning manner" such as automatic
            differentiation, SGD, and GPU support. The resulting models, called RAT-SPNs, yield
            comparable prediction results to deep neural networks, but maintain well-calibrated
            uncertainty estimates which makes them highly robust against missing data. Furthermore,
            they successfully capture uncertainty over their inputs in a convincing manner, yielding
            robust outlier and peculiarity detection.},
          Pages = {},
          Publisher = {},
          Title = {Probabilistic Deep Learning using Random Sum-Product Networks},
          Url = {https://arxiv.org/pdf/1806.01910.pdf},
          Year = {2018},
          Bdsk-Url-1 = {https://arxiv.org/pdf/1806.01910.pdf}}




  @incollection{sommer2018tpm,
    Anote = {./images/sommer2018tpm.png},
    Author = {Lukas Sommer and Julian Oppermann and Alejandro Molina and Carsten Binnig and Kristian Kersting and Andreas Koch},
    Booktitle = {Working Notes of the ICML 2018 Workshop on Tractable Probabilistic Models (TPM)},
    Note = {FPGAs have recently proven to be ideally suited for the implementation of efficient accelerators for a wide
      range of machine learning tasks. Here, we consider probabilistic models, specifically, (Mixed) Sum-Product Networks
      (SPN), a deep architecture that can provide tractable inference for multivariate distributions over mixed data-sources.
We show how to construct an FPGA-based accelerator for the inference in (mixed) SPNs. Starting from an input description of
the network, we develop a fully automatic synthesis flow to a custom FPGA-accelerator. The synthesized accelerator and its
interface to the external memory on the FPGA are fully pipelined, and computations are conducted using double-precision floating
point arithmetic.  To the best of our knowledge, this work is the first approach to offload the (mixed) SPN inference problem to FPGA-based
accelerators. Our evaluation shows that the SPN inference problem can profit from offloading to an FPGA accelerator.},
      Keywords = {FPGA, Sum-Product Networks, Deep learning, Hardware implementation },
    Pages = {},
    Title = {Automatic Synthesis of FPGA-based Accelerators for the Sum-Product Network Inference Problem},
    Url = {},
    Year = {2018}}


  @inproceedings{kordjamshidi2018ijcaiecai_systemsai,
    Anote = {./images/kordjamshidi2018ijcaiecai_systemsai.png},
    Author = {Parisa Kordjamshidi and Dan Roth and Kristian Kersting},
    Booktitle = {Proceedings of the 27th International Joint Conference on Artificial Intelligence and the 23rd European Conference on Artificial Intelligence (IJCAI-ECAI)},
    Note = {Data-driven approaches are becoming dominant problem-solving techniques in many areas of research and industry. Unfortunately, current technologies do not make it easy to use them for application experts that are not fluent in machine learning technologies. We review key efforts made by various AI communities to provide languages for high-level abstractions
over learning and reasoning techniques needed for designing complex AI systems. We classify the existing
frameworks based on the techniques as well as the data and knowledge representations they use, provide a
comparative study of the way they address the challenges of programming real-world applications, and highlight
some shortcomings and future directions.  },
      Keywords = {Systems AI, Learning based programming, Statistical relational learning, Complex AI systems, Survey},
    Pages = {},
    Title = {Systems AI: A Declarative Learning Based Programming Perspective},
    Url = {./papers/kordjamshidi2018ijcaiecai_systemsai.pdf},
    Year = {2018}}

  @incollection{gries2018esof,
    Anote = {./images/esof2018.png},
    Author = {Lucas Gries and Edith Luschmann and Laurent Gautier and Lars Koppers and Kristian Kersting and J√∂rg Rahnenf√ºhrer and Julia Serong and Holger Wormer },
    Booktitle = {Poster Proceedings of the Euroscience Open Forum (ESOF) Conference},
    Note = {It‚Äôs surprising how often scientists are surprised by their findings: This observation was
described in Nature by the Polish Researcher Michael Jasienski more than ten years ago. In
2015 Christiaan Vinkers from the Netherlands compared the use of and semantic valence
between positive and negative words of PubMed abstracts from 1974 to 2014 and found signs
for increasing sensationalism. However, systematic tools to evaluate "sensationalism" and
similar aspects concerning the wording in pieces of science communication are still missing.
Furthermore, comparisons between European countries are difficult, as systematic access
(e.g., to scientific press releases) differs from one country to another.
Our interdisciplinary group will present approaches to compare press releases from different
disciplines, institutions and countries by means of database examples from Germany
(providing an already analysed database extracted from the ‚ÄúInformationsdienst
Wissenschaft‚Äù of more than 300 000 press releases from about 1000 institutions) and France
(using a newly constructed corpus from more than 100 institutions). Considering different
perspectives from communication science, informatics and statistics we discuss to what extent
indexes based on a set of "sensational words" and other forms of linguistic analysis may deliver
hints for unethical exaggerations in different fields of science communication.},
      Keywords = {Poster, Sensationalism, Communication Science, Linguistic Analysis},
    Pages = {},
    Title = {Science, Surprise and Sensation in Science Communication: Towards a Concept for Cross-Country Comparisons of Press Releases from Research Institutions},
    Url = {./papers/gries2018ESOF_poster.pdf},
    Year = {2018}}


    @incollection{bauer2018ecda,
      Anote = {./images/ecda2018.png},
      Author = {Nadja Bauer and Malte Jastrow and Daniel Horn and Lukas Stankiewicz and Kristian Kersting and Jochen Deuse and Claus Weihs},
      Booktitle = {Abstract Proceedings of the European Conference on Data Analysis (ECDA)},
      Note = {The advent of industry 4.0 and the availability of large data storage systems lead to an increasing demand
for specially educated data-oriented professionals in industrial production. The education of such specialists
is supposed to combine elements from the three fields of engineering, data analysis and data administration.
Data administration skills are demanded to handle big data in diverse storage structures in data bases.
In order to extract knowledge from the stored data the proficient handling of data analysis tools - especially
machine learning - is essential. Finally, industrial domain knowledge is important to identify possible applications
for machine learning algorithms (and to interpret the results). However, to the best knowledge of the authors, an education program incorporating
elements of all three fields has not yet been established (in Germany).

In the context of the newly acquired project "Industrial Data Science" (InDaS) we aim to develop a qualification
concept for machine learning in industrial production targeted at two different groups. On the one hand advanced
students from any of the three fields mentioned above and on the other hand experienced professionals working in
industrial production. For the first group a one term lecture is going to be designed while coping with different
levels of knowledge in the inhomogeneous audience. It will be followed by a seminar with focus on use cases delivered
by partners from industrial production. Separately a workshop concept for the second target group will be developed
taking into account the strong domain knowledge of the participants.

The contents of the qualification concept should be selected according to the needs of industrial companies.
Therefore a survey was created to inquire the use and potentials of machine learning and the requirements for future
employees in industrial production. The evaluation of the survey and the resulting conclusions affecting the qualification
concept are going to be presented in this talk.},
        Keywords = {Abstract, machine learning, engineering, production, qualification concept},
      Pages = {},
      Title = {Industrial Data Science: developing a qualification concept for machine learning in industrial production},
      Url = {},
      Year = {2018}}


  @inproceedings{kolb2018ijcaiecai_xadds,
    Anote = {./images/kolb2018ijcaiecai_xadds.png},
    Author = {Samuel Kolb and Martin Mladenov and Scott Sanner and Vaishak Belle and Kristian Kersting},
    Booktitle = {Proceedings of the 27th International Joint Conference on Artificial Intelligence and the 23rd European Conference on Artificial Intelligence (IJCAI-ECAI)},
    Note = {In recent years, weighted model counting (WMC) has become the de facto work-horse for logically-structured inference tasks.
      Problems ranging over probabilistic inference, planning, game theory, and numerical optimization have been tackled successfully using WMC technology.
      Weighted model integration (WMI) is a recent formalism generalizing WMC to the integration of functions over models of mixed discrete-continuous theories.
      WMI has already shown tremendous promise for solving inference problems in graphical models and probabilistic programs.
      Yet, state of the art tools for WMI are nowhere as versatile as those for WMC--solvers are generally limited either by the range of amenable theories, or in terms of performance.
      Moreover, the data structures to represent, compose, and transform formulas over mixed theories have not reached the efficiency of their Boolean counterparts.
      Arguably, the latter is one of the prime reasons why WMC is so widely applicable.
      To address both limitations, we propose the use of extended algebraic decision diagrams (XADDs) as a compilation language for WMI.
      Aside from tackling typical WMI problems XADDs also enable partial WMI yielding parametrized solutions.
      To overcome the main roadblock of XADDs---the computational cost of variable elimination---we formulate a novel and powerful exact symbolic dynamic programming (SDP)
      algorithm that unlike its predecessor is able to effectively cache partial computations and seamlessly handle Boolean, integer-valued, and real variables.
      Our empirical results demonstrate that the improved algorithm can lead to an exponential to linear computational reduction in the best case and that it exceeds or
      matches state-of-the-art WMI solvers in terms of performance.
},
      Keywords = {Variable elimination for WMI, XADDs, SMT, Weighted Model Integration (WMI), Probabilsitic Inference, Smybolic Integration},
    Pages = {},
    Title = {An Efficient Symbolic Partial Integration Operator for Probabilistic Inference},
    Url = {./papers/kolb2018ijcaiecai_xadds.pdf},
    Year = {2018}}


    @inproceedings{luedtke2018ijcaiecai_liftedFiltering,
      Anote = {./images/liftedFiltering.png},
      Author = {Stefan L√ºdtke and Max Schr√∂der and Sebastian Bader and Kristian Kersting and Thomas Kirste},
      Booktitle = {Proceedings of the 27th International Joint Conference on Artificial Intelligence and the 23rd European Conference on Artificial Intelligence (IJCAI-ECAI)},
      Keywords = {Lifted Inference, Filtering, Planning, Symmetries},
      Note = {We present a model for recursive Bayesian filtering based on lifted multiset states.
        Combining multisets with lifting makes it possible to simultaneously exploit multiple strategies
        for reducing inference complexity when compared to list-based grounded state representations. The
        core idea is to borrow the concept of Maximally Parallel Multiset Rewriting Systems and to enhance
        it by concepts from Rao-Blackwellisation and Lifted Inference, giving a representation of state
        distributions that enables efficient inference. In worlds where the random variables that define
        the system state are exchangeable - where the identity of entities does not matter - it automatically
        uses a representation that abstracts from ordering (achieving an exponential reduction
        in complexity) and it automatically adapts when observations or system dynamics destroy
        exchangeability by breaking symmetry.},
      Title = {Lifted Filtering via Exchangeable Decomposition},
      Url = {https://arxiv.org/pdf/1801.10495.pdf},
      Year = {2018},
      Bdsk-Url-1 = {https://arxiv.org/pdf/1801.10495.pdf}}


      @article{antanas2019auro,
        Anote = {./images/antanas2018auro.png},
        Author = {Laura Antanas and Plinio Moreno and Marion Neumann and Rui {Pimentel de Figueiredo} and Kristian Kersting and Jos√© Santos-Victor and Luc {De Raedt}},
        Journal = {Autonomous Robots (AURO)},
        Keywords = {Robotics, Statistical Relational Learning, Grasping, Hybrid Domains },
        Note = {While any grasp must satisfy the grasping stability criteria, good grasps depend on the
specific manipulation scenario: the object, its properties and functionalities, as well as
the task and grasp constraints. We propose a probabilistic logic approach for robot
grasping, which improves grasping capabilities by leveraging semantic object parts. It
provides the robot with semantic reasoning skills about the most likely object part to be
grasped, given the task constraints and object properties, while also dealing with the
uncertainty of visual perception and grasp planning. The probabilistic logic framework
is task-dependent. It semantically reasons about pre-grasp configurations with respect
to the intended task and employs object-task affordances and object/task ontologies to
encode rules that generalize over similar object parts and object/task categories. The
use of probabilistic logic for task-dependent grasping contrasts with current
approaches that usually learn direct mappings from visual perceptions to task-dependent
grasping points. The logic-based module receives data from a low-level
module that extracts semantic objects parts, and sends information to the low-level
grasp planner. These three modules define our probabilistic logic framework, which is
able to perform robotic grasping in realistic kitchen-related scenarios.},
        Publisher = {Springer},
        Title = {Semantic and Geometric Reasoning for Robotic Grasping: A Probabilistic Logic Approach},
        Url = {https://link.springer.com/article/10.1007/s10514-018-9784-8},
        Volume = {43},
        Number = {6},
        Pages = {1393--1418},
        Year = {2019},
    }



    @article{kersting2018ki,
      Anote = {./images/ki2018.jpg},
      Author = {Kristian Kersting and Ulrich Meyer},
      Journal = {K√ºnstliche Intelligenz (KI)},
      Keywords = {Artificial Intelligence, Big Data, Algorithmic Challenges, Editorial},
      Note = {Big Data is no fad. The world is growing at an exponential rate, and so is the size of data
        collected across the globe. The data is becoming more meaningful and contextually relevant, breaks
        new ground for machine learning and artificial intelligence (AI), and even moves them from research
        labs to production. That is, the problem has shifted from collecting massive amounts of data to
        understanding it, i.e., turning data into knowledge, conclusions, and actions. This Big AI,
        however, often faces poor scale-up behaviour from algorithms that have been designed based on
        models of computation that are no longer realistic for Big Data. This special issue constitutes
        an attempt to highlight the algorithmic challenges and opportunities but also the social and
        ethical issues of Big Data. Of specific interest and focus have been computation- and
        resource-efficient algorithms when searching through data to find and mine relevant or
        pertinent information.},
      Pages = {3--8},
      Publisher = {Springer},
      Title = {From Big Data to Big Artificial Intelligence? - Algorithmic Challenges and Opportunities of Big Data},
      Url = {https://link.springer.com/content/pdf/10.1007%2Fs13218-017-0523-7.pdf},
      Volume = {32},
      number = {1},
      Year = {2018},
  }

  @misc{teso2018explanatory,
    Anote = {./images/teso2018explanatory.png},
    Author = {Stefano Teso and Kristian Kersting},
    Howpublished = {arXiv preprint arXiv:1805.08578; also abstract at the 14th Biannual Conference of the German Society for Cognitive Science (KogWis) 2018},
    Keywords = {trust, explanations, active learning, explantory interactive learning},
    Note = {Although interactive learning puts the user into the loop, the learner
      remains mostly a black box for the user. Understanding the reasons behind queries
      and predictions is important when assessing how the learner works and, in turn, trust.
      Consequently, we propose the novel framework of explanatory interactive learning:
      in each step, the learner explains its interactive query to the user, and she
      queries of any active classifier for visualizing explanations of the corresponding
      predictions. We demonstrate that this can boost the predictive and explanatory powers of and the trust into the learned model, using text (e.g. SVMs) and image classification (e.g. neural networks) experiments as well as a user study.},
    Title = {"Why Should I Trust Interactive Learners?" Explaining Interactive Queries of Classifiers to Users},
    Url = {https://arxiv.org/pdf/1805.08578.pdf},
    Year = {2018},
    Bdsk-Url-1 = {https://arxiv.org/pdf/1805.08578.pdf}}





  @misc{schramowski2018neuralfw,
  	Anote = {./images/neuralfw.png},
  	Author = {Patrick Schramowski and Christian Bauckhage and Kristian Kersting},
  	Howpublished = {arXiv preprint arXiv:1803.04300},
  	Keywords = {Frank Wolfe, Deep Learning, Neural Networks, Learnign to learn, meta learning},
  	Note = {The move from hand-designed to learned optimizers
in machine learning has been quite successful
for gradient-based and -free optimizers. When
facing a constrained problem, however, maintaining
feasibility typically requires a projection step,
which might be computationally expensive and
not differentiable. We show how the design of
projection-free convex optimization algorithms
can be cast as a learning problem based on FrankWolfe
Networks: recurrent networks implementing
the Frank-Wolfe algorithm aka. conditional
gradients. This allows them to learn to exploit
structure when, e.g., optimizing over rank-1 matrices.
Our LSTM-learned optimizers outperform
hand-designed as well learned but unconstrained
ones. We demonstrate this for training support
vector machines and softmax classifiers},
  	Title = {Neural Conditional Gradients},
  	Url = {https://arxiv.org/pdf/1803.04300.pdf},
  	Year = {2018},
  	Bdsk-Url-1 = {https://arxiv.org/pdf/1803.04300.pdf}}



    @misc{luedtke2018liftedFiltering,
      Anote = {./images/liftedFiltering.png},
      Author = {Stefan L√ºdtke and Max Schr√∂der and Sebastian Bader and Kristian Kersting and Thomas Kirste},
      Howpublished = {arXiv preprint arXiv:1801.10495},
      Keywords = {Lifted Inference, Filtering, Planning, Symmetries},
      Note = {We present a model for recursive Bayesian filtering based on lifted multiset states.
        Combining multisets with lifting makes it possible to simultaneously exploit multiple strategies
        for reducing inference complexity when compared to list-based grounded state representations. The
        core idea is to borrow the concept of Maximally Parallel Multiset Rewriting Systems and to enhance
        it by concepts from Rao-Blackwellisation and Lifted Inference, giving a representation of state
        distributions that enables efficient inference. In worlds where the random variables that define
        the system state are exchangeable - where the identity of entities does not matter - it automatically
        uses a representation that abstracts from ordering (achieving an exponential reduction
        in complexity) and it automatically adapts when observations or system dynamics destroy
        exchangeability by breaking symmetry.},
      Title = {Lifted Filtering via Exchangeable Decomposition},
      Url = {https://arxiv.org/pdf/1801.10495.pdf},
      Year = {2018},
      Bdsk-Url-1 = {https://arxiv.org/pdf/1801.10495.pdf}}

  @inproceedings{lioutikov2018icra_probGramMove,
    Anote = {./images/lioutikov2018icra_probGramMove.png},
    Author = {Rudolf Lioutikov and Guilherme Maeda and Filipe Veiga and Kristian Kersting and Jan Peters},
    Booktitle = {Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)},
    Note = {Movement Primitives are a well studied and
widely applied concept in modern robotics. Composing primitives
out of an existing library, however, has shown to be
a challenging problem. We propose the use of probabilistic
context-free grammars to sequence a series of primitives to
generate complex robot policies from a given library of primitives.
The rule-based nature of formal grammars allows an
intuitive encoding of hierarchically and recursively structured
tasks. This hierarchical concept strongly connects with the way
robot policies can be learned, organized, and re-used. However,
the induction of context-free grammars has proven to be a
complicated and yet unsolved challenge. In this work, we exploit
the physical nature of robot movement primitives to restrict
and efficiently search the grammar space. The grammar is
learned with Markov Chain Monte Carlo optimization over the
posteriors of the grammars given the observations. Restrictions
over operators connecting the search define the corresponding
proposal distributions and, therefore, guide the optimization
additionally. In experiments, we validate our method on a
redundant 7 degree-of-freedom lightweight robotic arm on tasks
that require the generation of complex sequences of motions out
of simple primitives.},
      Keywords = {Robotics, Movement Primitives, Probabilistic Grammar, Bayesian Grammar Induction, Grammar Prior},
    Pages = {},
    Title = {Inducing Probabilistic Context-Free Grammars for the Sequencing of Robot Movement Primitives},
    Url = {./papers/lioutikov2018icra_probGramMove.pdf},
    Year = {2018}}

  @inproceedings{binnig2018sysml_deepVizdom,
    Anote = {./images/binnig2018sysml_deepVizdom.png},
    Author = {Carsten Binnig and Kristian Kersting and Alejandro Molina and Emanuel Zgraggen},
    Booktitle = {Proceedings of the Inaugural Systems and Machine Learning Conference (SysML)},
    Note = {We make the case for a new generation of interactive data exploration systems
      that seamlessly integrate deep models as first class citizens into the data exploration stack.
      Based on three case studies, we argue that this not only enables users to gain a much
      deeper insights into a broader range of data sets but also
      helps to improvethe performance and quality of existing data exploration systems.},
      Keywords = {Databases, Interactive ML, Automatic Statistician, ML Systems, Deep Learning, Generative Model, Sum Product Networks},
    Pages = {},
    Title = {DeepVizdom: Deep Interactive Data Exploration},
    Url = {./papers/binnig2018sysml_deepVizdom.pdf},
    Year = {2018}}

  @inproceedings{molina2018aaai_mspn,
    Anote = {./images/molina2018aaai_mspn.png},
    Author = {Alejandro  Molina and Antonio Vergari and Nicola Di Mauro and Floriana Esposito and Siraam Natarajan and Kristian Kersting},
    Booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)},
    Keywords = {Hybrid Domains,Deep Learning, Generative Model, Sum Product Networks, Hirschfeld-Gebelein-R√©nyi, Automatic Statistician, Structure Learning},
    Note = {While all kinds of mixed data ‚Äî from personal data, over panel and scientific data,
      to public and commercial data ‚Äî are collected and stored, building probabilistic graphical models for
      these hybrid domains becomes more difficult. Users spend significant amounts of time in identifying the
      parametric form of the random variables (Gaussian, Poisson, Logit, etc.) involved and learning the mixed
      models. To make this difficult task easier, we propose the first trainable probabilistic deep
      architecture for hybrid domains that features tractable queries. It is based on Sum-Product Networks (SPNs)
      with piecewise polynomial leave distributions together with novel nonparametric decomposition and
      conditioning steps using the Hirschfeld-Gebelein-R√©nyi Maximum Correlation Coefficient. This relieves
      the user from deciding a-priori the parametric form of the random variables but is still expressive
      enough to effectively approximate any continuous distribution and permits efficient learning and inference.
      Our empirical evidence shows that the architecture, called Mixed SPNs, can indeed capture complex distributions
      across a wide range of hybrid domains.},
    Pages = {},
    Title = {Mixed Sum-Product Networks: A Deep Architecture for Hybrid Domains},
      Crossref = {https://github.com/SPFlow/SPFlow},
    Url = {./papers/molina2018aaai_mspns.pdf},
    Year = {2018}}

    @inproceedings{molina2018aaai_cdn,
      Anote = {./images/molina2018aaai_cdn.png},
      Author = {Alejandro  Molina and Alexander Munteanu and Kristian Kersting},
      Booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)},
      Keywords = {Core Sets, eps-subspace embedding, Dependency Networks, Graphical Models, Generative Model, Structure Learning, Gaussian, Poisson},
      Note = {Many applications infer the structure of a probabilistic graphical model from data to elucidate the
        relationships between variables. But how can we train graphical models on a massive data set? In this paper,
        we show how to construct coresets ‚Äî compressed data sets which can be used as proxy for the original data and
        have provably bounded worst case error ‚Äî for Gaussian dependency networks (DNs), i.e., cyclic directed graphical
        models over Gaussians, where the parents of each variable are its Markov blanket. Specifically, we prove that
        Gaussian DNs admit coresets of size independent of the size of the data set. Unfortunately, this does not
        extend to DNs over members of the exponential family in general. As we will prove, Poisson DNs do not admit
        small coresets. Despite this worst-case result, we will provide an argument why our coreset construction for
        DNs can still work well in practice on count data. To corroborate our theoretical results, we empirically
        evaluated the resulting Core DNs on real data sets. The results demonstrate significant gains over no or naive
        sub-sampling, even in the case of count data.},
      Pages = {},
      Title = {Core Dependency Networks},
      Url = {./papers/molina2018aaai_cdns.pdf},
      Year = {2018}}

      @inproceedings{vergari2018aaai_spae,
        Anote = {./images/vergari2018aaai_spae.png},
        Booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence (AAAI); also in the Working Notes of the ICML 2018 Workshop on Tractable Probabilistic Models (TPM)},
        Keywords = {Auto Encoder, MPE, Deep Learning, Sum Product Networks, Graphical Models, Generative Model, Embedding},
        Author = {Antonio Vergari and Robert Peharz and Nicola Di Mauro and Alejandro  Molina and Kristian Kersting and Floriana Esposito},
        Note = {Sum-Product Networks (SPNs) are a deep probabilistic architecture that up to now has been successfully
          employed for tractable inference. Here, we extend their scope towards unsupervised representation learning:
          we encode samples into continuous and categorical embeddings and show that they can also be decoded back into
          the original input space by leveraging MPE inference. We characterize when this Sum-Product Autoencoding (SPAE)
          leads to equivalent reconstructions and extend it towards dealing with missing embedding information.
          Our experimental results on several multi-label classification problems demonstrate that SPAE is competitive
          with state-of-the-art autoencoder architectures, even if the SPNs were never trained to reconstruct their inputs.},
        Pages = {},
        Title = {Sum-Product Autoencoding: Encoding and Decoding Representations using Sum-Product Networks},
        Url = {./papers/vergari2018aaai_spae.pdf},
        Year = {2018}}

        @inproceedings{kaur2017ilp_rrbms,
          Anote = {./images/kaur2017ilp_rrbms.png},
          Booktitle = {Proceedings of the 27th international Conference on Inductive Logic Prorgamming (ILP)},
          Keywords = {Relational Learning, Restricted Boltzman Maschine, Deep Learning, Random Walks},
          Author = {Navdeep Kaur and Gautam Kunapuli and Tushar Khot and Kristian Kersting and William Cohen and Sriraam Natarajan. },
          Note = {We consider the problem of learning Boltzmann machine classifiers from relational data. Our goal is to extend the deep belief framework of RBMs to
            statistical relational models. This allows one to exploit the feature hierarchies and the non-linearity inherent in RBMs over the rich representations used in statistical
            relational learning (SRL). Specifically, we use lifted random walks to generate features for predicates that are then used to construct the observed features in the
            RBM in a manner similar to Markov Logic Networks. We show empirically that this method of constructing an RBM is comparable or better than the state-of-the-art
            probabilistic relational learning algorithms on four relational domains.},
          Pages = {},
          Title = {Relational Restricted Boltzmann Machines: A Probabilistic Logic Learning Approach},
          Url = {./papers/kaur2017ilp_rrbms.pdf},
          Year = {2017}}


  @article{wahabzada2017plosone,
    Anote = {./images/wahabzada2017plosone_woundHealing.png},
    Author = {Mirwaes Wahabzada and Manuela Besser and Milad Khosravani and Matheus Thomas
Kuska and Kristian Kersting and Anne-Katrin Mahlein and Ewa Sturmer},
    Journal = {PLoS One},
    Keywords = { Wound Healing, Hyperspectral Imaging, Hierachical Decomposition, FastMap projection, Convex NMF, Machine Learning},
    Note = {Wound healing is a complex and dynamic process with different distinct and
overlapping phases from homeostasis, inflammation and proliferation to remodelling.
Monitoring the healing response of injured tissue is of high importance for basic
research and clinical practice. In traditional application, biological markers characterize
normal and abnormal wound healing. Understanding functional relationships of these
biological processes is essential for developing new treatment strategies. However, most
of the present techniques (in vitro or in vivo) include invasive microscopic or analytical
tissue sampling. In the present study, a non-invasive alternative for monitoring
processes during wound healing is introduced. Within this context, hyperspectral
imaging (HSI) is an emerging and innovative non-invasive imaging technique with
different opportunities in medical applications. HSI acquires the spectral reflectance of
an object, depending on its biochemical and structural characteristics. For analysing
the complex hyperspectral data, an efficient unsupervised approach for clustering
massive hyperspectral data was designed, based on efficient hierarchical decomposition
of spectral information according to archetypal data points. It represents, to the best of
our knowledge, the first application of an advanced Data Mining approach in context of
non-invasive analysis of wounds using hyperspectral imagery.},
    Pages = {},
    Publisher = {Public Library of Science},
    Title = {Monitoring Wound Healing in a 3D Wound Model by Hyperspectral Imaging and Efficient Clustering},
    Url = {./papers/wahabzada2017plosone_woundHealing.pdf},
    Volume = {},
    Year = {2017},
}


  @inproceedings{yang2017bibm,
    Anote = {./images/yang2017bibm.png},
    Author = {Shuo Yang and Fabian Hadiji and Kristian Kersting and Shaun Grannis and Sriraam Natarajan },
    Booktitle = {Proceedings of the IEEE Conference on Bioinformatics and Biomedicine (BIBM)},
    Keywords = {Angioplasty, Electronic Health Records, Predicting the number of Procedures, Dependency Networks, Multinomials, Poisson, Functional Gradient Boosting },
    Note = {We consider the problem of predicting the number of coronary artery procedures, specifically Angioplasty, on patients by learning from Electronic Health Record
      (EHR) data. To model this realistic task, we consider two types of exponential family members - multinomial distribution and Poisson distribution that model
      the target variable as categorical-valued and count-valued respectively. From the perspective of exponential family, we derive functional gradient boosting for these
      two distributions and analyze their assumptions with real EHR data. Our empirical results show that Poisson models appear to be more faithful for modeling the
      number of procedures given historical medical conditions about the target patients.},
    Pages = {},
    Title = {Modeling Heart Procedures from EHRs: An Application of Exponential Families},
    Url = {./papers/yang2017bibm.pdf},
    Year = {2017},
    Bdsk-Url-1 = {./papers/yang2017bibm.pdf}}

  @inproceedings{morris2017stochasticWL,
    Anote = {./images/morris2017stochasticWL.png},
    Author = {Christopher Morris and Kristian Kersting and Petra Mutzel },
    Booktitle = {Proceedings of the IEEE Conference on Data Mining (ICDM)},
    Keywords = {Weisfeiler Lehman, Rademacher Averages, Stochastic, k-dimensional, Graph Kernels, Bounds},
    Note = {Most state-of-the-art graph kernels only take local
graph properties into account, i.e., the kernel is computed with
regard to properties of the neighborhood of vertices or other
small substructures only. On the other hand, kernels that do
take global graph properties into account may not scale well to
large graph databases. Here we propose to start exploring the
space between local and global graph kernels, striking the balance
between both worlds. Specifically, we introduce a novel graph
kernel based on the k-dimensional Weisfeiler-Lehman algorithm,
and show that it takes local as well as global properties into
account. Unfortunately, the k-dimensional Weisfeiler-Lehman
algorithm scales exponentially in k. Consequently, we devise a
stochastic version of the kernel with provable approximation
guarantees using conditional Rademacher averages. On bounded degree
graphs, it can even be computed in constant time. We
support our theoretical results with experiments on several graph
classification benchmarks, showing that our kernels often outperform
the state-of-the-art in terms of classification accuracies.},
    Pages = {},
    Title = {Glocalized Weisfeiler-Lehman Graph Kernels: Global-Local Feature Maps of Graphs},
    Url = {https://arxiv.org/pdf/1703.02379.pdf},
    Year = {2017},
    Bdsk-Url-1 = {https://arxiv.org/pdf/1703.02379.pdf}}


  @article{kuska2017spectral,
    Anote = {./images/kuska2017spectral.jpg},
    Author = {Matheus Thomas Kuska and Anna Brugger and Stefan Thomas and Mirwaes Wahabzada and Kristian Kersting and Erich-Christian Oerke and Ulrike Steiner and Anne-Katrin Mahlein},
    Journal = {Phytopathology},
    Keywords = { Plant Phenotyping, Plant Diseases, Resistance, Hyperspectral Imaging, Machine Learning},
    Note = {Differences in early plant-pathogen interactions are mainly characterized by using destructive methods. Optical
      sensors are advanced techniques for phenotyping host-pathogen interactions on different scales and for detecting subtle plant resistance
      responses against pathogens. A microscope with a hyperspectral camera was used to study interactions between Blumeria graminis f. sp. hordei
      and barley (Hordeum vulgare) genotypes with high susceptibility or resistance due to hypersensitive response (HR) and papilla formation.
      Qualitative and quantitative assessment of pathogen development was used to explain changes in hyperspectral signatures.
      Within 48 hours after inoculation, genotype specific changes in the green and red range (500-690 nm) and a blue shift of the red edge
      inflection point were observed. Manual analysis indicated resistance-specific reflectance patterns from one to three days after inoculation.
      These changes could be linked to host plant modifications depending on individual host-pathogen interactions, respectively. Retrospective analysis of hyperspectral
      images revealed spectral characteristics of HR against B. graminis f. sp. hordei. For early HR detection, an advanced data mining approach localized HR spots
      before they became visible on the RGB images derived from hyperspectral imaging. The link among processes during pathogenesis and host resistance to changes
      in hyperspectral signatures, provide evidence that sensor based phenotyping is suitable to advance time-consuming and cost-expensive visual rating of plant disease resistances.},
    Pages = {1388-1398},
    Publisher = {APS},
    Title = {Spectral patterns reveal early resistance reactions of barley against Blumeria graminis f. sp. hordei},
    Url = {http://apsjournals.apsnet.org/doi/10.1094/PHYTO-04-17-0128-R},
    Volume = {107},
    numer = {11},
    Year = {2017},
    Bdsk-Url-1 = {http://apsjournals.apsnet.org/doi/10.1094/PHYTO-04-17-0128-R}}


@article{mahlein2017labfield,
  Anote = {./images/labfield2017.png},
  Author = {Anne-Katrin Mahlein and Matheus Kuska and Stefan Thomas and
    David Bohnenkamp and Elias Alisaac and Jens Behmann and Mirwaes Wahabzada and Kristian Kersting},
  Journal = {Advances in Animal Biosciences},
  Keywords = { Plant Phenotyping, Plant Diseases, Resistance, Hyperspectral Imaging, Machine Learning},
  Note = {The detection and identification of plant diseases is a fundamental task in sustainable crop production. An accurate estimate of disease incidence,
    disease severity and negative effects on yield quality and quantity is important for precision crop production, horticulture, plant breeding or fungicide
    screening as well as in basic and applied plant research. Particularly hyperspectral imaging of diseased plants offers insight into processes during pathogenesis.
    By hyperspectral imaging and subsequent data analysis routines, it was possible to realize an early detection, identification and quantification of different relevant plant diseases.
    Depending on the measuring scale, even subtle processes of defence and resistance mechanism of plants could be evaluated. Within this scope, recent results
    from studies in barley, wheat and sugar beet and their relevant foliar diseases will be presented.},
  Pages = {238-243},
  Publisher = {Cambridge University Press},
  Title = {Plant disease detection by hyperspectral imaging: from the lab to the field},
  Url = {https://www.cambridge.org/core/journals/advances-in-animal-biosciences/article/plant-disease-detection-by-hyperspectral-imaging-from-the-lab-to-the-field/AA32F5F78BB706729A20FA5289B34F21},
  Volume = {8(Proceedings of the European Conference on Precision Agriculture 2017)},
  Year = {2017},
  Bdsk-Url-1 = {https://www.cambridge.org/core/journals/advances-in-animal-biosciences/article/plant-disease-detection-by-hyperspectral-imaging-from-the-lab-to-the-field/AA32F5F78BB706729A20FA5289B34F21}}

@article{hallau2018cellphone,
  Anote = {./images/hallau2018cellphone.png},
  Author = {Lisa Hallau and Marion Neumann and Benjamin Klatt and Benno Kleinhenz and Thomas Klein and Christian Kuhn and Manfred R√∂hrig and Christian Bauckhage and Kristian Kersting and Anne-Katrin Mahlein and Ulrike Steiner and Erich-Christian Oerke},
  Journal = {Plant Pathology},
  Keywords = {Plant Phenotyping, Smartphone, Classification, Plant Disease},
  Note = {Cercospora leaf spot (CLS) poses a high economic risk to sugar beet production due to its potential to greatly reduce yield and quality. For
    successful integrated management of CLS a rapid and accurate means of identifying the disease is required. Accurate diagnosis on the basis of typical symptoms is
    often compromised by the inability to accurately differentiate CLS symptoms from those caused by other foliar pathogens of sugar beet of varying significance,
    or from abiotic stress. An automated detection and classification of CLS and similar leaf diseases, enabling a reliable basis for decisions in disease control,
    would be an alternative to molecular and serological methods. This paper presents an algorithm - based on a RGB-image database captured with smartphone cameras ‚Äì for the identification of sugar beet leaf diseases that could be used by extension services and farmers in the field. },
  Pages = {399-410},
  Publisher = {Wiley},
  Title = {Automated identification of sugar beet diseases using smartphones},
  Url = {https://doi.org/10.1111/ppa.12741},
  Volume = {67},
  Number = {2},
  Year = {2018},
  Bdsk-Url-1 = {https://doi.org/10.1111/ppa.12741}}

@incollection{thimm2017lfu,
  Anote = {./images/thimm2017lfu.png},
  Author = {Matthias Thimm and Kristian Kersting},
  Booktitle = {Working Notes of the IJCAI 2017 Workshop on the Logical Foundations for Uncertainty and Machine Learning (LFU)},
  Keywords = {Argumentation, Frequent Itemsets, Classification},
  Note = {In this position paper, we envisage to significantly
generalize this reasoning aspect of machine learning towards
the use of computational models of argumentation, a popular approach to commonsense reasoning,
for reasoning within machine learning. More concretely, we
consider the following two-step classification approach. In
the first step, rule learning algorithms are used to extract frequent
patterns and rules from a given data set. The output of
this step comprises a huge number of rules (given fairly low
confidence and support parameters) and these cannot directly
be used for the purpose of classification as they are usually inconsistent
with one another. Therefore, in the second step, we
interpret these rules as the input for approaches to structured
argumentation. Using the argumentative
inference procedures of these approaches and given a new
observation, the classification of the new observation is determined
by constructing arguments on top of these rules for
the different classes and determining their justification status.},
  Pages = {},
  Publisher = {},
  Title = {Towards Argumentation-based Classification},
  Url = {./papers/thimm2017lfu.pdf},
  Year = {2017},
  Bdsk-Url-1 = {./papers/thimm2017lfu.pdf}}

@inproceedings{xu2017memoryNetworks,
  Anote = {./images/xu2017memoryNetworks.png},
  Author = {Zhao Xu and Romain Vial and Kristian Kersting },
  Booktitle = {Proceedings of the European Conference on Machine Learning and Principles of Knowledge Discovery in Databases (ECML PKDD)},
  Keywords = {Memory Models, Graph Laplacian, Attention Model, Deep Learning, Sentiment Analysis},
  Note = {Memory networks model information and knowledge as memories which can be manipulated for prediction, inference and reasoning
on the basis of attention mechanism in neural networks. In many cases,
there exist complicated relations between memories, by which the memories are linked together into graphs. Typical examples
include dependency tree of a sentence and knowledge graph in a dialogue system. In
this paper, we present graph enhanced memory networks to integrate
the relational information between memories into deep neural networks.
Our approach can exploit two types of attentions, graph- and content-based ones, to effectively identify the important memories for the given
question, and thus leads to a better inference and reasoning about the
final response. We demonstrate the eectiveness of the proposed approach
with an interesting application on aspect based sentiment classication.
The empirical analysis on real data shows the advantages of incorporating relational dependencies into the memory networks.},
  Pages = {},
  Title = {Graph Enhanced Memory Networks for Sentiment Analysis},
  Url = {./papers/xu2017memoryNetworks.pdf},
  Year = {2017},
  Bdsk-Url-1 = {./papers/xu2017memoryNetworks.pdf}}


  @inproceedings{gurevych2017interactive,
  	Anote = {./images/interactive-data-analytics.png},
  	Author = {Iryna Gurevych and Christian M. Meyer and Carsten Binnig and Johannes F√ºrnkranz and Kristian Kersting and Stefan Roth and Edwin Simpson },
  	Booktitle = {Proceedings of the 18th International Conference on Computational Linguistics and Intelligent Text Processing (CICLing)},
  	Keywords = {Interactive Machine Learning, Digitial Humanities},
  	Note = {In this vision paper, we argue that current solutions to data analytics
are not suitable for complex tasks from the humanities, as they are agnostic
of the user and focused on static, predefined tasks with large-scale benchmarks.
Instead, we believe that the human must be put into the loop to address small
data scenarios that require expert domain knowledge and fluid, incrementally defined
tasks, which are common for many humanities use cases.},
  	Pages = {},
  	Title = {Interactive Data Analytics for the Humanities},
  	Url = {./papers/cicling2017-interactive-data-analytics.pdf},
  	Year = {2017},
  	Bdsk-Url-1 = {./papers/cicling2017-interactive-data-analytics.pdf}}



      @article{kriege2019unifying,
      	Anote = {./images/expliciteFMgraphs.png},
      	Author = {Nils Kriege and Marion Neumann and Christopher Morris and Kristian Kersting and Petra Mutzel},
        Journal = {Data Mining and Knowledge Discovary (DAMI) 33(6): 1505-1547; a previous version also as arXiv preprint arXiv:1703.00676},
      	Keywords = {Graph Kernels, Explicit Feature Map, Kernel Trick, Phase Transition},
      	Note = {Non-linear kernel methods can be approximated by fast linear ones using suitable explicit feature maps
allowing their application to large scale problems. We investigate how convolution kernels for structured
data are composed from base kernels and construct corresponding feature maps. On this basis we propose
exact and approximative feature maps for widely used graph kernels based on the kernel trick. We analyze
for which kernels and graph properties computation by explicit feature maps is feasible and actually more
efficient. In particular, we derive approximative, explicit feature maps for state-of-the-art kernels
supporting real-valued attributes including the GraphHopper and graph invariant kernels. In extensive
experiments we show that our approaches often achieve a classification accuracy close to the exact
methods based on the kernel trick, but require only a fraction of their running time. Moreover, we propose
and analyze algorithms for computing random walk, shortest-path and subgraph matching kernels by
explicit and implicit feature maps. Our theoretical results are confirmed experimentally by observing a
phase transition when comparing running time with respect to label diversity, walk lengths and subgraph
size, respectively.},
      	Title = {A Unifying View of Explicit and Implicit Feature Maps of Graph Kernels},
      	Url = {https://link.springer.com/article/10.1007/s10618-019-00652-0},
      	Year = {2019}}

  @misc{kriege2017unifying,
  	Anote = {./images/expliciteFMgraphs.png},
  	Author = {Nils Kriege and Marion Neumann and Christopher Morris and Kristian Kersting and Petra Mutzel},
  	Howpublished = {arXiv preprint arXiv:1703.00676},
  	Keywords = {Graph Kernels, Explicit Feature Map, Kernel Trick, Phase Transition},
  	Note = {We investigate how general convolution kernels are composed from base kernels and construct
    corresponding feature maps. We apply our results to widely used graph kernels and analyze for which kernels
    and graph properties computation by explicit feature maps is feasible and actually more efficient than
    implicite computations. In particular, we derive feature maps for random walk and subgraph matching kernels
    and apply them to real-world graphs with discrete labels.},
  	Title = {A Unifying View of Explicit and Implicit Feature Maps for Structured Data: Systematic Studies of Graph Kernels},
  	Url = {https://arxiv.org/pdf/1703.00676.pdf},
  	Year = {2017},
  	Bdsk-Url-1 = {https://arxiv.org/pdf/1703.00676.pdf}}

  @misc{morris2017global,
  	Anote = {./images/globalWL2017.png},
  	Author = {Christopher Morris and Kristian Kersting and Petra Mutzel},
  	Howpublished = {arXiv preprint arXiv:1703.02379},
  	Keywords = {Graph Kernel, k-Dimensional Weifeiler Lehman, Randomization},
  	Note = {We introduce a novel graph kernel based on the k-dimensional Weisfeiler-Lehman algorithm, and show that it takes local as well as global properties into account. Unfortunately, the k-dimensional Weisfeiler-Lehman scales exponentially in k. Consequently, we devise a stochastic version of the kernel with provable approximation guarantees using conditional Rademacher averages. On bounded-degree graphs, it can even be computed in constant time. We support our theoretical results with experiments on several graph classification benchmarks, showing that our kernels often outperform the state-of-the-art in terms of classification accuracies.},
  	Title = {Global Weisfeiler-Lehman Graph Kernels},
  	Url = {https://arxiv.org/pdf/1703.02379.pdf},
  	Year = {2017},
  	Bdsk-Url-1 = {https://arxiv.org/pdf/1703.02379.pdf}}

  @inproceedings{molina2017poisson,
  	Anote = {./images/molina2017aaai_pspn.png},
  	Author = {Alejandro Molina and Sriraam Natarajan and Kristian Kersting},
  	Booktitle = {Proceedings of the 31st {AAAI} Conference on Artificial Intelligence (AAAI)},
  	Keywords = {Multivariate Poisson, Sum Product Networks, Efficient Inference, Structure Learning, Deep Learning},
  	Note = {Multivariate count data are pervasive in science in the form
of histograms, contingency tables and others. Previous work
on modeling this type of distributions do not allow for fast
and tractable inference. In this paper we present a novel Poisson
graphical model, the first based on sum product networks,
called PSPN, allowing for positive as well as negative dependencies.
We present algorithms for learning tree PSPNs from
data as well as for tractable inference via symbolic evaluation.
With these, information-theoretic measures such as entropy,
mutual information, and distances among count variables can
be computed without resorting to approximations. Additionally,
we show a connection between PSPNs and LDA, linking
the structure of tree PSPNs to a hierarchy of topics. The
experimental results on several synthetic and real world datasets
demonstrate that PSPN often outperform state-of-the-art
while remaining tractable.},
  	Pages = {1199--1205},
  	Title = {Poisson Sum-Product Networks: A Deep Architecture for Tractable Multivariate Poisson Distributions},
  	Url = {http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14530/14416},
      Crossref = {https://github.com/SPFlow/SPFlow},
  	Year = {2017},
  	Bdsk-Url-1 = {http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14530/14416}}

  @inproceedings{xu2017anomaly,
  	Anote = {./images/ijcai2017anomaly.png},
  	Author = {Zhao Xu and Kristian Kersting and Lorenzo {von Ritter}},
  	Booktitle = {Proceedings of the 26th International Joint Conference on Artificial Intelligence (IJCAI)},
  	Keywords = {Anomaliy Detection, Streams, Gaussian Processes, OLAD, Online},
  	Note = {Detecting anomalous activities from time series is critical for enhancing availability and security of systems in many domains. In real applications, the time series often arrive sequentially without fix length, and usually only a single scan is allowed through the data. In such situations, classical batch learning methods as well as retrospective segmentation methods would be less flexible for anomaly detection. Instead, we propose an online nonparametric Bayesian method OLAD for anomaly detection in streaming time series collected continuously. The method can effectively learn the underlying dynamics of anomaly-contaminated heavytailed time series and identify potential anomalous events. Empirical evaluations on both synthetic and real-world datasets demonstrates the effectiveness of our method.},
  	Title = {Robust Online Anomaly Detection for Streaming Time Series},
    Url = {./papers/xu2017anomaly.pdf},
  	Year = {2017}}

  @inproceedings{mladenovBK17,
  	Anote = {./images/adds_aaai2017.png},
  	Author = {Martin Mladenov and Vaishak Belle and Kristian Kersting},
  	Booktitle = {Proceedings of the 31st {AAAI} Conference on Artificial Intelligence (AAAI)},
  	Keywords = {Symbolic Numerical Inference, Algebraic Decision Diagrams, Matrix-free Optimization},
  	Note = {Numerical optimization is arguably the most prominent computational framework in machine learning and AI. It can be seen as an assembly language for hard combinatorial problems ranging from classification and regression in learning, to computing optimal policies and equilibria in decision theory, to entropy minimization in information sciences. Unfortunately, specifying such problems in complex domains involving relations, objects and other logical dependencies is cumbersome at best, requiring considerable expert knowledge, and solvers require models to be painstakingly reduced to standard forms. To overcome this, we introduce a rich modeling framework for optimization problems that allows convenient codification of symbolic structure. Rather than reducing this symbolic structure to a sparse or dense matrix, we represent and exploit it directly using algebraic decision diagrams (ADDs). Combining efficient ADD-based matrix-vector algebra with a matrix-free interior-point method, we develop an engine that can fully leverage the structure of symbolic representations to solve convex linear and quadratic optimization problems. We demonstrate the flexibility of the resulting symbolic-numeric optimizer on decision making and compressed sensing tasks with millions of non-zero entries.},
  	Pages = {1199--1205},
  	Title = {The Symbolic Interior Point Method},
  	Url = {http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14966/13901},
  	Year = {2017},
  	Bdsk-Url-1 = {http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14966/13901}}

  @inproceedings{mladenovKK17,
  	Anote = {./images/lqp_aaai2017.png},
  	Author = {Martin Mladenov and Leonard Kleinhans and Kristian Kersting},
  	Booktitle = {Proceedings of the 31st {AAAI} Conference on Artificial Intelligence (AAAI)},
  	Keywords = {Statistical Relational AI, Lifted Inference, Symmetries, Quadratic Programs, Weisfeiler Lehmann, SVM},
  	Note = {Symmetry is the essential element of lifted inference that has
recently demonstrated the possibility to perform very efficient
inference in highly-connected, but symmetric probabilistic
models. This raises the question, whether this holds for optimization
problems in general. Here we show that for a large
class of optimization methods this is actually the case. Specifically,
we introduce the concept of fractional symmetries of
convex quadratic programs (QPs), which lie at the heart of
many AI and machine learning approaches, and exploit it to
lift, i.e., to compress QPs. These lifted QPs can then be tackled
with the usual optimization toolbox (off-the-shelf solvers, cutting
plane algorithms, stochastic gradients etc.). If the original
QP exhibits symmetry, then the lifted one will generally be
more compact, and hence more efficient to solve.},
  	Pages = {2350--2356},
  	Title = {Lifted Inference for Convex Quadratic Programs},
  	Url = {http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14505/14415},
  	Year = {2017},
  	Bdsk-Url-1 = {http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14505/14415}}

  @article{serong2017publizistik,
  	Anote = {./images/publizistik2017.png},
  	Author = {Julia Serong and Lars Koppers and Edith Luschmann and Alejandro Molina and Kristian Kersting and J√∂rg Rahnenf√ºhrer and Holger Wormer},
  	Journal = {Publizistik},
  	Keywords = {Datajournalism},
  	Note = {Die Qualit√§t der Wissenschaftskommunikation in Deutschland ist Gegenstand intensiver Debatten. Empirische Daten zu Inhalt und Umfang sind jedoch rar. Hier pr√§sentieren wir eine deskriptiven L√§ngsschnittstudie zu 300000 Pressemitteilungen von Forschungseinrichtungen, die vom idw im Zeitraum von 1995 bis 2015 verbreitet wurden.},
  	Pages = {1--26},
  	Publisher = {Springer},
  	Title = {√ñffentlichkeitsorientierung von Wissenschaftsinstitutionen und Wissenschaftsdisziplinen: Eine L√§ngsschnittanalyse des ‚ÄûInformationsdienstes Wissenschaft`` (idw) 1995--2015},
  	Url = {http://em.rdcu.be/wf/click?upn=KP7O1RED-2BlD0F9LDqGVeSOAz6NZGoC90jKX1m6R-2FH8A-3D_mvz2iKCHyENW-2B9fXEOaXvtY05-2B3qXrkZqr8QzxVXjpeZJttbYwlgYnD-2By3ZoZ74wOT-2BSRPCDS-2BrKBgqsBFrY414ZP3z9u-2BeShpYWS-2B2pznbeohzp6tA-2Be9M0NvOrWCleTEBZxpZsgXFl8ZVdh-2F158DlrSUeZA7qzPR-2BeCkTO3QfUhyJmNNnaDl4j03FVaI0lSCL-2FugbPzo6cEQIm4CczT2Yr39qxE-2FIPnvuQ0nQZI9msmoX6-2BiiZcwcOcvvjMG8KPk2Hw-2BOdse-2Fww0WqYm3sHr0WxsHuCnlKWyEnR2qmCkw-3D},
  	Volume = {132},
  	Year = {2017},
  	Bdsk-Url-1 = {https://link.springer.com/article/10.1007/s11616-017-0336-6}}

  @article{kersting2017relational,
  	Anote = {./images/rlp_aij2017.png},
  	Author = {Kristian Kersting and Martin Mladenov and Pavel Tokmakov},
  	Journal = {Artificial Intelligence (AIJ)},
  	Keywords = {Relational Linear Programs, Statistical Relational AI, Lifted Inference, Symmetries, Weisfeiler Lehmann},
  	Note = {We propose relational linear programs. Together with a logical knowledge base, effectively a logic program consisting of logical facts and rules, a relational LP induces a ground LP. This ground LP is solved using lifted linear programming. That is, symmetries within the ground LP are employed to reduce its dimensionality, and the reduced program is solved using any off-the-shelf LP solver. We illustrate this empirically by experiments on approximate inference in Markov logic networks using LP relaxations, on solving Markov decision processes, and on collective inference using LP support vector machines.},
  	Pages = {188-216},
  	Publisher = {Elsevier},
  	Title = {Relational linear programming},
  	Url = {http://www.sciencedirect.com/science/article/pii/S0004370215001010},
  	Volume = {244},
  	Year = {2017},
  	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0004370215001010}}

  @article{alfeld2017simplex,
  	Anote = {./images/microchemical2017.png},
  	Author = {Matthias Alfeld and Mirwaes Wahabzada and Christian Bauckhage and Kristian Kersting and Geert {van der Snickt} and Petria Noble and Koen Janssens and Gerd Wellenreuther and Gerald Falkenberg},
  	Journal = {Microchemical Journal},
  	Keywords = {Interpretable Matrix Factorization, Simplex Volume Maximization, Material Science},
  	Note = {Matrix factorization, the representation of data sets by bases (or loads) and coefficient (or score) images is long used to support the interpretation of complex data sets. We propose in this publication Simplex Volume Maximization (SiVM) for the analysis of X-ray fluorescence (XRF) imaging data sets. SiVM selects archetypical data points that represents the data set and thus provides easily understandable bases, preserves the non-negative character of XRF data sets and has low demands concerning computing resources. We apply SiVM on an XRF data set of Hans Memling's Portrait of a man from the Lespinette family from the collection of the Mauritshuis (The Hague, NL) and discuss capabilities and shortcomings of SiVM.},
  	Pages = {179-184},
  	Publisher = {Elsevier},
  	Title = {Simplex Volume Maximization (SiVM): A matrix factorization algorithm with non-negative constrains and low computing demands for the interpretation of full spectral X-ray fluorescence imaging data},
  	Url = {http://www.sciencedirect.com/science/article/pii/S0026265X16304374},
  	Volume = {132},
  	Year = {2017},
  	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0026265X16304374}}

  @inproceedings{morris2016faster,
  	Anote = {./images/icdm2016kernel.png},
  	Author = {Christopher Morris and Nils Kriege and Kristian Kersting and Petra Mutzel},
  	Booktitle = {Proceedings of the IEEE International Conference on Data Mining (ICDM)},
  	Keywords = {Graph Kernels, Continuous Attributes, Hashing},
  	Note = {While state-of-the-art kernels for graphs with discrete labels scale well to graphs with thousands of nodes, the few existing kernels for graphs with continuous attributes, unfortunately, do not scale well. To overcome this limitation, we present hash graph kernels, a general framework to derive kernels for graphs with continuous attributes from discrete ones. The idea is to iteratively turn continuous attributes into discrete labels using randomized hash functions. We illustrate hash graph kernels for the Weisfeiler-Lehman subtree kernel and for the shortest-path kernel. The resulting novel graph kernels are shown to be, both, able to handle graphs with continuous attributes and scalable to large graphs and data sets. This is supported by our theoretical analysis and demonstrated by an extensive experimental evaluation.},
  	Title = {Faster Kernels for Graphs with Continuous Attributes via Hashing},
  	Url = {https://arxiv.org/pdf/1610.00064.pdf},
  	Year = {2016},
  	Bdsk-Url-1 = {https://arxiv.org/pdf/1610.00064.pdf}}

  @inproceedings{yang2016learning,
  	Anote = {./images/yang_aaai2016.png},
  	Author = {Shuo Yang and Tushar Khot and Kristian Kersting and Sriraam Natarajan},
  	Booktitle = {Proceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI)},
  	Keywords = {Continous Time Bayesian Networks, Statistical Relational Learning, Functional Gradient Boosting},
  	Note = {We develop Relational Continuous-Time Bayesian Networks (RCTBNs). They feature a nonparametric learning method that allows for efficiently learning the complex dependencies and their strengths simultaneously from sequence data. Our experimental results demonstrate that RCTBNs can learn as effectively as state-of-the-art approaches for propositional tasks while modeling relational tasks faithfully.},
  	Title = {Learning Continuous-Time Bayesian Networks in Relational Domains: A Non-Parametric Approach},
  	Url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/11955/11871},
  	Year = {2016},
  	Bdsk-Url-1 = {https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/11955/11871}}

  @inproceedings{das2016scaling,
  	Anote = {./images/16-SDM-fig.jpg},
  	Author = {Mayukh Das and Yuqing Wu and Tushar Khot and Kristian Kersting and Sriraam Natarajan},
  	Booktitle = {Proceedings of the SIAM Conference on Data Mining (SDM)},
  	Keywords = {Lifted Inference, Graph Databases, Approximate Counting},
  	Note = {Over the past decade, exploiting relations and symmetries within probabilistic models has been proven to be surprisingly effective at solving large scale data mining problems. One of the key operations inside these lifted approaches is counting - be it for parameter/structure learning or for efficient inference. This paper demonstrates that `Compilation to Graph Databases' could be a practical tool for scaling lifted probabilistic inference and learning methods via counting in graph databases.},
  	Organization = {SIAM},
  	Pages = {738-746},
  	Title = {Scaling Lifted Probabilistic Inference and Learning Via Graph Databases},
  	Url = {http://epubs.siam.org/doi/pdf/10.1137/1.9781611974348.83},
  	Year = {2016},
  	Bdsk-Url-1 = {http://epubs.siam.org/doi/pdf/10.1137/1.9781611974348.83}}

  @incollection{habel2016traffic,
  	Anote = {./images/granularFlow2015.png},
  	Author = {Lars Habel and Alejandro Molina and Thomas Zaksek and Kristian Kersting and Michael Schreckenberg},
  	Booktitle = {Traffic and Granular Flow 2015},
  	Keywords = {Traffic, Multivariate Poisson, Dependeny Networks, Functional Gradient Boosting},
  	Note = {For the real-time microscopic simulation of traffic on a real-world road network, a continuous input stream of empirical data from different locations is usually needed to achieve good results. Traffic flows for example are needed to properly simulate the influence of slip roads and motorway exits. However, quality and reliability of empirical traffic data is sometimes a problem for example because of damaged detectors, transmission errors or simply lane diversions at road works. In this contribution, we attempt to close those data gaps of missing traffic flows with processed historical traffic data. Therefore, we compare a temporal approach based on exponential smoothing with a data-driven approach based on Poisson Dependency Networks.},
  	Pages = {491--498},
  	Publisher = {Springer},
  	Title = {Traffic Simulations with Empirical Data: How to Replace Missing Traffic Flows?},
  	Url = {https://link.springer.com/chapter/10.1007/978-3-319-33482-0_62},
  	Year = {2016},
  	Bdsk-Url-1 = {https://link.springer.com/chapter/10.1007/978-3-319-33482-0_62}}

  @incollection{mladenovHKGK16,
  	Anote = {./images/reloop2016.png},
  	Author = {Martin Mladenov and Danny Heinrich and Leonard Kleinhans and Felix Gonsior and Kristian Kersting},
  	Booktitle = {Working Notes of the AAAI 2016 Workshop on Declarative Learning Based Programming},
  	Keywords = {Relational Linear Programming, Statistical Relational AI, Symmetries, Lifted Inference},
  	Note = {We present RELOOP, a domain-specific language for relational optimization embedded in Python. It allows the user to express relational optimization problems in a natural syntax that follows logic and linear algebra, rather than in the restrictive standard form required by solvers, and can automatically compile the model to a lower-order but equivalent model. Moreover, RELOOP makes it easy to combine relational optimization with high-level features of Python such as loops, parallelism and interfaces to relational databases. RELOOP is available at http://www-ai.cs.uni-dortmund.de/weblab/ static/RLP/html/ along with documentation and examples.},
  	Title = {{RELOOP:} {A} Python-Embedded Declarative Language for Relational Optimization},
  	Url = {http://www.aaai.org/ocs/index.php/WS/AAAIW16/paper/download/12614/12396},
  	Year = {2016},
  	Bdsk-Url-1 = {http://www.aaai.org/ocs/index.php/WS/AAAIW16/paper/download/12614/12396}}

  @incollection{natarajanSWVK16,
  	Anote = {./images/deepSupervision2016.png},
  	Author = {Sriraam Natarajan and Ameet Soni and Anurag Wazalwar and Dileep Viswanathan and Kristian Kersting},
  	Booktitle = {Solving Large Scale Learning Tasks: Challenges and Algorithms - Essays Dedicated to Katharina Morik on the Occasion of Her 60th Birthday},
  	Keywords = {Distance Supervision, Background Knowledge, Statistical Relational Learning, Functional Gradient Boosting},
  	Note = {Most distant supervision methods rely on a given set of propositions as a source of supervision. We propose a different approach: we infer weakly supervised examples for relations from statistical relational models learned by using knowledge outside the natural language task. We argue that this deep distant supervision creates more robust examples that are particularly useful when learning the entire model (the structure and parameters). We demonstrate on several domains that this form of weak supervision improves performance when learning structure.},
  	Pages = {331--345},
  	Title = {Deep Distant Supervision: Learning Statistical Relational Models for Weak Supervision in Natural Language Extraction},
  	Url = {http://dx.doi.org/10.1007/978-3-319-41706-6_18},
  	Year = {2016},
  	Bdsk-Url-1 = {http://dx.doi.org/10.1007/978-3-319-41706-6_18}}

  @incollection{erdmannBKNPMMMR16,
  	Anote = {./images/erdmann2016.png},
  	Author = {Elena Erdmann and Karin Boczek and Lars Koppers and Gerret von Nordheim and Christian P√∂litz and Alejandro Molina and Katharina Morik and Henrik M√ºller and J√∂rg Rahnenf√ºhrer and Kristian Kersting},
  	Booktitle = {Working Notes of the ICML Workshop #Data4Good: Machine Learning in Social Good Applications},
  	Keywords = {Datajournalism},
  	Note = {Migration crisis, climate change or tax havens: Global challenges need global solutions. But agreeing on a joint approach is difficult without a common ground for discussion. Public spheres are highly segmented because news are mainly produced and received on a national level. Gaining a global view on international debates about important issues is hindered by the enormous quantity of news and by language barriers. Media analysis usually focuses only on qualitative research. In this position statement, we argue that it is imperative to pool methods from machine learning, journalism studies and statistics to help bridging the segmented data of the international public sphere, using the Transatlantic Trade and Investment Partnership (TTIP) as a case study.},
  	Title = {Machine Learning meets Data-Driven Journalism: Boosting International Understanding and Transparency in News Coverage},
  	Url = {https://arxiv.org/pdf/1606.05110.pdf},
  	Volume = {abs/1606.05110},
  	Year = {2016},
  	Bdsk-Url-1 = {https://arxiv.org/pdf/1606.05110.pdf}}

  @inbook{kersting2016feeding,
  	Anote = {./images/kersting2016feeding.png},
  	Author = {Kristian Kersting and Christian Bauckhage and Mirwaes Wahabzada and Anne-Katrin Mahlein and Ulrike Steiner and Erich-Christian Oerke and Christoph R√∂mer and Lutz Pl√ºmer},
  	Booktitle = {Computational Sustainability},
  	Keywords = {Sustainability, Plant Phenotyping, Hyperspectral Images, Simplex Volume Maximiaztion},
  	Note = {Modern communication, sensing, and actuator technologies as well as methods from signal processing, pattern recognition, and data mining are increasingly applied in agriculture, ultimately helping to meet the challenge of ``How to feed a hungry world?'' Developments such as increased mobility, wireless networks, new environmental sensors, robots, and the computational cloud put the vision of a sustainable agriculture for anybody, anytime, and anywhere within reach. Unfortunately, data-driven agriculture also presents unique computational problems in scale and interpretability: (1) Data is gathered often at massive scale, and (2) researchers and experts of complementary skills have to cooperate in order to develop models and tools for data intensive discovery that yield easy-to-interpret insights for users that are not necessarily trained computer scientists. On the problem of mining hyperspectral images to uncover spectral characteristic and dynamics of drought stressed plants, we showcase that both challenges can be met and that big data mining can---and should---play a key role for feeding the world, while enriching and transforming data mining.},
  	Pages = {99-120},
  	Publisher = {Springer},
  	Title = {Feeding the World with Big Data: Uncovering Spectral Characteristics and Dynamics of Stressed Plants},
  	Url = {https://link.springer.com/chapter/10.1007%2F978-3-319-31858-5_6},
  	Year = {2016},
  	Bdsk-Url-1 = {https://link.springer.com/chapter/10.1007%2F978-3-319-31858-5_6}}

  @book{laessig2016,
  	Anote = {./images/computational-sustainability.jpg},
  	Author = {J√∂rg L{\"a}ssig and Kristian Kersting and Katharina Morik},
  	Keywords = {Edited Volume, Sustainability},
  	Note = {This book examines the foundations of combining logic and probability into what are called relational probabilistic models. It introduces representations, inference, and learning techniques for probability, logic, and their combinations.},
  	Publisher = {Springer},
  	Series = {Studies in Computational Intelligence},
  	Title = {Computational Sustainability},
  	Url = {https://link.springer.com/book/10.1007%2F978-3-319-31858-5},
  	Volume = {645},
  	Year = {2016},
  	Bdsk-Url-1 = {https://link.springer.com/book/10.1007%2F978-3-319-31858-5}}

  @book{deraedt2016,
  	Anote = {./images/starai.jpg},
  	Author = {Luc {De Raedt} and Kristian Kersting and Sriraam Natarajan and David Poole},
  	Keywords = {Statistical Relational Learning, Statistical Relational AI, Introduction},
  	Note = {This book examines the foundations of combining logic and probability into what are called relational probabilistic models. It introduces representations, inference, and learning techniques for probability, logic, and their combinations.},
  	Publisher = {Morgan {\&} Claypool Publishers},
  	Series = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
  	Title = {Statistical Relational Artificial Intelligence: Logic, Probability, and Computation},
  	Url = {http://www.morganclaypool.com/doi/10.2200/S00692ED1V01Y201601AIM032},
  	Year = {2016},
    Crossref = {https://starling.utdallas.edu/software/boostsrl/wiki/},
  	Bdsk-Url-1 = {http://www.morganclaypool.com/doi/10.2200/S00692ED1V01Y201601AIM032}}

  @article{szymanskiKK16,
  	Anote = {./images/entropy2016.png},
  	Author = {Piotr Szymanski and Tomasz Kajdanowicz and Kristian Kersting},
  	Journal = {Entropy},
  	Keywords = {Multi-Label Classification, Community Detection},
  	Note = {We propose using five data-driven community detection approaches from social networks to partition the label space for the task of multi-label classification as an alternative to random partitioning into equal subsets as performed by RAkELd. We show that in almost all cases these educated guess approaches are more likely to outperform RAkELd in all measures, but Hamming Loss.},
  	Number = {8},
  	Pages = {282},
  	Title = {How Is a Data-Driven Approach Better than Random Choice in Label Space Division for Multi-Label Classification?},
  	Url = {http://www.mdpi.com/1099-4300/18/8/282/pdf},
  	Volume = {18},
  	Year = {2016},
  	Bdsk-Url-1 = {http://www.mdpi.com/1099-4300/18/8/282/pdf}}

  @article{alfeld2016non,
  	Anote = {./images/synchro2016.gif},
  	Author = {Matthias Alfeld and Mirwaes Wahabzada and Christian Bauckhage and Kristian Kersting and Gerd Wellenreuther and Pere Barriobero-Vila and Guillermo Requena and Ulrike Boesenberg and Gerald Falkenberg},
  	Journal = {Journal of Synchrotron Radiation},
  	Keywords = {Interpretable Matrix Factorization, Simplex Volume Maximization, Material Science},
  	Note = {Elemental distribution images acquired by imaging X-ray fluorescence analysis can contain high degrees of redundancy and weakly discernible correlations. In this article near real-time non-negative matrix factorization (NMF) is described for the analysis of a number of data sets acquired from samples of a bi-modal Œ±+Œ≤ Ti-6Al-6V-2Sn alloy. NMF was used for the first time to reveal absorption artefacts in the elemental distribution images of the samples, where two phases of the alloy, namely Œ± and Œ≤, were in superposition. The findings and interpretation of the NMF results were confirmed by Monte Carlo simulation of the layered alloy system. Furthermore, it is shown how the simultaneous factorization of several stacks of elemental distribution images provides uniform basis vectors and consequently simplifies the interpretation of the representation.},
  	Number = {2},
  	Publisher = {International Union of Crystallography},
  	Title = {Non-negative matrix factorization for the near real-time interpretation of absorption effects in elemental distribution images acquired by X-ray fluorescence imaging},
  	Url = {http://scripts.iucr.org/cgi-bin/paper?cnor=hf5308},
  	Volume = {23},
  	Year = {2016},
  	Bdsk-Url-1 = {http://scripts.iucr.org/cgi-bin/paper?cnor=hf5308}}

  @article{wahabzada2016plant,
  	Anote = {./images/srep22482-f1.jpg},
  	Author = {Mirwaes Wahabzada and Anne-Katrin Mahlein and Christian Bauckhage and Ulrike Steiner and Erich-Christian Oerke and Kristian Kersting},
  	Journal = {Scientific Reports (Nature)},
  	Keywords = {Hyperspectral Images, Plant Phenotpying, Latent Dirichlet Allocation, Sustainability},
  	Note = {We present an approach to plant phenotyping that integrates non-invasive sensors, computer vision, as well as data mining techniques and allows for monitoring how plants respond to stress. To uncover latent hyperspectral characteristics of diseased plants reliably and in an easy-to-understand way, we ``wordify'' the hyperspectral images, i.e., we turn the images into a corpus of text documents and apply probabilistic topic models. Based on recent regularized topic models, we demonstrate that one can track automatically the development of three foliar diseases of barley.},
  	Publisher = {Nature Publishing Group},
  	Title = {Plant phenotyping using probabilistic topic models: uncovering the hyperspectral language of plants},
  	Url = {http://www.nature.com/articles/srep22482},
  	Volume = {6},
  	Year = {2016},
  	Bdsk-Url-1 = {http://www.nature.com/articles/srep22482}}

  @article{neumann2016propagation,
  	Anote = {./images/propKernel_mlg2016.png},
  	Author = {Marion Neumann and Roman Garnett and Christian Bauckhage and Kristian Kersting},
  	Journal = {Machine Learning (MLJ)},
  	Keywords = {Graph Kernels, Weisfeiler Lehmann, Belief Propagation, Random Walks, Information Propagation},
  	Note = {We introduce propagation kernels, a general graph-kernel framework for efficiently measuring the similarity of structured data. Propagation kernels are based on monitoring how information spreads through a set of given graphs. They leverage early-stage distributions from propagation schemes such as random walks to capture structural information encoded in node labels, attributes, and edge information.},
  	Number = {2},
  	Pages = {209--245},
  	Publisher = {Springer},
  	Title = {Propagation kernels: efficient graph kernels from propagated information},
  	Url = {https://link.springer.com/content/pdf/10.1007%2Fs10994-015-5517-9.pdf},
  	Volume = {102},
  	Year = {2016},
  	Bdsk-Url-1 = {https://link.springer.com/content/pdf/10.1007%2Fs10994-015-5517-9.pdf}}

  @article{bauckhage2016collective,
  	Anote = {./images/collective2016fat.png},
  	Author = {Christian Bauckhage and Kristian Kersting},
  	Journal = {Foundations and Trends in Web Science},
  	Keywords = {Collective Attention, Memes, Distance Distribution, Generalized Gamma},
  	Note = {Understanding the dynamics of collective human attention has been called a key scientific challenge for the information age. Tackling this challenge, this monograph explores the dynamics of collective attention related to Internet phenomena such as Internet memes, viral videos, or social media platforms and Web-based businesses. We discuss mathematical models that provide plausible explanations as to what drives the apparently dominant dynamics of rapid initial growth and prolonged decline.},
  	Number = {1-2},
  	Pages = {1--136},
  	Publisher = {Now Publishers, Inc.},
  	Title = {Collective Attention on the Web},
  	Url = {http://www.nowpublishers.com/article/Details/WEB-024},
  	Volume = {5},
  	Year = {2016},
  	Bdsk-Url-1 = {http://www.nowpublishers.com/article/Details/WEB-024}}

  @article{leuker2016hyperspectral,
  	Anote = {./images/leuker2016hyperspectral.gif},
  	Author = {Marlene Leucker and Mirwaes Wahabzada and Kristian Kersting and Madlaina Peter and Werner Beyer and Ulrike Steiner and Anne-Katrin Mahlein and Erich-Christian Oerke},
  	Journal = {Functional Plant Biology},
  	Keywords = {Hyperspectral Images, Genetics, Sustainability, Plant Phenotyping, Quantitative Trait Loci, Simplex Volume Maximization},
  	Note = {The quantitative resistance of sugar beet (Beta vulgaris L.) against Cercospora leaf spot (CLS) caused by Cercospora beticola (Sacc.) was characterised by hyperspectral imaging. Two closely related inbred lines, differing in two quantitative trait loci (QTL), which made a difference in disease severity of 1.1--1.7 on the standard scoring scale (1--9), were investigated under controlled conditions. The temporal and spatial development of CLS lesions on the two genotypes were monitored using a hyperspectral microscope. The lesion development on the QTL-carrying, resistant genotype was characterised by a fast and abrupt change in spectral reflectance, whereas it was slower and ultimately more severe on the genotype lacking the QTL. An efficient approach for clustering of hyperspectral signatures was adapted in order to reveal resistance characteristics automatically. The presented method allowed a fast and reliable differentiation of CLS dynamics and lesion composition providing a promising tool to improve resistance breeding by objective and precise plant phenotyping.},
  	Pages = {1-9},
  	Publisher = {Csiro Publishing},
  	Title = {Hyperspectral imaging reveals the effect of sugar beet QTLs on Cercospora leaf spot resistance},
  	Url = {http://www.publish.csiro.au/FP/FP16121},
  	Volume = {44},
  	Year = {2016},
  	Bdsk-Url-1 = {http://www.publish.csiro.au/FP/FP16121}}

  @misc{bauckhage2015maximum,
  	Anote = {./images/bauckhage2015maxent.png},
  	Author = {Christian Bauckhage and Kristian Kersting and Fabian Hadiji},
  	Howpublished = {arXiv preprint arXiv:1501.04232},
  	Keywords = {Generalzed Gamma, Shortest Path, Distances Distribution, Maxumum Entropy, Undirected Network},
  	Note = {Properties of networks are often characterized in terms of features such as node degree distributions, average path lengths, diameters, or clustering coefficients. Here, we study shortest path length distributions. On the one hand, average as well as maximum distances can be determined therefrom; on the other hand, they are closely related to the dynamics of network spreading processes. Because of the combinatorial nature of networks, we apply maximum entropy arguments to derive a general, physically plausible model. In particular, we establish the generalized Gamma distribution as a continuous characterization of shortest path length histograms of networks or arbitrary topology. Experimental evaluations corroborate our theoretical results.},
  	Title = {Maximum Entropy Models of Shortest Path and Outbreak Distributions in Networks},
  	Url = {https://arxiv.org/pdf/1501.04232.pdf},
  	Year = {2015},
  	Bdsk-Url-1 = {https://arxiv.org/pdf/1501.04232.pdf}}

  @inproceedings{natarajan2015effectively,
  	Anote = {./images/ilp2015KBsupervision.png},
  	Author = {Sriraam Natarajan and Jose Picado and Tushar Khot and Kristian Kersting and Christopher Re and Jude Shavlik},
  	Booktitle = {Proceedings of the 25th International Conference on Inductive Logic Programming (ILP)},
  	Keywords = {Domain Knowledge, Statistical Relational Learning, Functional Gradient Boosting},
  	Note = {One of the challenges to information extraction is the requirement of human annotated examples, commonly called gold-standard examples. Many successful approaches alleviate this problem by employing some form of distant supervision, i.e., look into knowledge bases such as Freebase as a source of supervision to create more examples. While this is perfectly reasonable, most distant supervision methods rely on a hand-coded background knowledge that explicitly looks for patterns in text. For example, they assume all sentences containing Person X and Person Y are positive examples of the relation married(X, Y). In this work, we take a different approach -- we infer weakly supervised examples for relations from models learned by using knowledge outside the natural language task. We argue that this method creates more robust examples that are particularly useful when learning the entire information-extraction model (the structure and parameters). We demonstrate on three domains that this form of weak supervision yields superior results when learning structure compared to using distant supervision labels or a smaller set of gold-standard labels.},
  	Pages = {92--107},
  	Publisher = {Springer},
  	Title = {Effectively creating weakly labeled training examples via approximate domain knowledge},
  	Url = {./papers/ilp2015KBsupervision.pdf},
  	Year = {2015},
  	Bdsk-Url-1 = {./papers/ilp2015KBsupervision.pdf}}

  @inproceedings{yang2015modeling,
  	Anote = {./images/aime2015.png},
  	Author = {Shuo Yang and Kristian Kersting and Greg Terry and Jefferey Carr and Sriraam Natarajan},
  	Booktitle = {Proceedings of the Conference on Artificial Intelligence in Medicine in Europe (AIME)},
  	Keywords = {Dynamic Baysian Networks, Coronary Artery Calcification, Behavioral Data},
  	Note = {Cardiovascular disease (CVD) is one of the key causes for death worldwide. We consider the problem of modeling an imaging biomarker, Coronary Artery Calcification (CAC) measured by computed tomography, based on behavioral data. We employ the formalism of Dynamic Bayesian Network (DBN) and learn a DBN from these data. Our learned DBN provides insights about the associations of specific risk factors with CAC levels. Exhaustive empirical results demonstrate that the proposed learning method yields reasonable performance during cross-validation.},
  	Organization = {Springer},
  	Pages = {182--187},
  	Title = {Modeling Coronary Artery Calcification Levels from Behavioral Data in a Clinical Study},
  	Url = {https://pdfs.semanticscholar.org/0a36/1d0f57bb7458d265bebd564e03eba0959dda.pdf},
  	Year = {2015},
  	Bdsk-Url-1 = {https://pdfs.semanticscholar.org/0a36/1d0f57bb7458d265bebd564e03eba0959dda.pdf}}

  @inproceedings{bauckhage2015viral,
  	Anote = {./images/icwsm2015.png},
  	Author = {Christian Bauckhage and Fabian Hadiji and Kristian Kersting},
  	Booktitle = {Proceedings of the 9th International Conference on Web and Social Media (ICWSM)},
  	Keywords = {Compartment Models, SIR Model, Viral Models},
  	Note = {Within only a few years after the launch of video sharing platforms, viral videos have become a pervasive Internet phenomenon. Yet, notwithstanding growing scholarly interest, the suitability of the viral metaphor seems not to have been studied so far. In this paper, we therefore investigate the attention dynamics of viral videos from the point of view of mathematical epidemiology. We introduce a novel probabilistic model of the progression of infective diseases and use it to analyze time series of YouTube view counts and Google searches. Our results on a data set of almost 800 videos show that their attention dynamics are indeed well accounted for by our epidemic model. In particular, we find that the vast majority of videos considered in this study show very high infection rates.},
  	Pages = {22--30},
  	Title = {How Viral Are Viral Videos?},
  	Url = {http://www.aaai.org/ocs/index.php/ICWSM/ICWSM15/paper/viewFile/10505/10485},
  	Year = {2015},
  	Bdsk-Url-1 = {http://www.aaai.org/ocs/index.php/ICWSM/ICWSM15/paper/viewFile/10505/10485}}

  @inproceedings{mladenov2015equitable,
  	Anote = {./images/uai2015lift.png},
  	Author = {Martin Mladenov and Kristian Kersting},
  	Booktitle = {Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence (UAI)},
  	Keywords = {Statistical Relational AI, Lifed Inference, Symmetries, Free Energies, Weifeiler Lehmann, Marginals},
  	Note = {Significant progress has recently been made towards formalizing symmetry-aware variational inference approaches into a coherent framework. With the exception of TRW for marginal inference, however, this framework resulted in approximate MAP algorithms only, based on equitable and orbit partitions of the graphical model. Here, we deepen our understanding of it for marginal inference. We show that a large class of concave free energies admits equitable partitions, of which orbit partitions are a special case, that can be exploited for lifting. Although already interesting on its own, we go one step further. We demonstrate that concave free energies of pairwise models can be reparametrized so that existing convergent algorithms for lifted marginal inference can be used without modification.},
  	Pages = {602--611},
  	Title = {Equitable Partitions of Concave Free Energies},
  	Url = {http://auai.org/uai2015/proceedings/papers/228.pdf},
  	Year = {2015},
  	Bdsk-Url-1 = {http://auai.org/uai2015/proceedings/papers/228.pdf}}

  @inproceedings{bauckhage2015parameterizing,
  	Anote = {./images/uai2015distance.png},
  	Author = {Christian Bauckhage and Kristian Kersting and Fabian Hadiji},
  	Booktitle = {Proceedings of the 31st Conference on Uncertainty in Artificial Intelligence (UAI)},
  	Keywords = {Generalized Gamma, Distance Distribution, Shortest Path, Undirected Networks, Maximum Entropy},
  	Note = {We apply maximum entropy arguments to derive a general, physically plausible model of path length histograms. Based on the model, we then establish the generalized Gamma as a three-parameter distribution for shortest-path distance in strongly-connected, undirected networks. Extensive experiments corroborate our theoretical results, which thus provide new approaches to network analysis.},
  	Pages = {121--130},
  	Title = {Parameterizing the Distance Distribution of Undirected Networks},
  	Url = {http://auai.org/uai2015/proceedings/papers/62.pdf},
  	Year = {2015},
  	Bdsk-Url-1 = {http://auai.org/uai2015/proceedings/papers/62.pdf}}

  @inproceedings{hadiji2015computer,
  	Anote = {./images/ijcai2015move.png},
  	Author = {Fabian Hadiji and Martin Mladenov and Christian Bauckhage and Kristian Kersting},
  	Booktitle = {Proceedings of the International Conference on Artificial Intelligence (IJCAI)},
  	Keywords = {Lifted Label Propagation, Symmetries, Lifted Inference, Statistical Relational AI, Statistical Laws, Migration},
  	Note = {Many collective human activities have been shown to exhibit universal patterns. However, the possibility of regularities underlying researcher migration in computer science (CS) has barely been explored at global scale. To a large extend, this is due to official and commercial records being restricted, incompatible between countries, and especially not registered across researchers. We overcome these limitations by building our own, transnational, large-scale dataset inferred from publicly available information on the Web. Essentially, we developed Compressed Label Propagation (CLP) to infer missing geo-tags of author-paper-pairs retrieved from online bibliographies tkaing symmetries of the network into account for speed up of LP. On this dataset, we then find statistical regularities that explain how researchers in CS move from one place to another.},
  	Pages = {171--177},
  	Title = {Computer Science on the Move: Inferring Migration Regularities from the Web via Compressed Label Propagation},
  	Url = {https://www.ijcai.org/Proceedings/15/Papers/031.pdf},
  	Year = {2015},
  	Bdsk-Url-1 = {https://www.ijcai.org/Proceedings/15/Papers/031.pdf}}

  @inproceedings{kumaraswamy2015transfer,
  	Anote = {./images/icdm2015transfer.png},
  	Author = {Raksha Kumaraswamy and Phillip Odom and Kristian Kersting and David Leake and Sriraam Natarajan},
  	Booktitle = {Proceedings of the IEEE International Conference on Data Mining (ICDM)},
  	Keywords = {Transfer Learning, Statistical Relational Learning, Type Matching},
  	Note = {Transfer learning is typically performed between problem instances within the same domain. We consider the problem of transferring across domains. To this effect, we adopt a probabilistic logic approach. First, our approach automatically identifies predicates in the target domain that are similar in their relational structure to predicates in the source domain. Second, it transfers the logic rules and learns the parameters of the transferred rules using target data. Finally, it refines the rules as necessary using theory refinement. Our experimental evidence supports that this transfer method finds models as good or better than those found with state-of-the-art methods, with and without transfer, and in a fraction of the time.},
  	Organization = {IEEE},
  	Pages = {811--816},
  	Title = {Transfer learning via relational type matching},
  	Url = {./papers/TransferLearningICDM15.pdf},
  	Year = {2015},
  	Bdsk-Url-1 = {./papers/TransferLearningICDM15.pdf}}

  @inproceedings{sifa2015predicting,
  	Anote = {./images/aiide2015.png},
  	Author = {Rafet Sifa and Fabian Hadiji and Julian Runge and Anders Drachen and Kristian Kersting and Christian Bauckhage},
  	Booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment (AIIDE)},
  	Keywords = {Purchase Decision, Predictions, Free-to-Play Games},
  	Note = {Mobile digital games are dominantly released under the freemium business model, but only a small fraction of the players makes any purchases. The ability to predict who will make a purchase enables optimization of marketing efforts, and tailoring customer relationship management to the specific user's profile. Here this challenge is addressed via two models for predicting purchasing players, using a 100,000 player dataset: 1) A classification model focused on predicting whether a purchase will occur or not. 2) a regression model focused on predicting the number of purchases a user will make. Both models are presented within a decision and regression tree framework for building rules that are actionable by companies. To the best of our knowledge, this is the first study investigating purchase decisions in freemium mobile products from a user behavior perspective and adopting behavior-driven learning approaches to this problem.},
  	Title = {Predicting purchase decisions in mobile free-to-play games},
  	Url = {https://www.aaai.org/ocs/index.php/AIIDE/AIIDE15/paper/viewFile/11544/11359},
  	Year = {2015},
    Key = {Best Paper Award at AIIDE 2015},
  	Bdsk-Url-1 = {https://www.aaai.org/ocs/index.php/AIIDE/AIIDE15/paper/viewFile/11544/11359}}

  @inproceedings{ide2015lte,
  	Anote = {./images/vtcFall2015.png},
  	Author = {Christoph Ide and Fabian Hadiji and Lars Habel and Alejandro Molina and Thomas Zaksek and Michael Schreckenberg and Kristian Kersting and Christian Wietfeld},
  	Booktitle = {Proceedings of the IEEE 82nd Vehicular Technology Conference (VTC Fall)},
  	Keywords = {Poisson regression, LTE Conectivity, Vehicular Traffic Prediction},
  	Note = {The prediction of both, vehicular traffic and communication connectivity are important research topics. In this paper, we propose the usage of innovative machine learning approaches for these objectives. For this purpose, Poisson Dependency Networks (PDNs) are introduced to enhance the prediction quality of vehicular traffic flows. The machine learning model is fitted based on empirical vehicular traffic data. The results show that PDNs enable a significantly better short-term prediction in comparison to a prediction based on the physics of traffic. To combine vehicular traffic with cellular communication networks, a correlation between connectivity indicators and vehicular traffic flow is shown based on measurement results. This relationship is leveraged by means of Poisson regression trees in both directions, and hence, enabling the prediction of both types of network utilization.},
  	Organization = {IEEE},
  	Pages = {1--5},
  	Title = {LTE Connectivity and vehicular traffic prediction based on machine learning approaches},
  	Url = {./papers/vtcFall2015.pdf},
  	Year = {2015},
  	Bdsk-Url-1 = {./papers/vtcFall2015.pdf}}

  @incollection{bauckhage2015archetypal,
  	Anote = {./images/autoencoder2015.png},
  	Author = {Christian Bauckhage and Kristian Kersting and Florian Hoppe and Christian Thurau},
  	Booktitle = {Workshop on New Challenges in Neural Computation},
  	Keywords = {Simplex Volume Maximization, Autoencoder},
  	Note = {We present an effcient approach to archetypal analysis where we use sub-gradient algorithms for optimization over the simplex to determine archetypes and reconstruction coeffcients. Runtime evaluations reveal our approach to be notably more effcient than previous techniques. As an practical application, we consider archetypal analysis for autoencoding.},
  	Title = {Archetypal analysis as an autoencoder},
  	Url = {./papers/autoencode2015nc2.pdf},
  	Year = {2015},
    Key = {Best Presentation Award at NC^2},
  	Bdsk-Url-1 = {./papers/autoencode2015nc2.pdf}}

  @inbook{bauckhage2015cell,
  	Anote = {./images/cellPhone2015.png},
  	Author = {Christian Bauckhage and Marion Neumann and Lisa Hallau and Kristian Kersting},
  	Booktitle = {Computer Vision and Pattern Recognition in Environmental Informatics},
  	Keywords = {Sustainability, Cell Phone, Plant Disease, Classification},
  	Note = {This chapter reviews and presents approaches to plant disease classification based on cell phone images, a novel way to supply farmers with personalized information and processing recommendations in real time. Several statistical image features and a novel scheme of measuring local textures of leaf spots are introduced. The classification of disease symptoms caused by various fungi or bacteria are evaluated for two important agricultural crop varieties, wheat and sugar beet.},
  	Pages = {295},
  	Publisher = {IGI Global},
  	Title = {Cell Phone Image-Based Plant Disease Classification},
  	Url = {http://www.igi-global.com/chapter/cell-phone-image-based-plant-disease-classification/139599},
  	Year = {2015},
  	Bdsk-Url-1 = {http://www.igi-global.com/chapter/cell-phone-image-based-plant-disease-classification/139599}}

  @book{natarajan2015boosting,
  	Anote = {./images/boostedSRL.jpg},
  	Author = {Sriraam Natarajan and Kristian Kersting and Tushar Khot and Jude Shavlik},
  	Issn = {978-3-319-13643-1},
  	Keywords = {Introduction, Statistical Relational Learning, Functional Gradient Boosting, Inverse Reinforcement Learning, Reinforcement Learning, Structure Learning},
  	Note = {This SpringerBrief addresses the challenges of analyzing multi-relational and noisy data by proposing several Statistical Relational Learning (SRL) methods. It reviews the use of functional gradients for boosting the structure and the parameters of statistical relational models. The algorithms have been applied successfully in several SRL settings and have been adapted to several real problems from Information extraction in text to medical problems.},
  	Publisher = {SpringerBrief},
  	Title = {Boosted Statistical Relational Learners},
  	Url = {http://www.springer.com/de/book/9783319136431},
    Crossref = {https://starling.utdallas.edu/software/boostsrl/wiki/},
  	Year = {2015},
  	Bdsk-Url-1 = {http://www.springer.com/de/book/9783319136431}}

  @article{khot2015gradient,
  	Anote = {./images/mlj2015boosting.png},
  	Author = {Tushar Khot and Sriraam Natarajan and Kristian Kersting and Jude Shavlik},
  	Journal = {Machine Learning (MLJ)},
  	Keywords = {Statistical Relational Learning, Functional Gradient Boosting, Markov Logic Network, Missing Data, EM},
  	Note = {Most of the current methods for learning MLN structure follow a two-step approach where first they search through the space of possible clauses (i.e. structures) and then learn weights via gradient descent for these clauses. We present a functional-gradient boosting algorithm to learn both the weights (in closed form) and the structure of the MLN simultaneously. Moreover most of the learning approaches for SRL apply the closed-world assumption, i.e., whatever is not observed is assumed to be false in the world. We attempt to open this assumption. We extend our algorithm for MLN structure learning to handle missing data by using an EM-based approach and show this algorithm can also be used to learn Relational Dependency Networks and relational policies. Our results in many domains demonstrate that our approach can effectively learn MLNs even in the presence of missing data.},
  	Number = {1},
  	Pages = {75--100},
  	Publisher = {Springer},
  	Title = {Gradient-based boosting for statistical relational learning: the Markov logic network and missing data cases},
  	Url = {https://link.springer.com/content/pdf/10.1007%2Fs10994-015-5481-4.pdf},
  	Volume = {100},
  	Year = {2015},
  	Bdsk-Url-1 = {https://link.springer.com/content/pdf/10.1007%2Fs10994-015-5481-4.pdf}}

  @article{wahabzada2015metro,
  	Anote = {./images/journal.pone.0116902.g006.png},
  	Author = {Mirwaes Wahabzada and Anne-Katrin Mahlein and Christian Bauckhage and Ulrike Steiner and Erich-Christian Oerke and Kristian Kersting},
  	Journal = {PLoS One},
  	Keywords = {Hyperspectral Images, Metro Maps, Plant Phenotpying, Sustainability, Simplex Volume Maximization, Interpretable Matrix Factorization},
  	Note = {We present a cascade of data mining techniques for fast and reliable data-driven sketching of complex hyperspectral dynamics in plant science and plant phenotyping. We automatically discover archetypal hyperspectral signatures in linear time and use them to create structured summaries that are inspired by metro maps, i.e. schematic diagrams of public transport networks.},
  	Number = {1},
  	Pages = {e0116902},
  	Publisher = {Public Library of Science},
  	Title = {Metro maps of plant disease dynamics---automated mining of differences using hyperspectral images},
  	Url = {http://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0116902&type=printable},
  	Volume = {10},
  	Year = {2015},
  	Bdsk-Url-1 = {http://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0116902&type=printable}}

  @article{kuska2015hyperspectral,
  	Anote = {./images/hyperMicro2015.png},
  	Author = {Matheus Kuska and Mirwaes Wahabzada and Marlene Leucker and Heinz-Wilhelm Dehne and Kristian Kersting and Erich-Christian Oerke and Ulrike Steiner and Anne-Katrin Mahlein},
  	Journal = {Plant Methods},
  	Keywords = {Hyperspectral Images, Sustainability, Plant Phenotyping, Mircoscope, Simplex Volume Maximization},
  	Note = {Data analysis showed no significant differences in spectral signatures between non-inoculated genotypes. Barley leaves of the near-isogenic genotypes, inoculated with B. graminis f.sp. hordei differed in the spectral reflectance over time, respectively. The susceptible genotypes (WT, Mla12) showed an increase in reflectance in the visible range according to symptom development. However, the spectral signature of the resistant mlo-genotype did not show significant changes over the experimental period. In addition, a recent data driven approach for automated discovery of disease specific signatures, which is based on a new representation of the data using Simplex Volume Maximization (SiVM) was applied. The automated approach - evaluated in only a fraction of time revealed results similar to the time and labor intensive manually assessed hyperspectral signatures. The new representation determined by SiVM was also used to generate intuitive and easy to interpretable summaries, e.g. fingerprints or traces of hyperspectral dynamics of the different genotypes.},
  	Number = {1},
  	Pages = {28},
  	Publisher = {BioMed Central},
  	Title = {Hyperspectral phenotyping on the microscopic scale: towards automated characterization of plant-pathogen interactions},
  	Url = {http://download.springer.com/static/pdf/574/art%253A10.1186%252Fs13007-015-0073-7.pdf?originUrl=http%3A%2F%2Fplantmethods.biomedcentral.com%2Farticle%2F10.1186%2Fs13007-015-0073-7&token2=exp=1493050344~acl=%2Fstatic%2Fpdf%2F574%2Fart%25253A10.1186%25252Fs13007-015-0073-7.pdf*~hmac=8f797422b40d706e7eb0f33d14dc654de20dda990314a238f0f0b65e2af14cc9},
  	Volume = {11},
  	Year = {2015},
  	Bdsk-Url-1 = {http://download.springer.com/static/pdf/574/art%253A10.1186%252Fs13007-015-0073-7.pdf?originUrl=http%3A%2F%2Fplantmethods.biomedcentral.com%2Farticle%2F10.1186%2Fs13007-015-0073-7&token2=exp=1493050344~acl=%2Fstatic%2Fpdf%2F574%2Fart%25253A10.1186%25252Fs13007-015-0073-7.pdf*~hmac=8f797422b40d706e7eb0f33d14dc654de20dda990314a238f0f0b65e2af14cc9}}

  @article{kersting2015statistical,
    Anote = {./images/kersting2015statistical.jpg},
  	Author = {Kristian Kersting and Sriraam Natarajan},
  	Journal = {K√ºnstliche Intelligenz (KI)},
  	Keywords = {Overview, Statistical Relational AI},
  	Note = {Statistical Relational AI---the science and engineering of making intelligent machines acting in noisy worlds composed of objects and relations among the objects---is currently motivating a lot of new AI research and has tremendous theoretical and practical implications. Theoretically, combining logic and probability in a unified representation and building general-purpose reasoning tools for it has been the dream of AI, dating back to the late 1980s. Practically, successful statistical relational AI tools enable new applications in several large, complex real-world domains including those involving big data, natural text, social networks, the web, medicine and robotics, among others. Such domains are often characterized by rich relational structure and large amounts of uncertainty. Logic helps to faithfully model the former while probability helps to effectively manage the latter. Our intention here is to give a brief (and necessarily incomplete) overview and invitation to the emerging field of Statistical Relational AI from the perspective of acting optimally and learning to act.},
  	Number = {4},
  	Pages = {363--368},
  	Publisher = {Springer},
  	Title = {Statistical Relational Artificial Intelligence: From Distributions through Actions to Optimization},
  	Url = {http://dx.doi.org/10.1007/s13218-015-0386-8},
  	Volume = {29},
  	Year = {2015},
  	Bdsk-Url-1 = {http://dx.doi.org/10.1007/s13218-015-0386-8}}

  @article{hadiji2015poisson,
  	Anote = {./images/mlj2015pdns.png},
  	Author = {Fabian Hadiji and Alejandro Molina and Sriraam Natarajan and Kristian Kersting},
  	Journal = {Machine Learning (MLJ)},
  	Keywords = {Dependency Networks, Multivariate Poisson, Funcational Gradient Boosting},
  	Note = {Although count data are increasingly ubiquitous, surprisingly little work has employed probabilistic graphical models for modeling count data. To ease the modeling of multivariate count data, we therefore introduce a novel family of Poisson graphical models, called Poisson Dependency Networks (PDNs). A PDN consists of a set of local conditional Poisson distributions, each representing the probability of a single count variable given the others, that naturally facilitates a simple Gibbs sampling inference. In contrast to existing Poisson graphical models, PDNs are non-parametric and trained using functional gradient ascent, i.e., boosting.},
  	Number = {2-3},
  	Pages = {477--507},
  	Publisher = {Springer},
  	Title = {Poisson dependency networks: Gradient boosted models for multivariate count data},
  	Url = {https://link.springer.com/content/pdf/10.1007%2Fs10994-015-5506-z.pdf},
  	Volume = {100},
  	Year = {2015},
  	Bdsk-Url-1 = {https://link.springer.com/content/pdf/10.1007%2Fs10994-015-5506-z.pdf}}

  @article{wahabzada2015automated,
  	Anote = {./images/bmc2015.png},
  	Author = {Mirwaes Wahabzada and Stefan Paulus and Kristian Kersting and Anne-Katrin Mahlein},
  	Journal = {BMC bioinformatics},
  	Keywords = {Clustering, Plant Phenotpying, 3D, Laser Scan, Simplex Volume maximization},
  	Note = {Plant organ segmentation from 3D point clouds is a relevant task for plant phenotyping and plant growth observation. We developed a fully automated, fast and reliable data driven approach for plant organ segmentation. Since normalized histograms, acquired from 3D point clouds, can be seen as samples from a probability simplex, we propose to map the data from the simplex space into Euclidean space using Aitchisons log ratio transformation, or into the positive quadrant of the unit sphere using square root transformation. This, in turn, paves the way to a wide range of commonly used analysis techniques that are based on measuring the similarities between data points using Euclidean distance. The resulting pipeline is fast. Within seconds first insights into plant data can be derived, even from non-labelled data.},
  	Number = {1},
  	Pages = {248},
  	Publisher = {BioMed Central},
  	Title = {Automated interpretation of 3D laserscanned point clouds for plant organ segmentation},
  	Url = {https://bmcbioinformatics.biomedcentral.com/track/pdf/10.1186/s12859-015-0665-2?site=bmcbioinformatics.biomedcentral.com},
  	Volume = {16},
  	Year = {2015},
  	Bdsk-Url-1 = {https://bmcbioinformatics.biomedcentral.com/track/pdf/10.1186/s12859-015-0665-2?site=bmcbioinformatics.biomedcentral.com}}

  @article{neumann2015pygps,
    Anote = {./images/neumann2015pygps.png},
  	Author = {Marion Neumann and Shan Huang and Daniel Marthaler and Kristian Kersting},
  	Journal = {Journal of Machine Learning Research (JMLR)},
  	Keywords = {Toolbox, Gaussian Processes, Python},
  	Note = {We introduce pyGPs, an object-oriented implementation of Gaussian processes (gps) for machine learning. The library provides a wide range of functionalities reaching from simple gp specification via mean and covariance and gp inference to more complex implementations of hyperparameter optimization, sparse approximations, and graph based learning. Using Python we focus on usability for both ``users'' and ``researchers''. Our main goal is to offer a user-friendly and flexible implementation of gps for machine learning.},
  	Pages = {2611--2616},
  	Title = {pyGPs--A python library for Gaussian process regression and classification},
  	Url = {http://www.jmlr.org/papers/volume16/neumann15a/neumann15a.pdf},
  	Volume = {16},
  	Year = {2015},
  	Bdsk-Url-1 = {http://www.jmlr.org/papers/volume16/neumann15a/neumann15a.pdf}}

  @misc{bauckhage2014strong,
  	Anote = {./images/strong2014.png},
  	Author = {Christian Bauckhage and Kristian Kersting},
  	Howpublished = {arXiv preprint arXiv:1406.6529},
  	Keywords = {Collective Attention, Diffusion Models, Social Media},
  	Note = {We analyze general trends and pattern in time series that characterize the dynamics of collective attention to social media services and Web-based businesses. Our study is based on search frequency data available from Google Trends and considers 175 different services. For each service, we collect data from 45 different countries as well as global averages. This way, we obtain more than 8,000 time series which we analyze using diffusion models from the economic sciences. We find that these models accurately characterize the empirical data and our analysis reveals that collective attention to social media grows and subsides in a highly regular and predictable manner. Regularities persist across regions, cultures, and topics and thus hint at general mechanisms that govern the adoption of Web-based services. We discuss several cases in detail to highlight interesting findings. Our methods are of economic interest as they may inform investment decisions and can help assessing at what stage of the general life-cycle a Web service is at.},
  	Title = {Strong Regularities in Growth and Decline of Popularity of Social Media Services},
  	Url = {https://arxiv.org/pdf/1406.6529.pdf},
  	Year = {2014},
  	Bdsk-Url-1 = {https://arxiv.org/pdf/1406.6529.pdf}}

  @misc{kersting2014relational,
  	Anote = {./images/rlp_aij2017.png},
  	Author = {Kristian Kersting and Martin Mladenov and Pavel Tokmakov},
  	Howpublished = {arXiv preprint arXiv:1410.3125},
  	Keywords = {Statistical Relational AI, Relational Linear Programs, Symmetries, Lifted Inference},
  	Note = {We propose relational linear programming, a simple framework for combing linear programs (LPs) and logic programs. A relational linear program (RLP) is a declarative LP template defining the objective and the constraints through the logical concepts of objects, relations, and quantified variables. This allows one to express the LP objective and constraints relationally for a varying number of individuals and relations among them without enumerating them. Together with a logical knowledge base, effectively a logical program consisting of logical facts and rules, it induces a ground LP. This ground LP is solved using lifted linear programming. That is, symmetries within the ground LP are employed to reduce its dimensionality, if possible, and the reduced program is solved using any off-the-shelf LP solver. In contrast to mainstream LP template languages like AMPL, which features a mixture of declarative and imperative programming styles, RLP's relational nature allows a more intuitive representation of optimization problems over relational domains. We illustrate this empirically by experiments on approximate inference in Markov logic networks using LP relaxations, on solving Markov decision processes, and on collective inference using LP support vector machines.},
  	Title = {Relational linear programs},
  	Url = {https://arxiv.org/pdf/1410.3125.pdf},
  	Year = {2014},
  	Bdsk-Url-1 = {https://arxiv.org/pdf/1410.3125.pdf}}

  @misc{neumann2014propagation,
  	Anote = {./images/propKernel_mlg2016.png},
  	Author = {Marion Neumann and Roman Garnett and Christian Bauckhage and Kristian Kersting},
  	Howpublished = {arXiv preprint arXiv:1410.3314},
  	Keywords = {Graph Kernels, Belief Propagation, Information Propagation, Random Walks, Weisfeiler Lehmann},
  	Note = {We introduce propagation kernels, a general graph-kernel framework for efficiently measuring the similarity of structured data. Propagation kernels are based on monitoring how information spreads through a set of given graphs. They leverage early-stage distributions from propagation schemes such as random walks to capture structural information encoded in node labels, attributes, and edge information. This has two benefits. First, off-the-shelf propagation schemes can be used to naturally construct kernels for many graph types, including labeled, partially labeled, unlabeled, directed, and attributed graphs. Second, by leveraging existing efficient and informative propagation schemes, propagation kernels can be considerably faster than state-of-the-art approaches without sacrificing predictive performance. We will also show that if the graphs at hand have a regular structure, for instance when modeling image or video data, one can exploit this regularity to scale the kernel computation to large databases of graphs with thousands of nodes. We support our contributions by exhaustive experiments on a number of real-world graphs from a variety of application domains.},
  	Title = {Propagation Kernels},
  	Url = {https://arxiv.org/pdf/1410.3314.pdf},
  	Year = {2014},
  	Bdsk-Url-1 = {https://arxiv.org/pdf/1410.3314.pdf}}

  @misc{antanas2014high,
  	Anote = {./images/antanas2014high.png},
  	Author = {Laura Antanas and Plinio Moreno and Marion Neumann and Rui Pimentel de Figueiredo and Kristian Kersting and Jose Santos-Victor and Luc De Raedt},
  	Howpublished = {arXiv preprint arXiv:1411.1108},
  	Keywords = {Robotics, Statistical Relational Learning, Grapsing, Graph Kernels, Affordance, Domain Knowledge},
  	Note = {While grasps must satisfy the grasping stability criteria, good grasps depend on the specific manipulation scenario: the object, its properties and functionalities, as well as the task and grasp constraints. In this paper, we consider such information for robot grasping by leveraging manifolds and symbolic object parts. Specifically, we introduce a new probabilistic logic module to first semantically reason about pre-grasp configurations with respect to the intended tasks. Further, a mapping is learned from part-related visual features to good grasping points. The probabilistic logic module makes use of object-task affordances and object/task ontologies to encode rules that generalize over similar object parts and object/task categories. The use of probabilistic logic for task-dependent grasping contrasts with current approaches that usually learn direct mappings from visual perceptions to task-dependent grasping points. We show the benefits of the full probabilistic logic pipeline experimentally and on a real robot.},
  	Title = {High-level Reasoning and Low-level Learning for Grasping: A Probabilistic Logic Pipeline},
  	Url = {https://arxiv.org/pdf/1411.1108.pdf},
  	Year = {2014},
  	Bdsk-Url-1 = {https://arxiv.org/pdf/1411.1108.pdf}}

  @inproceedings{alfeld2014non,
  	Anote = {./images/JoPhy2014.png},
  	Author = {Matthias Alfeld and Mirwaes Wahabzada and Christian Bauckhage and Kristian Kersting and Gerd Wellenreuther and Gerald Falkenberg},
  	Booktitle = {Journal of Physics: Conference Series},
  	Keywords = {Rembrandt, Interpretable Matrix Factorization, Simplex Volume Maximization, Material Science},
  	Note = {Stacks of elemental distribution images acquired by XRF can be difficult to interpret, if they contain high degrees of redundancy and components differing in their quantitative but not qualitative elemental composition. Factor analysis, mainly in the form of Principal Component Analysis (PCA), has been used to reduce the level of redundancy and highlight correlations. PCA, however, does not yield physically meaningful representations as they often contain negative values. This limitation can be overcome, by employing factor analysis that is restricted to non-negativity. In this paper we present the first application of the Python Matrix Factorization Module (pymf) on XRF data. This is done in a case study on the painting Saul and David from the studio of Rembrandt van Rijn. We show how the discrimination between two different Co containing compounds with minimum user intervention and a priori knowledge is supported by Non-Negative Matrix Factorization (NMF).},
  	Number = {1},
  	Organization = {IOP Publishing},
  	Pages = {012013},
  	Title = {Non-negative factor analysis supporting the interpretation of elemental distribution images acquired by XRF},
  	Url = {http://iopscience.iop.org/article/10.1088/1742-6596/499/1/012013/pdf},
  	Volume = {499},
  	Year = {2014},
  	Bdsk-Url-1 = {http://iopscience.iop.org/article/10.1088/1742-6596/499/1/012013/pdf}}

  @inproceedings{grohe2014dimension,
  	Anote = {./images/esa2014.png},
  	Author = {Martin Grohe and Kristian Kersting and Martin Mladenov and Erkal Selman},
  	Booktitle = {Proceedings of the European Symposium on Algorithms (ESA)},
  	Keywords = {Weifeiler Lehmann, Linear Programs, Colour Refinement, Symmetries, Matrices},
  	Note = {We introduce a version of colour refinement for matrices and extend existing quasilinear algorithms for computing the colour classes. Then we generalise the correspondence between colour refinement and fractional automorphisms and develop a theory of fractional automorphisms and isomorphisms of matrices. Finally, we apply our results to reduce the dimensions of systems of linear equations and linear programs.},
  	Organization = {Springer},
  	Pages = {505--516},
  	Title = {Dimension reduction via colour refinement},
  	Url = {https://arxiv.org/pdf/1307.5697.pdf},
  	Year = {2014},
  	Bdsk-Url-1 = {https://arxiv.org/pdf/1307.5697.pdf}}

  @inproceedings{bauckhage2014collective,
  	Anote = {./images/bauckhage2014collective.png},
  	Author = {Christian Bauckhage and Kristian Kersting and Bashir Rastegarpanah},
  	Booktitle = {Proceedings of the 23rd International Conference on World Wide Web (WWW)},
  	Keywords = {Diffusion Models, Social Media, Collective Attention},
  	Note = {We investigate patterns of adoption of 175 social media services and Web businesses using data from Google Trends. For each service, we collect aggregated search frequencies from 45 countries as well as global averages. This results in more than 8.000 time series which we analyze using economic diffusion models. The models are found to provide accurate and statistically significant fits to the data and show that collective attention to social media grows and subsides in a highly regular manner. Regularities persist across regions, cultures, and topics and thus hint at general mechanisms that govern the adoption of Web-based services.},
  	Organization = {ACM},
  	Pages = {223--224},
  	Title = {Collective attention to social media evolves according to diffusion models},
  	Url = {http://wwwconference.org/proceedings/www2014/companion/p223.pdf},
  	Year = {2014},
  	Bdsk-Url-1 = {http://wwwconference.org/proceedings/www2014/companion/p223.pdf}}

  @inproceedings{mladenov2014efficient,
  	Anote = {./images/aistats2014locality.png},
  	Author = {Martin Mladenov and Kristian Kersting and Amir Globerson},
  	Booktitle = {Proceedings of the 17th International Conference on Artificial Intelligence and Statistics (AISTATS)},
  	Keywords = {Statistical Relational AI, Lifted Inference, MAP, Symmetries, Sherali-Adams, k-Locality, Tightening},
  	Note = {We show that symmetry in MAP inference problems can be discovered using an elegant algorithm known as the k-dimensional Weisfeiler-Lehman (k-WL) algorithm. We run k-WL on the original graphical model, and not on the far larger graph of the linear program (LP) as proposed in earlier work in the field. Furthermore, the algorithm is polynomial and thus far more practical than other previous approaches which rely on orbit partitions that are GI complete to find. The fact that k-WL can be used in this manner follows from the recently introduced notion of k-local LPs and their relation to Sherali Adams relaxations of graph automorphisms.},
  	Pages = {623--632},
  	Title = {Efficient Lifting of MAP LP Relaxations Using k-Locality},
  	Url = {http://proceedings.mlr.press/v33/mladenov14.pdf},
  	Year = {2014},
  	Bdsk-Url-1 = {http://proceedings.mlr.press/v33/mladenov14.pdf}}

  @inproceedings{apsel2014lifting,
  	Address = {AAAI Press},
  	Anote = {./images/apsel2014lifting.png},
  	Author = {Udi Apsel and Kristian Kersting and Martin Mladenov},
  	Booktitle = {Proceedings of the 28th AAAI Conference on Artificial Intelligence (AAAI)},
  	Keywords = {Statistical Relational AI, Lifted Inference, MAP, Cluster Signature Graphs, Symmetries, Sherali-Adams},
  	Note = {Large scale graphical models often exhibit considerable symmetry, and it is a challenge to devise algorithms that exploit this symmetry to speed up inference. Recently, the automorphism group has been proposed to formalize mathematically what exploiting symmetry means. However, obtaining symmetry derived from automorphism is GI-hard, and consequently only a small fraction of the symmetry is easily available for effective employment. In this paper, we improve upon efficiency in two ways. First, we introduce the Cluster Signature Graph (CSG), a platform on which greater portions of the symmetries can be revealed and exploited. CSGs classify clusters of variables by projecting relations between cluster members onto a graph, allowing for the efficient pruning of symmetrical clusters even before their generation. Second, we introduce a novel framework based on CSGs for the Sherali-Adams hierarchy of linear program (LP) relaxations, dedicated to exploiting this symmetry for the benefit of tight Maximum A Posteriori (MAP) approximations. Combined with the pruning power of CSG, the framework quickly generates compact formulations for otherwise intractable LPs, as demonstrated by several empirical results.},
  	Pages = {2403--2409},
  	Title = {Lifting Relational MAP-LPs Using Cluster Signatures},
  	Url = {./papers/apsel2014lifting.pdf},
  	Year = {2014},
  	Bdsk-Url-1 = {./papers/apsel2014lifting.pdf}}

  @inproceedings{neumann2014erosion,
  	Anote = {./images/neumann2014erosion.png},
  	Author = {Marion Neumann and Lisa Hallau and Benjamin Klatt and Kristian Kersting and Christian Bauckhage},
  	Booktitle = {Proceedings of the 22nd IEEE International Conference on Pattern Recognition (ICPR)},
  	Keywords = {Plant Disease, Cell Phone, Classification, Sustainability},
  	Note = {We introduce a novel set of features for a challenging image analysis task in agriculture where cell phone camera images of beet leaves are analyzed as to the presence of plant diseases. Aiming at minimal computational costs on the cellular device and highly accurate prediction results, we present an efficient detector of potential disease regions and a robust classification method based on texture features. We evaluate several first- and second-order statistical features for classifying textures of leaf spots and we find that a combination of descriptors derived on multiple erosion bands of the RGB color channels, as well as, the local binary patterns of gradient magnitudes of the extracted regions accurately distinguish between symptoms caused by five diseases, including infections of the fungi Cercospora beticola, Ramularia beticola, Uromyces betae, and Phoma betae, and the bacterium Pseudomonas syringae pv. aptata.},
  	Organization = {IEEE},
  	Pages = {3315--3320},
  	Title = {Erosion band features for cell phone image based plant disease classification},
  	Url = {./papers/neumann2014erosion.pdf},
  	Year = {2014},
  	Bdsk-Url-1 = {./papers/neumann2014erosion.pdf}}

  @inproceedings{hernandez2014mind,
  	Anote = {./images/hernandez2014mind.png},
  	Author = {Daniel Hernandez-Lobato and Viktoriia Sharmanska and Kristian Kersting and Christoph Lampert and Novi Quadrianto},
  	Booktitle = {Proceedings of Neural Information Processing Systems (NIPS)},
  	Keywords = {Gaussian Processes, Privileged Information, LUPI},
  	Note = {The learning with privileged information setting has recently attracted a lot of attention within the machine learning community, as it allows the integration of additional knowledge into the training process of a classifier, even when this comes in the form of a data modality that is not available at test time. Here, we show that privileged information can naturally be treated as noise in the latent function of a Gaussian process classifier (GPC). That is, in contrast to the standard GPC setting, the latent function is not just a nuisance but a feature: it becomes a natural measure of confidence about the training data by modulating the slope of the GPC probit likelihood function. Extensive experiments on public datasets show that the proposed GPC method using privileged noise, called GPC+, improves over a standard GPC without privileged knowledge, and also over the current state-of-the-art SVM-based method, SVM+. Moreover, we show that advanced neural networks and deep learning methods can be compressed as privileged information.},
  	Pages = {837--845},
  	Title = {Mind the nuisance: Gaussian process classification using privileged noise},
  	Url = {https://papers.nips.cc/paper/5373-mind-the-nuisance-gaussian-process-classification-using-privileged-noise.pdf},
  	Year = {2014},
  	Bdsk-Url-1 = {https://papers.nips.cc/paper/5373-mind-the-nuisance-gaussian-process-classification-using-privileged-noise.pdf}}

  @inproceedings{kersting2014power,
  	Anote = {./images/aaai2014picr.png},
  	Author = {Kristian Kersting and Martin Mladenov and Roman Garnett and Martin Grohe},
  	Booktitle = {Proceedings of the 28th AAAI Conference on Artificial Intelligence (AAAI)},
  	Keywords = {Weisfeiler Lehmann, Power Iteration, Page Rank, Equivalence},
  	Note = {Color refinement is a basic algorithmic routine for graph isomorphism testing and has recently been used for computing graph kernels as well as for lifting belief propagation and linear programming. So far, color re- finement has been treated as a combinatorial problem. Instead, we treat it as a nonlinear continuous optimization problem and prove that it implements a conditional gradient optimizer that can be turned into graph clustering approaches using hashing and truncated power iterations. This shows that color refinement is easy to understand in terms of random walks, easy to implement (matrix-matrix/vector multiplications) and readily parallelizable. We support our theoretical results with experiments on real-world graphs with millions of edges.},
  	Title = {Power iterated color refinement},
  	Url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI14/paper/view/8377/8828},
  	Year = {2014},
  	Bdsk-Url-1 = {https://www.aaai.org/ocs/index.php/AAAI/AAAI14/paper/view/8377/8828}}

  @inproceedings{poole2014population,
  	Anote = {./images/poole2014population.png},
  	Author = {David Poole and David Buchman and Seyed Mehran Kazemi and Kristian Kersting and Sriraam Natarajan},
  	Booktitle = {Proceedings of the International Conference on Scalable Uncertainty Management (SUM)},
  	Keywords = {Statistical Relational Learning, Relational Logistic Regression, Population Size},
  	Note = {When building probabilistic relational models it is often difficult to determine what formulae or factors to include in a model. Different models make quite di erent predictions about how probabilities are a effected by population size. We show some general patterns that hold in some classes of models for all numerical parametrizations. Given a data set, it is often easy to plot the dependence of probabilities on population size, which, together with prior knowledge, can be used to rule out classes of models, where just assessing or fitting numerical parameters will be misleading. In this paper we analyze the dependence on population for relational undirected models (in particular Markov logic networks) and relational directed models (for relational logistic regression). Finally we show how probabilities for real data sets depend on the population size.},
  	Organization = {Springer},
  	Pages = {292--305},
  	Title = {Population size extrapolation in relational probabilistic modelling},
  	Url = {./papers/poole2014population.pdf},
  	Year = {2014},
  	Bdsk-Url-1 = {./papers/poole2014population.pdf}}

  @inproceedings{mladenov2014lifted,
  	Anote = {./images/reparam2014uai.png},
  	Author = {Martin Mladenov and Amir Globerson and Kristian Kersting},
  	Booktitle = {Proceedings of the 30th International Conference on Uncertainty in AI (UAI)},
  	Keywords = {Statistical Relational AI, Lifted Inference, Symmetries, MPLP, Linear Programs, Reparametrization, Energies},
  	Note = {Lifted inference approaches can considerably speed up probabilistic inference in Markov random fields (MRFs) with symmetries. Given evidence, they essentially form a lifted, i.e., reduced factor graph by grouping together indistinguishable variables and factors. Typically, however, lifted factor graphs are not amenable to offthe-shelf message passing (MP) approaches, and hence requires one to use either generic optimization tools, which would be slow for these problems, or design modified MP algorithms. Here, we demonstrate that the reliance on modified MP can be eliminated for the class of MP algorithms arising from MAP-LP relaxations of pairwise MRFs. Specifically, we show that a given MRF induces a whole family of MRFs of different sizes sharing essentially the same MAPLP solution. In turn, we give an efficient algorithm to compute from them the smallest one that can be solved using off-the-shelf MP.},
  	Pages = {603--612},
  	Title = {Lifted Message Passing as Reparametrization of Graphical Models},
  	Url = {http://www.auai.org/uai2014/proceedings/individuals/215.pdf},
  	Year = {2014},
  	Bdsk-Url-1 = {http://www.auai.org/uai2014/proceedings/individuals/215.pdf}}

  @inproceedings{hadiji2014predicting,
  	Anote = {./images/hadiji2014predicting.png},
  	Author = {Fabian Hadiji and Rafet Sifa and Anders Drachen and Christian Thurau and Kristian Kersting and Christian Bauckhage},
  	Booktitle = {Proceedings of the IEEE Conference on Computational intelligence and games (CIG)},
  	Keywords = {Churn Prediction, Computer Games, Free-to-play Games},
  	Note = {The ability to model, understand and predict future player behavior has a crucial value, allowing developers to obtain data-driven insights to inform design, development and marketing strategies. One of the key challenges is modeling and predicting player churn. This paper presents the first cross-game study of churn prediction in Free-to-Play games. Churn in games is discussed and thoroughly defined as a formal problem, aligning with industry standards. Furthermore, a range of features which are generic to games are defined and evaluated for their usefulness in predicting player churn, e.g. playtime, session length and session intervals. Using these behavioral features, combined with the individual retention model for each game in the dataset used, we develop a broadly applicable churn prediction model, which does not rely on gamedesign specific features. The presented classifiers are applied on a dataset covering five free-to-play games resulting in high accuracy churn prediction.},
  	Organization = {IEEE},
  	Pages = {1--8},
  	Title = {Predicting player churn in the wild},
  	Url = {./papers/hadiji2014predicting.pdf},
  	Year = {2014},
  	Bdsk-Url-1 = {./papers/hadiji2014predicting.pdf}}

  @inproceedings{yang2014learning,
  	Anote = {./images/icdm2014softBalance.png},
  	Author = {Shuo Yang and Tushar Khot and Kristian Kersting and Gautam Kunapuli and Kris Hauser and Sriraam Natarajan},
  	Booktitle = {Proceedings of the IEEE International Conference on Data Mining (ICDM)},
  	Keywords = {Statistical Relational Learning, Functional Gradient Boosting, Imbalanced Data, Soft Margin},
  	Note = {We consider the problem of learning probabilistic models from relational data. One of the key issues with relational data is class imbalance where the number of negative examples far outnumbers the number of positive examples. The common approach for dealing with this problem is the use of sub-sampling of negative examples. We, on the other hand, consider a soft margin approach that explicitly trades off between the false positives and false negatives. We apply this approach to the recently successful formalism of relational functional gradient boosting. Specifically, we modify the objective function of the learning problem to explicitly include the trade-off between false positives and negatives. We show empirically that this approach is more successful in handling the class imbalance problem than the original framework that weighed all the examples equally.},
  	Organization = {IEEE},
  	Pages = {1085--1090},
  	Title = {Learning from imbalanced data in relational domains: A soft margin approach},
  	Url = {./papers/icdm2014softMargin.pdf},
  	Year = {2014},
  	Bdsk-Url-1 = {./papers/icdm2014softMargin.pdf}}

  @inproceedings{kriege2014explicit,
  	Anote = {./images/expliciteFMgraphs.png},
  	Author = {Nils Kriege and Marion Neumann and Kristian Kersting and Petra Mutzel},
  	Booktitle = {Proceedings of the IEEE International Conference on Data Mining (ICDM)},
  	Keywords = {Graph Kernels, Explicit Feature Map, Phase Transition},
  	Note = {Surprisingly, many of the recent graph kernels do not employ the kernel trick anymore but rather compute an explicit feature map and report higher efficiency. So, is there really no benefit of the kernel trick when it comes to graphs? Triggered by this question, we investigate under which conditions it is possible to compute a graph kernel explicitly and for which graph properties this computation is actually more efficient. We give a sufficient condition for R-convolution kernels that enables kernel computation by explicit mapping. We theoretically and experimentally analyze efficiency and flexibility of implicit kernel functions and dot products of explicitly computed feature maps for widely used graph kernels such as random walk kernels, sub graph matching kernels, and shortest-path kernels. For walk kernels we observe a phase transition when comparing runtime with respect to label diversity and walk lengths leading to the conclusion that explicit computations are only favourable for smaller label sets and walk lengths whereas implicit computation is superior for longer walk lengths and data sets with larger label diversity.},
  	Organization = {IEEE},
  	Pages = {881--886},
  	Title = {Explicit versus implicit graph feature maps: A computational phase transition for walk kernels},
  	Url = {http://ieeexplore.ieee.org/document/7023417/?reload=true},
  	Year = {2014},
  	Bdsk-Url-1 = {http://ieeexplore.ieee.org/document/7023417/?reload=true}}

  @incollection{apsel2014lifting_ws,
  	Author = {Udi Apsel and Kristian Kersting and Martin Mladenov},
  	Booktitle = {Working Notes of the AAAI Workshop on Statistical Relational Artificial Intelligence (StarAI)},
  	Keywords = {Statistical Relational AI, Lifted Inference, MAP, Cluster Signature Graphs, Symmetries, Sherali-Adams},
  	Note = {Large scale graphical models often exhibit considerable symmetry, and it is a challenge to devise algorithms that exploit this symmetry to speed up inference. Recently, the automorphism group has been proposed to formalize mathematically what exploiting symmetry means. However, obtaining symmetry derived from automorphism is GI-hard, and consequently only a small fraction of the symmetry is easily available for effective employment. In this paper, we improve upon efficiency in two ways. First, we introduce the Cluster Signature Graph (CSG), a platform on which greater portions of the symmetries can be revealed and exploited. CSGs classify clusters of variables by projecting relations between cluster members onto a graph, allowing for the efficient pruning of symmetrical clusters even before their generation. Second, we introduce a novel framework based on CSGs for the Sherali-Adams hierarchy of linear program (LP) relaxations, dedicated to exploiting this symmetry for the benefit of tight Maximum A Posteriori (MAP) approximations. Combined with the pruning power of CSG, the framework quickly generates compact formulations for otherwise intractable LPs, as demonstrated by several empirical results.},
  	Title = {Lifting Relational MAP-LPs Using Cluster Signatures},
  	Year = {2014}}

  @incollection{kazemi2014relational,
  	Author = {Seyed Mehran Kazemi and David Buchman and Kristian Kersting and Sriraam Natarajan and David Poole},
  	Booktitle = {Working Notes of the AAAI Workshop on Statistical Relational Artificial Intelligence (StarAI)},
  	Keywords = {Statistical Relational Learning, Relational Logistics Regression, Markov Logic Networks, Population Size},
  	Note = {Relational logistic regression (RLR) is the directed analogue of Markov logic networks. Whereas Markov logic networks define distributions in terms of weighted formulae, RLR defines conditional probabilities in terms of weighted formulae. They agree for the supervised learning case when all variables except a query leaf variable are observed. However, they are quite different in representing distributions. The KR-2014 paper defined the RLR formalism, defined canonical forms for RLR in terms of positive conjunctive (or disjunctive) formulae, indicated the class of conditional probability distributions that can and cannot be represented by RLR, and defined many other aggregators in terms of RLR. In this paper, we summarize these results and compare RLR to Markov logic networks.},
  	Title = {Relational Logistic Regression: The Directed Analog of Markov Logic Networks},
  	Url = {https://www.aaai.org/ocs/index.php/WS/AAAIW14/paper/download/8849/8233},
  	Year = {2014},
  	Bdsk-Url-1 = {https://www.aaai.org/ocs/index.php/WS/AAAIW14/paper/download/8849/8233}}

  @incollection{poyrekar2014deeper,
  	Anote = {./images/poyrekar2014deeper.png},
  	Author = {Shrutika Poyrekar and Sriraam Natarajan and Kristian Kersting},
  	Booktitle = {Working Notes of the AAAI Workshop on Statistical Relational Artificial Intelligence (StarAI)},
  	Keywords = {Statistical Relational AI, Lifted Inference, Symmetries, Grounding},
  	Note = {In this work-in-progress, we consider a lifted inference algorithm and analyze its scaling properties. We compare two versions of this algorithm -- the original implementation and a newer implementation built on a database. Our preliminary results show that constructing the factor graph from the relational model rather than the construction of the compressed model is the key bottleneck for the application of lifted inference in large domains.},
  	Title = {A Deeper Empirical Analysis of CBP Algorithm: Grounding Is the Bottleneck},
  	Url = {https://pdfs.semanticscholar.org/ab7a/c6b528b4774ce6965f3f18ec42da8c303989.pdf},
  	Year = {2014},
  	Bdsk-Url-1 = {https://pdfs.semanticscholar.org/ab7a/c6b528b4774ce6965f3f18ec42da8c303989.pdf}}

  @article{natarajan2014relational,
  	Anote = {./images/alzheimer2014cypernetic.png},
  	Author = {Sriraam Natarajan and Baidya Saha and Saket Joshi and Adam Edwards and Tushar Khot and Elizabeth M Davenport and Kristian Kersting and Christopher T Whitlow and Joseph A Maldjian},
  	Journal = {International Journal of Machine Learning and Cybernetics},
  	Keywords = {Statistical Relational Learning, Medicine, Alzheimer, Classification, Relational Grids},
  	Note = {Magnetic resonance imaging (MRI) has emerged as an important tool to identify intermediate biomarkers of Alzheimer's disease (AD) due to its ability to measure regional changes in the brain that are thought to reflect disease severity and progression. In this paper, we set out a novel pipeline that uses volumetric MRI data collected from different subjects as input and classifies them into one of three classes: AD, mild cognitive impairment (MCI) and cognitively normal (CN). Our pipeline consists of three stages---(1) a segmentation layer where brain MRI data is divided into clinically relevant regions; (2) a classification layer that uses relational learning algorithms to make pairwise predictions between the three classes; and (3) a combination layer that combines the results of the different classes to obtain the final classification. One of the key features of our proposed approach is that it allows for domain expert's knowledge to guide the learning in all the layers. We evaluate our pipeline on 397 patients acquired from the Alzheimer's Disease Neuroimaging Initiative and demonstrate that it obtains state-of-the-art performance with minimal feature engineering.},
  	Number = {5},
  	Pages = {659--669},
  	Publisher = {Springer},
  	Title = {Relational learning helps in three-way classification of alzheimer patients from structural magnetic resonance images of the brain},
  	Url = {https://link.springer.com/article/10.1007/s13042-013-0161-9},
  	Volume = {5},
  	Year = {2014},
  	Bdsk-Url-1 = {https://link.springer.com/article/10.1007/s13042-013-0161-9}}

  @article{bauckhage2014kunstliche,
  	Anote = {./images/spiele2014.png},
  	Author = {Christian Bauckhage and Kristian Kersting and Christian Thurau},
  	Journal = {Informatik-Spektrum},
  	Keywords = {Overview, Computer Games, Artificial Intelligence},
  	Note = {Die technische Entwicklung von Computerspielen und die Entwicklung von Methoden der K√ºnstlichen Intelligenz (KI) gehen seit Jahrzehnten Hand in Hand. Spektakul{\"a}re Erfolge der KI in Spieleszenarien sind etwa der Sieg des Schachcomputers Deep Blue √ºber den damaligen Weltmeister Gary Kasparow im Jahr 1997 oder der Gewinn der Quizshow Jeopardy durch das Programm Watson im Jahr 2010. Standen lange Zeit Fragen zur Implementierung m√∂glichst intelligenter und glaubw√ºrdiger k√ºnstlicher Spieler im Vordergrund, ergeben sich durch aktuelle Entwicklungen in den Bereichen mobile- und social gaming neue Problemstellungen f√ºr die KI. Dieser Artikel beleuchtet die historische Entwicklung der KI in Computerspielen und diskutiert die Herausforderungen, die sich in modernen Spieleszenarien ergeben.},
  	Number = {6},
  	Pages = {531--538},
  	Publisher = {Springer},
  	Title = {K√ºnstliche Intelligenz f√ºr Computerspiele},
  	Url = {https://link.springer.com/content/pdf/10.1007%2Fs00287-014-0822-4.pdf},
  	Volume = {37},
  	Year = {2014},
  	Bdsk-Url-1 = {https://link.springer.com/content/pdf/10.1007%2Fs00287-014-0822-4.pdf}}

  @misc{hadiji2013geodblp,
  	Anote = {./images/geoDBLP2013.png},
  	Author = {Fabian Hadiji and Kristian Kersting and Christian Bauckhage and Babak Ahmadi},
  	Howpublished = {arXiv preprint arXiv:1304.7984},
  	Keywords = {DBLP, Affiliations, Locations, Label Propahtion, Statistical Laws, Migration},
  	Note = {Official and commercial records of research migration are often access restricted, incompatible between countries, and especially not registered across researchers. Instead, we introduce GeoDBLP where we propagate geographical seed locations retrieved from the web across the DBLP database of 1,080,958 authors and 1,894,758 papers. But perhaps more important is that we are able to find statistical patterns and create models that explain the migration of researchers. For instance, we show that the science job market can be treated as a Poisson process with individual propensities to migrate following a log-normal distribution over the researcher's career stage. That is, although jobs enter the market constantly, researchers are generally not memoryless but have to care greatly about their next move. The propensity to make k>1 migrations, however, follows a gamma distribution suggesting that migration at later career stages is memoryless. This aligns well but actually goes beyond scientometric models typically postulated based on small case studies. On a very large, transnational scale, we establish the first general regularities that should have major implications on strategies for education and research worldwide.},
  	Title = {GeoDBLP: Geo-tagging dblp for mining the sociology of computer science},
  	Url = {https://arxiv.org/pdf/1304.7984.pdf},
  	Year = {2013},
  	Bdsk-Url-1 = {https://arxiv.org/pdf/1304.7984.pdf}}

  @misc{bauckhage2013efficient,
  	Anote = {./images/infTheoClust2013.png},
  	Author = {Christian Bauckhage and Kristian Kersting},
  	Howpublished = {arXiv preprint arXiv:1310.7114},
  	Keywords = {Clustering, Information-Theoretic, Discrete Lattices},
  	Note = {We consider the problem of clustering data that reside on discrete, low dimensional lattices. Canonical examples for this setting are found in image segmentation and key point extraction. Our solution is based on a recent approach to information theoretic clustering where clusters result from an iterative procedure that minimizes a divergence measure. We replace costly processing steps in the original algorithm by means of convolutions. These allow for highly efficient implementations and thus significantly reduce runtime. This paper therefore bridges a gap between machine learning and signal processing.},
  	Title = {Efficient information theoretic clustering on discrete lattices},
  	Url = {https://arxiv.org/pdf/1310.7114.pdf},
  	Year = {2013},
  	Bdsk-Url-1 = {https://arxiv.org/pdf/1310.7114.pdf}}

  @inproceedings{natarajan2013accelerating,
  	Anote = {./images/natarajan2013accelerating.png},
  	Author = {Sriraam Natarajan and Phillip Odom and Saket Joshi and Tushar Khot and Kristian Kersting and Prasad Tadepalli},
  	Booktitle = {Proceedings of the 23rd International Conference on Inductive Logic Programming (ILP)},
  	Keywords = {Statistical Relational Learning, Relational Imitation Learning, Transfer Learning, Relational Domains},
  	Note = {The problem of learning to mimic a human expert/teacher from training trajectories is called imitation learning. To make the process of teaching easier in this setting, we propose to employ transfer learning (where one learns on a source problem and transfers the knowledge to potentially more complex target problems). We consider multi-relational environments such as real-time strategy games and use functional-gradient boosting to capture and transfer the models learned in these environments. Our experiments demonstrate that our learner learns a very good initial model from the simple scenario and effectively transfers the knowledge to the more complex scenario thus achieving a jump start, a steeper learning curve and a higher convergence in performance.},
  	Organization = {Springer},
  	Pages = {64--75},
  	Title = {Accelerating imitation learning in relational domains via transfer by initialization},
  	Url = {./papers/natarajan2013accelerating.pdf},
  	Year = {2013},
  	Bdsk-Url-1 = {./papers/natarajan2013accelerating.pdf}}

  @inproceedings{bauckhage2013mathematical,
  	Anote = {./images/bauckhage2013mathematical.png},
  	Author = {Christian Bauckhage and Kristian Kersting and Fabian Hadiji},
  	Booktitle = {Proceedings of the 7th International AAAI Conference on Weblogs and Social Media (ICWSM)},
  	Keywords = {Collective Attention, Dynamics, Memes, Statistical Models},
  	Note = {Internet memes are a pervasive phenomenon on the social Web. They typically consist of viral catch phrases, images, or videos that spread through instant messaging, (micro) blogs, forums, and social networking sites. Due to their popularity and proliferation, Internet memes attract interest in areas as diverse as marketing, sociology, or computer science and have been dubbed a new form of communication or artistic expression. In this paper, we examine the merits of such claims and analyze how collective attention into Internet memes evolves over time. We introduce and discuss statistical models of the dynamics of fads and fit them to meme related time series obtained from Google Trends. Given data as to more than 200 memes, we find that our models provide more accurate descriptions of the dynamics of growth and decline of collective attention to individual Internet memes than previous approaches from the literature. In short, our results suggest that Internet memes are nothing but fads.},
  	Title = {Mathematical Models of Fads Explain the Temporal Dynamics of Internet Memes},
  	Url = {./papers/bauckhage2013mathematical.pdf},
  	Year = {2013},
  	Bdsk-Url-1 = {./papers/bauckhage2013mathematical.pdf}}

  @inproceedings{natarajan2013early,
  	Anote = {./images/natarajan2013early.png},
  	Author = {Sriraam Natarajan and Kristian Kersting and Edward Ip and David R Jacobs and Jeffrey Carr},
  	Booktitle = {Proceedings of the 25th Innovative Applications of Artificial Intelligence Conference (IAAI)},
  	Keywords = {Statistical Relational Learning, Longitudinal Study, Functional Gradient Boosting, Coronary Artery Calcification},
  	Note = {Coronary heart disease (CHD) is a major cause of death worldwide.In the U.S. CHD is responsible for approximated 1 in every 6 deaths with a coronary event occurring every 25 seconds and about 1 death every minute based on data current to 2007.Although a multitude of cardiovascular risks factors have been identified, CHD actually reflects complexinteractions of these factors over time. Today's datasets from longitudinal studies offer great promise to uncover these interactions but also pose enormous analytical problems due to typically large amount of both discrete and continuous measurements and risk factors with potential long-range interactions over time. Our investigation demonstrates that a statistical relational analysis of longitudinal data can easily uncover complex interactions of risks factors and actually predict future coronary artery calcification (CAC) levels --- an indicator of the risk of CHD present subclinically in an individual --- significantly better than traditional non-relational approaches. The uncovered long-range interactions between risk factors conform to existing clinical knowledgeand are successful in identifying risk factors at the early adult stage. This may contribute to monitoring young adults via smartphones and to designing patient-specific treatments in young adults to mitigate their risk later.},
  	Title = {Early Prediction of Coronary Artery Calcification Levels Using Machine Learning},
  	Url = {./papers/natarajan2013early.pdf},
  	Year = {2013},
  	Bdsk-Url-1 = {./papers/natarajan2013early.pdf}}

  @inproceedings{khot2013learning,
  	Anote = {./images/mlj2015boosting.png},
  	Author = {Tushar Khot and Sriraam Natarajan and Kristian Kersting and Jude Shavlik},
  	Booktitle = {Proceedings of the 23rd International Conference on Inductive Logic Programming (ILP)},
  	Keywords = {Statistical Relational Learning, Missing Data, EM, Functional Gradient Boosting},
  	Note = {Recent years have seen a surge of interest in learning the structure of Statistical Relational Learning (SRL) models that combine logic with probabilities. Most of these models apply the closed-world assumption i.e., whatever is not observed is false in the world. In this work, we consider the problem of learning the structure of SRL models in the presence of hidden data i.e. we open the closed-world assumption. We develop a functional-gradient boosting algorithm based on EM to learn the structure and parameters of the models simultaneously and apply it to learn different kinds of models -- Relational Dependency Networks, Markov Logic Networks and relational policies. Our results in a variety of domains demonstrate that the algorithms can effectively learn with missing data.},
  	Title = {Learning relational probabilistic models from partially observed data - Opening the closed-world assumption},
  	Url = {./papers/ilp2013openingCWA.pdf},
  	Year = {2013},
  	Bdsk-Url-1 = {./papers/ilp2013openingCWA.pdf}}

  @inproceedings{neumann2013coinciding,
  	Anote = {./images/neumann2013coinciding.png},
  	Author = {Marion Neumann and Roman Garnett and Kristian Kersting},
  	Booktitle = {Proceedings of the 5th Asian Conference on Machine Learning (ACML)},
  	Keywords = {Graph Kernels, Coinciding Walks, Random Walks},
  	Note = {Exploiting autocorrelation for node-label prediction in networked data has led to great success. However, when dealing with sparsely labeled networks, common in present-day tasks, the autocorrelation assumption is difficult to exploit. Taking a step beyond, we propose the coinciding walk kernel (cwk), a novel kernel leveraging label-structure similarity -- the idea that nodes with similarly arranged labels in their local neighbourhoods are likely to have the same label -- for learning problems on partially labeled graphs. Inspired by the success of random walk based schemes for the construction of graph kernels, cwk is defined in terms of the probability that the labels encountered during parallel random walks coincide. In addition to its intuitive probabilistic interpretation, coinciding walk kernels outperform existing kernel- and walk-based methods on the task of node-label prediction in sparsely labeled graphs with high label-structure similarity. We also show that computing cwks is faster than many state-of-the-art kernels on graphs. We evaluate cwks on several realworld networks, including cocitation and coauthor graphs, as well as a graph of interlinked populated places extracted from the dbpedia knowledge base.},
  	Pages = {357--372},
  	Title = {Coinciding Walk Kernels: Parallel Absorbing Random Walks for Learning with Graphs and Few Labels},
  	Url = {http://proceedings.mlr.press/v29/Neumann13.pdf},
  	Year = {2013},
  	Bdsk-Url-1 = {http://proceedings.mlr.press/v29/Neumann13.pdf}}

  @incollection{mahlein2013hypersectral,
  	Author = {Anne-Katrin Mahlein and Mirwaes Wahabzada and Kristian Kersting and Ulrike Steiner and Erich-Christian Oerke},
  	Booktitle = {Bornimer Agrartechnische Berichte: Proceedings of the 19. Workshop Computer-Bildanalyse in der Landwirtschaft},
  	Keywords = {Hyperspectral Images, Plant Phenotpying, Interpretable Matrix Factorization, Sustainability},
  	Note = {Optical hyperspectral sensors are promising tools for the detection and monitoring of plant diseases. Innovative sensor systems can provide detailed and highly resolved information on crop canopies and single plants in the visible, near infrared and shortwave infrared range (400 to 2500 nm). The interpretation and analysis of the complex and high dimensional hyperspectral data is challenging. Powerful and disease-specific analysis methods have to be developed for a successful evaluation of diseased plants by hyperspectral sensors. Based on the model system barley and the foliar diseases net blotch (caused by Pyrenophora teres), leaf rust (due to Puccinia hordei) and powdery mildew (due to Blumeria graminis hordei) full range spectral signatures of infected leaves were recorded using hyperspectral VIS/NIR and SWIR cameras. Spectral data cubes were analysed by extracting specific spectral signatures of disease symptoms during pathogenesis. Spectral patterns were observed with high resolution in time and space. Advanced data mining methods were used for the differentiation and quantification of diseased leaf tissue. An effective and automatic image analysis approach has been established by now. In a next step the models will be extended to other crops and their diseases.},
  	Pages = {154-158},
  	Title = {Hyperspectral image analysis for automatic detection of plant diseases},
  	Url = {https://opus4.kobv.de/opus4-slbp/frontdoor/index/index/docId/3617},
  	Year = {2013},
  	Bdsk-Url-1 = {https://opus4.kobv.de/opus4-slbp/frontdoor/index/index/docId/3617}}

  @incollection{ballvora2013deep,
  	Author = {Agim Ballvora and Christoph R√∂mer and Mirwaes Wahabzada and Uwe Rascher and Christian Thurau and Christian Bauckhage and Kristian Kersting and  Pl√ºmer and Jens Leon},
  	Booktitle = {Advance in Barley Sciences},
  	Keywords = {Sustainability, Plant Phenotyping, Simplex Volume Maximization},
  	Note = {The basic mechanisms of yield maintenance under drought conditions are far from being understood. Pre-symptomatic water stress recognition would help to get insides into complex plant mechanistic basis of plant response when confronted to water shortage conditions and is of great relevance in precision plant breeding and production. The plant reactions to drought stress result in spatial, temporal and tissue-specific pattern changes which can be detected using non-invasive sensor techniques, such as hyperspectral imaging. The ``response turning time-point'' in the temporal curve of plant response to stress rather than the maxima is the most relevant time-point for guided sampling to get insights into mechanistic basis of plant response to drought stress. Comparative hyperspectral image analysis was performed on barley (Hordeum vulgare) plants grown under well-watered and water stress conditions in two consecutive years. The obtained massive, high-dimensional data cubes were analysed with a recent matrix factorization technique based on simplex volume maximization of hyperspectral data and compared to several drought-related traits. The results show that it was possible to detect and visualize the accelerated senescence signature in stressed plants earlier than symptoms become visible by the naked eye.},
  	Pages = {317--326},
  	Publisher = {Springer},
  	Title = {``Deep Phenotyping'' of Early Plant Response to Abiotic Stress Using Non-invasive Approaches in Barley},
  	Url = {https://link.springer.com/chapter/10.1007%2F978-94-007-4682-4_26},
  	Year = {2013},
  	Bdsk-Url-1 = {https://link.springer.com/chapter/10.1007%2F978-94-007-4682-4_26}}

    @inproceedings{hadiji2013aaai_reduce,
    	Anote = {./images/hadiji2013reduce.png},
    	Author = {Fabian Hadiji and Kristian Kersting},
    	Booktitle = {Proceedings of the Twenty-Seventh AAAI Conference on Artificial
               Intelligence (AAAI)},
    	Keywords = {Statistical Relational AI, Lifted Inference, Re-lifting, MAP, Likelihood Maximization},
    	Note = {By handling whole sets of indistinguishable objects together, lifted belief propagation approaches have rendered large, previously intractable, probabilistic inference problems quickly solvable. In this paper, we show that Kumar and Zilberstein's likelihood maximization (LM) approach to MAP inference is liftable, too, and actually provides additional structure for optimization. Specifically, it has been recognized that some pseudo marginals may converge quickly, turning intuitively into pseudo evidence. This additional evidence typically changes the structure of the lifted network: it may expand or reduce it. The current lifted network, however, can be viewed as an upper bound on the size of the lifted network required to finish likelihood maximization. Consequently, we re-lift the network only if the pseudo evidence yields a reduced network, which can efficiently be computed on the current lifted network. Our experimental results on Ising models, image segmentation and relational entity resolution demonstrate that this bootstrapped LM via ``reduce and re-lift'' finds MAP assignments comparable to those found by the original LM approach, but in a fraction of the time.},
    	Title = {Reduce and Re-Lift: Bootstrapped Lifted Likelihood Maximization for MAP},
    	Url = {./papers/hadiji2013reduce.pdf},
    	Year = {2013},
    	Bdsk-Url-1 = {./papers/hadiji2013reduce.pdf}}

  @incollection{hadiji2013reduce,
  	Anote = {./images/hadiji2013reduce.png},
  	Author = {Fabian Hadiji and Kristian Kersting},
  	Booktitle = {Working Notes of the AAAI Workshop on Statistical Relational Artificial Intelligence (StarAI)},
  	Keywords = {Statistical Relational AI, Lifted Inference, Re-lifting, MAP, Likelihood Maximization},
  	Note = {By handling whole sets of indistinguishable objects together, lifted belief propagation approaches have rendered large, previously intractable, probabilistic inference problems quickly solvable. In this paper, we show that Kumar and Zilberstein's likelihood maximization (LM) approach to MAP inference is liftable, too, and actually provides additional structure for optimization. Specifically, it has been recognized that some pseudo marginals may converge quickly, turning intuitively into pseudo evidence. This additional evidence typically changes the structure of the lifted network: it may expand or reduce it. The current lifted network, however, can be viewed as an upper bound on the size of the lifted network required to finish likelihood maximization. Consequently, we re-lift the network only if the pseudo evidence yields a reduced network, which can efficiently be computed on the current lifted network. Our experimental results on Ising models, image segmentation and relational entity resolution demonstrate that this bootstrapped LM via ``reduce and re-lift'' finds MAP assignments comparable to those found by the original LM approach, but in a fraction of the time.},
  	Title = {Reduce and Re-Lift: Bootstrapped Lifted Likelihood Maximization for MAP},
  	Url = {./papers/hadiji2013reduce.pdf},
  	Year = {2013},
  	Bdsk-Url-1 = {./papers/hadiji2013reduce.pdf}}

  @incollection{neumann2013graph_ws,
  	Author = {Marion Neumann and Plinio Moreno and Laura Antanas and Roman Garnett and Kristian Kersting},
  	Booktitle = {Working Notes of the 11th workshop on mining and learning with graphs (MLG)},
  	Keywords = {Graph Kernels, Robotics, Grasping},
  	Note = {The choice of grasping region is highly dependent on the category of object, and the automated prediction of object category is the problem we focus on here. In this paper, we consider manifold information and semantic object parts in a graph kernel to predict categories of a large variety of household objects such as cups, pots, pans, bottles, and various tools. The similarity based category prediction is achieved by employing propagation kernels, a recently introduced graph kernel for partially labeled graphs, on graph representations of 3D point clouds of objects.},
  	Title = {Graph kernels for object category prediction in task-dependent robot grasping},
  	Url = {./papers/neumann2013graph.pdf},
  	Year = {2013},
  	Bdsk-Url-1 = {./papers/neumann2013graph.pdf}}

  @incollection{neumann2013coinciding_ws,
  	Author = {Marion Neumann and Roman Garnett and Kristian Kersting},
  	Booktitle = {Working Notes of the 11th Workshop on Mining and Learning with Graphs (MLG)},
  	Keywords = {Random Walks, Coinciding Walks, Graph Kernels},
  	Note = {Exploiting autocorrelation for node-label prediction in networked data has led to great success. However, when dealing with sparsely labeled networks, common in present-day tasks, the autocorrelation assumption is difficult to exploit. Taking a step beyond, we propose the coinciding walk kernel (cwk), a novel kernel leveraging label-structure similarity -- the idea that nodes with similarly arranged labels in their local neighbourhoods are likely to have the same label -- for learning problems on partially labeled graphs. Inspired by the success of random walk based schemes for the construction of graph kernels, cwk is defined in terms of the probability that the labels encountered during parallel random walks coincide. In addition to its intuitive probabilistic interpretation, coinciding walk kernels outperform existing kernel- and walk-based methods on the task of node-label prediction in sparsely labeled graphs with high label-structure similarity. We also show that computing cwks is faster than many state-of-the-art kernels on graphs. We evaluate cwks on several realworld networks, including cocitation and coauthor graphs, as well as a graph of interlinked populated places extracted from the dbpedia knowledge base.},
  	Title = {Coinciding Walk Kernels},
  	Year = {2013}}

  @incollection{ahmadi2013mapreduce,
  	Anote = {./images/ahmadi2013mapreduce.png},
  	Author = {Babak Ahmadi and Kristian Kersting and Sriraam Natarajan},
  	Booktitle = {Working Notes of the AAAI Workshop on Statistical Relational Artificial Intelligence (StarAI)},
  	Keywords = {Map Reduce, Lifted Inference, Belief Propagation, Statistical Relational AI},
  	Note = {Judging by the increasing impact of machine learning on large-scale data analysis in the last decade, one can anticipate a substantial growth in diversity of the machine learning applications for ``big data'' over the next decade. This exciting new opportunity, however, also raises many challenges. One of them is scaling inference within and training of graphical models. Typical ways to address this scaling issue are inference by approximate message passing, stochastic gradients, and MapReduce, among others. Often, we encounter inference and training problems with symmetries and redundancies in the graph structure. It has been shown that inference and training can indeed benefit from exploiting symmetries, for example by lifting loopy belief propagation (LBP).That is, a model is compressed by grouping nodes together that send and receive identical messages so that a modified LBP running on the lifted graph yields the same marginals as LBP on the original one, but often in a fraction of time. By establishing a link between lifting and radix sort, we show that lifting is MapReduce-able and thus combine the two orthogonal approaches to scaling inference, namely exploiting symmetries and employing parallel computations.},
  	Title = {MapReduce Lifting for Belief Propagation},
  	Url = {./papers/ahmadi2013mapreduce.pdf},
  	Year = {2013},
  	Bdsk-Url-1 = {./papers/ahmadi2013mapreduce.pdf}}

  @incollection{mladenov2013lifted,
  	Anote = {./images/mladenov2013lifted.png},
  	Author = {Martin Mladenov and Kristian Kersting},
  	Booktitle = {Working Notes of the AAAI Workshop on Statistical Relational Artificial Intelligence (StarAI)},
  	Keywords = {Statistical Relational AI, Lifted Inference, Sherali-Admas, k-Locality, Tighteting, MAP},
  	Note = {Lifted inference approaches exploit symmetries of a graphical model. So far, only the automorphism group of the graphical model has been proposed to formalize the symmetries used. We show that this is only the GIcomplete tip of a hierarchy and that the amount of lifting depends on how local the inference algorithm is: if the LP relaxation introduces constraints involving features over at most k variables, then the amount of lifting decreases monotonically with k. This induces a hierarchy of lifted inference algorithms, with lifted BP and MPLP at the bottom and exact inference methods at the top. In between, there are relaxations whose liftings are equitable partitions of intermediate coarseness, which all can be computed in polynomial time.},
  	Title = {Lifted Inference via k-Locality},
  	Url = {./papers/mladenov2013lifted.pdf},
  	Year = {2013},
  	Bdsk-Url-1 = {./papers/mladenov2013lifted.pdf}}

  @incollection{natarajan2013using_ws,
  	Author = {Sriraam Natarajan and Jose Picado and Tushar Khot and Kristian Kersting and Christopher Re and Jude W Shavlik},
  	Booktitle = {Proceedings of the AAAI Workshop on Statistical Relational Artificial Intelligence (StarAI)},
  	Keywords = {Distance Supervision, Statistical Relational Learning, Commonsense Knowledge, Functional Gradient Boosting},
  	Note = {One of the challenges to information extraction is the requirement of human annotated examples. Current successful approaches alleviate this problem by employing some form of distant supervision i.e., look into knowledge bases such as Freebase as a source of supervision to create more examples. While this is perfectly reasonable, most distant supervision methods rely on a hand coded background knowledge that explicitly looks for patterns in text. In this work, we take a different approach -- we create weakly supervised examples for relations by using commonsense knowledge. The key innovation is that this commonsense knowledge is completely independent of the natural language text. This helps when learning the full model for information extraction as against simply learning the parameters of a known CRF or MLN. We demonstrate on two domains that this form of weak supervision yields superior results when learning structure compared to simply using the gold standard labels.},
  	Pages = {13--16},
  	Title = {Using Commonsense Knowledge to Automatically Create (Noisy) Training Examples from Text},
  	Url = {./papers/natarajan2013using_ws.pdf},
  	Year = {2013},
  	Bdsk-Url-1 = {./papers/natarajan2013using_ws.pdf}}

  @incollection{bauckhage2013weibull,
  	Anote = {./images/bauckhage2013weibull.png},
  	Author = {Christian Bauckhage and Kristian Kersting and Bashir Rastegarpanah},
  	Booktitle = {Proceedings of the International Workshop on Mining and Learning with Graphs (MLG)},
  	Keywords = {Weibull, Shortest Path, Distance Distribution, Undirected Network},
  	Note = {We address the problem of characterizing shortest path his- tograms of networks in terms of continuous, analytically tractable distributions. Based on a recent model for the expected number of paths between arbitrary vertices in ran- dom networks, we establish the Weibull distribution as the corresponding distribution of minimal path lengths. Empir- ical tests with di erent graph topologies con rm our theo- retical prediction. Our methodology allows for computing non-linear low dimensional embeddings of path histograms for visual analytics.},
  	Title = {The Weibull as a model of shortest path distributions in random networks},
  	Url = {./papers/bauckhage2013weibull.pdf},
  	Year = {2013},
  	Bdsk-Url-1 = {./papers/bauckhage2013weibull.pdf}}

  @article{bauckhage2013can,
  	Author = {Christian Bauckhage and Kristian Kersting},
  	Journal = {K√ºnstliche Intelligenz (KI)},
  	Keywords = {Wisdom of the Crowd, Aesthetic, Images},
  	Note = {The social media revolution has led to an abundance of image and video data on the Internet. Since this data is typically annotated, rated, or commented upon by large communities, it provides new opportunities and challenges for computer vision. Social networking and content sharing sites seem to hold the key to the integration of context and semantics into image analysis. In this paper, we explore the use of social media in this regard. We present empirical results obtained on a set of 127,593 images with 3,741,176 tag assignments that were harvested from Flickr, a photo sharing site. We report on how users tag and rate photos and present an approach towards automatically recognizing the aesthetic appeal of images using confidence-based classifiers to alleviate effects due to ambiguously labeled data. Our results indicate that user generated content allows for learning about aesthetic appeal. In particular, established low-level image features seem to enable the recognition of beauty. A reliable recognition of unseemliness, on the other hand, appears to require more elaborate high-level analysis.},
  	Number = {1},
  	Pages = {25--35},
  	Publisher = {Springer},
  	Title = {Can Computers Learn from the Aesthetic Wisdom of the Crowd?},
  	Url = {https://link.springer.com/article/10.1007%2Fs13218-012-0232-1},
  	Volume = {27},
  	Year = {2013},
  	Bdsk-Url-1 = {https://link.springer.com/article/10.1007%2Fs13218-012-0232-1}}

  @article{ahmadi2013exploiting,
  	Anote = {./images/mlj2013lbp.png},
  	Author = {Babak Ahmadi and Kristian Kersting and Martin Mladenov and Sriraam Natarajan},
  	Journal = {Machine Learning (MLJ)},
  	Keywords = {Statistical Relational AI, Statistical Relational Learning, Symmetries, Lifted Inference, Parameter Estimation, Stochastic Gradient, Map Reduce},
  	Note = {Often, we encounter inference and training problems with symmetries and redundancies in the graph structure. In this paper, we show that inference and training can indeed benefit from exploiting symmetries. Specifically, we show that (loopy) belief propagation (BP) can be lifted. That is, a model is compressed by grouping nodes together that send and receive identical messages so that a modified BP running on the lifted graph yields the same marginals as BP on the original one, but often in a fraction of time. By establishing a link between lifting and radix sort, we show that lifting is MapReduce-able. Still, in many if not most situations training relational models will not benefit from this (scalable) lifting: symmetries within models easily break since variables become correlated by virtue of depending asymmetrically on evidence. An appealing idea for such situations is to train and recombine local models. This breaks long-range dependencies and allows to exploit lifting within and across the local training tasks. Moreover, it naturally paves the way for the first scalable lifted training approaches based on stochastic gradients, both in an online and a MapReduced fashion.},
  	Number = {1},
  	Pages = {91--132},
  	Publisher = {Springer},
  	Title = {Exploiting symmetries for scaling loopy belief propagation and relational training},
  	Url = {https://pdfs.semanticscholar.org/86ad/b234ca2a4bb9204ceaa66833b427f40b59df.pdf},
  	Volume = {92},
  	Year = {2013},
  	Bdsk-Url-1 = {https://pdfs.semanticscholar.org/86ad/b234ca2a4bb9204ceaa66833b427f40b59df.pdf}}

  @article{bauckhage2013data,
  	Author = {Christian Bauckhage and Kristian Kersting},
  	Journal = {K√ºnstliche Intelligenz (KI)},
  	Keywords = {Overview, Data Mining, Agriculture, Sustainability},
  	Note = {Modern communication, sensing, and actuator technologies as well as methods from signal processing, pattern recognition, and data mining are increasingly applied in agriculture. Developments such as increased mobility, wireless networks, new environmental sensors, robots, and the computational cloud put the vision of a sustainable agriculture for anybody, anytime, and anywhere within reach. Yet, precision farming is a fundamentally new domain for computational intelligence and constitutes a truly interdisciplinary venture. Accordingly, researchers and experts of complementary skills have to cooperate in order to develop models and tools for data intensive discovery that allow for operation through users that are not necessarily trained computer scientists. We present approaches and applications that address these challenges and underline the potential of data mining and pattern recognition in agriculture.},
  	Number = {4},
  	Pages = {313--324},
  	Publisher = {Springer},
  	Title = {Data mining and pattern recognition in agriculture},
  	Url = {https://link.springer.com/article/10.1007/s13218-013-0273-0},
  	Volume = {27},
  	Year = {2013},
  	Bdsk-Url-1 = {https://link.springer.com/article/10.1007/s13218-013-0273-0}}

  @misc{kersting2012say,
  	Anote = {./images/kersting2012say.png},
  	Author = {Kristian Kersting and Tapani Raiko},
  	Howpublished = {arXiv preprint arXiv:1207.1353},
  	Keywords = {Statistical Relational Learning, Structural EM, Structure Learning, Complex Sequences},
  	Note = {Many real world sequences such as protein secondary structures or shell logs exhibit a rich internal structures. Traditional probabilistic models of sequences, however, consider sequences of flat symbols only. Logical hidden Markov models have been proposed as one solution. They deal with logical sequences, i.e., sequences over an alphabet of logical atoms. This comes at the expense of a more complex model selection problem. Indeed, different abstraction levels have to be explored. In this paper, we propose a novel method for selecting logical hidden Markov models from data called SAGEM. SAGEM combines generalized expectation maximization, which optimizes parameters, with structure search for model selection using inductive logic programming refinement operators. We provide convergence and experimental results that show SAGEM's effectiveness.},
  	Title = {'Say EM'for selecting probabilistic models for logical sequences},
  	Url = {https://arxiv.org/pdf/1207.1353.pdf},
  	Year = {2012},
  	Bdsk-Url-1 = {https://arxiv.org/pdf/1207.1353.pdf}}

  @misc{blockeel2012revised,
  	Author = {Hendrik Blockeel and Kristian Kersting and Siegfried Nijssen and Filip Zelezny},
  	Howpublished = {arXiv preprint arXiv:1207.6324},
  	Keywords = {Publication model},
  	Note = {ECML PKDD is the main European conference on machine learning and data mining. Since its foundation it implemented the publication model common in computer science: there was one conference deadline; conference submissions were reviewed by a program committee; papers were accepted with a low acceptance rate. Proceedings were published in several Springer Lecture Notes in Artificial (LNAI) volumes, while selected papers were invited to special issues of the Machine Learning and Data Mining and Knowledge Discovery journals. In recent years, this model has however come under stress. Problems include: reviews are of highly variable quality; the purpose of bringing the community together is lost; reviewing workloads are high; the information content of conferences and journals decreases; there is confusion among scientists in interdisciplinary contexts. In this paper, we present a new publication model, which will be adopted for the ECML PKDD 2013 conference, and aims to solve some of the problems of the traditional model. The key feature of this model is the creation of a journal track, which is open to submissions all year long and allows for revision cycles.},
  	Title = {A revised publication model for ECML PKDD},
  	Url = {https://arxiv.org/pdf/1207.6324.pdf},
  	Year = {2012},
  	Bdsk-Url-1 = {https://arxiv.org/pdf/1207.6324.pdf}}

  @misc{wahabzada2012latent,
  	Anote = {./images/wahabzada2012latent.png},
  	Author = {Mirwaes Wahabzada and Kristian Kersting and Christian Bauckhage and Christoph R√∂mer and Agim Ballvora and Francisco Pinto and Uwe Rascher and Jens Leon and Lutz Pl√ºmer},
  	Howpublished = {arXiv preprint arXiv:1210.4919},
  	Keywords = {Plant Phenotyping, Sustainability, Latent Dirichlet Allocation, Online, Variational},
  	Note = {Understanding the adaptation process of plants to drought stress is essential in improving management practices, breeding strategies as well as engineering viable crops for a sustainable agriculture in the coming decades. Hyper-spectral imaging provides a particularly promising approach to gain such understanding since it allows to discover non-destructively spectral characteristics of plants governed primarily by scattering and absorption characteristics of the leaf internal structure and biochemical constituents. Several drought stress indices have been derived using hyper-spectral imaging. However, they are typically based on few hyper-spectral images only, rely on interpretations of experts, and consider few wavelengths only. In this study, we present the first data-driven approach to discovering spectral drought stress indices, treating it as an unsupervised labeling problem at massive scale. To make use of short range dependencies of spectral wavelengths, we develop an online variational Bayes algorithm for latent Dirichlet allocation with convolved Dirichlet regularizer. This approach scales to massive datasets and, hence, provides a more objective complement to plant physiological practices. The spectral topics found conform to plant physiological knowledge and can be computed in a fraction of the time compared to existing LDA approaches.},
  	Title = {Latent dirichlet allocation uncovers spectral characteristics of drought stressed plants},
  	Url = {https://arxiv.org/pdf/1210.4919.pdf},
  	Year = {2012},
  	Bdsk-Url-1 = {https://arxiv.org/pdf/1210.4919.pdf}}

  @inproceedings{kersting2012simplex,
  	Anote = {./images/kersting2012simplex.png},
  	Author = {Kristian Kersting and Mirwaes Wahabzada and Christoph R√∂mer and Christian Thurau and Agim Ballvora and Uwe Rascher and Jens Leon and Christian Bauckhage and Lutz Pl√ºmer},
  	Booktitle = {Proceedings of the SIAM International Conference on Data Mining (SDM)},
  	Keywords = {Simplex Volume Maximization, Eurclidean Embeddding, Simplex Distribuation, KL-divergence},
  	Note = {Early stress recognition is of great relevance in precision plant protection. Pre-symptomatic water stress detection is of particular interest, ultimately helping to meet the challenge of ``How to feed a hungry world?''. Due to the climate change, this is of considerable political and public interest. Due to its large-scale and temporal nature, e.g., when monitoring plants using hyper-spectral imaging, and the demand of physical meaning of the results, it presents unique computational problems in scale and interpretability. However, big data matrices over time also arise in several other real-life applications such as stock market monitoring where a business sector is characterized by the ups and downs of each of its companies per year or topic monitoring of document collections. Therefore, we consider the general problem of embedding data matrices into Euclidean space over time without making any assumption on the generating distribution of each matrix. To do so, we represent all data samples by means of convex combinations of only few extreme ones computable in linear time. On the simplex spanned by the extremes, there are then natural candidates for distributions inducing distances between and in turn embeddings of the data matrices. We evaluate our method across several domains, including synthetic, text, and financial data as well as a large-scale dataset on water stress detection in plants with more than 3 billion matrix entries. The results demonstrate that the embeddings are meaningful and fast to compute. The stress detection results were validated by a domain expert and conform to existing plant physiological knowledge. Read More: http://epubs.siam.org/doi/abs/10.1137/1.9781611972825.26},
  	Organization = {SIAM},
  	Pages = {295--306},
  	Title = {Simplex distributions for embedding data matrices over time},
  	Url = {http://epubs.siam.org/doi/pdf/10.1137/1.9781611972825.26},
  	Year = {2012},
  	Bdsk-Url-1 = {http://epubs.siam.org/doi/pdf/10.1137/1.9781611972825.26}}

  @inproceedings{thurau2012deterministic,
  	Anote = {./images/thurau2012deterministic.png},
  	Author = {Christian Thurau and Kristian Kersting and Christian Bauckhage},
  	Booktitle = {Proceedings of the SIAM International Conference on Data Mining (SDM)},
  	Keywords = {CUR, Matrix Factorization, Simplex Volume Maximization, Deterministic},
  	Note = {Low-rank approximations which are computed from selected rows and columns of a given data matrix have attracted considerable attention lately. They have been proposed as an alternative to the SVD because they naturally lead to interpretable decompositions which was shown to be successful in application such as fraud detection, fMRI segmentation, and collaborative filtering. The CUR decomposition of large matrices, for example, samples rows and columns according to a probability distribution that depends on the Euclidean norm of rows or columns or on other measures of statistical leverage. At the same time, there are various deterministic approaches that do not resort to sampling and were found to often yield factorization of superior quality with respect to reconstruction accuracy. However, these are hardly applicable to large matrices as they typically suffer from high computational costs. Consequently, many practitioners in the field of data mining have abandon deterministic approaches in favor of randomized ones when dealing with today's large-scale data sets. In this paper, we empirically disprove this prejudice. We do so by introducing a novel, linear-time, deterministic CUR approach that adopts the recently introduced Simplex Volume Maximization approach for column selection. The latter has already been proven to be successful for NMF-like decompositions of matrices of billions of entries. Our exhaustive empirical study on more than 30 synthetic and real-world data sets demonstrates that it is also beneficial for CUR-like decompositions. Compared to other deterministic CUR-like methods, it provides comparable reconstruction quality but operates much faster so that it easily scales to matrices of billions of elements. Compared to sampling-based methods, it provides competitive reconstruction quality while staying in the same run-time complexity class.},
  	Organization = {SIAM},
  	Pages = {684--695},
  	Title = {Deterministic CUR for improved large-scale data analysis: An empirical study},
  	Url = {http://epubs.siam.org/doi/pdf/10.1137/1.9781611972825.59},
  	Year = {2012},
  	Bdsk-Url-1 = {http://epubs.siam.org/doi/pdf/10.1137/1.9781611972825.59}}

  @inproceedings{mladenov2012lifted,
  	Anote = {./images/mladenov2012lifted.png},
  	Author = {Martin Mladenov and Babak Ahmadi and Kristian Kersting},
  	Booktitle = {Proceedings of the 15th International Conference on Artificial Intelligence and Statistics (AISTATS)},
  	Keywords = {Symmetries, Lifted Inference, Linear Programs, Weisfeiler Lehmann, Colour Refinement},
  	Note = {Lifted inference approaches have rendered large, previously intractable probabilistic inference problems quickly solvable by handling whole sets of indistinguishable objects together. Triggered by this success, we show that another important AI technique is liftable, too, namely linear programming. Intuitively, given a linear program (LP), we employ a lifted variant of Gaussian belief propagation (GaBP) to solve the systems of linear equations arising when running an interiorpoint method to solve the LP. However, this na¬®ƒ±ve solution cannot make use of standard solvers for linear equations and is doomed to construct lifted networks in each iteration of the interior-point method again, an operation that can itself be quite costly. To address both issues, we show how to read off an equivalent LP from the lifted GaBP computations that can be solved using any off-the-shelf LP solver. We prove the correctness of this compilation approac and experimentally demonstrate that it can greatly reduce the cost of solving LPs.},
  	Title = {Lifted Linear Programming},
  	Url = {http://proceedings.mlr.press/v22/mladenov12/mladenov12.pdf},
  	Year = {2012},
  	Bdsk-Url-1 = {http://proceedings.mlr.press/v22/mladenov12/mladenov12.pdf}}

  @inproceedings{schiegg2012markov,
  	Anote = {./images/schiegg2012markov.png},
  	Author = {Martin Schiegg and Marion Neumann and Kristian Kersting},
  	Booktitle = {Proceedings of the 15th International Conference on Artificial Intelligence and Statistics (AISTATS)},
  	Keywords = {Statistical Relational Learning, Markov Logic Networks, Gaussian Processes, Joint Training},
  	Note = {We propose a novel mixtures of Gaussian processes model in which the gating function is interconnected with a probabilistic logical model, in our case Markov logic networks. In this way, the resulting mixed graphical model, called Markov logic mixtures of Gaussian processes (MLxGP), solves joint Bayesian non-parametric regression and probabilistic relational inference tasks. In turn, MLxGP facilitates novel, interesting tasks such as regression based on logical constraints or drawing probabilistic logical conclusions about regression data, thus putting ``machines reading regression data'' in reach.},
  	Pages = {1002--1011},
  	Title = {Markov Logic Mixtures of Gaussian Processes: Towards Machines Reading Regression Data},
  	Url = {http://proceedings.mlr.press/v22/schiegg12/schiegg12.pdf},
  	Year = {2012},
  	Bdsk-Url-1 = {http://proceedings.mlr.press/v22/schiegg12/schiegg12.pdf}}

  @inproceedings{kersting2012pre,
  	Anote = {./images/kersting2012pre.png},
  	Author = {Kristian Kersting and Zhao Xu and Mirwaes Wahabzada and Christian Bauckhage and Christian Thurau and Christoph Roemer and Agim Ballvora and Uwe Rascher and Jens Leon and Lutz Pl√ºmer},
  	Booktitle = {Proceedings of the 26th AAAI Conference on Artificial Intelligence (AAAI)},
  	Keywords = {Sustainability, Prediction, Dirichlet-Aggregation, Regression, Gaussian Processes, Simplex Volume Maximization, Plant Phenotyping},
  	Note = {Pre-symptomatic drought stress prediction is of great relevance in precision plant protection, ultimately helping to meet the challenge of How to feed a hungry world?. Unfortunately, it also presents unique computational problems in scale and interpretability: it is a temporal, large-scale prediction task, e.g., when monitoring plants over time using hyperspectral imaging, and features are `things' with a `biological' meaning and interpretation and not just mathematical abstractions computable for any data. In this paper we propose Dirichlet-aggregation regression (DAR) to meet the challenge. DAR represents all data by means of convex combinations of only few extreme ones computable in linear time and easy to interpret.Then, it puts a Gaussian process prior on the Dirichlet distributions induced on the simplex spanned by the extremes. The prior can be a function of any observed meta feature such as time, location, type of fertilization, and plant species. We evaluated DAR on two hyperspectral image series of plants over time with about 2 (resp. 5.8) Billion matrix entries. The results demonstrate that DAR can be learned efficiently and predicts stress well before it becomes visible to the human eye.},
  	Title = {Pre-Symptomatic Prediction of Plant Drought Stress Using Dirichlet-Aggregation Regression on Hyperspectral Images},
  	Url = {./papers/kersting2012pre.pdf},
  	Year = {2012},
  	Bdsk-Url-1 = {./papers/kersting2012pre.pdf}}

  @inproceedings{kersting2012matrix,
  	Anote = {./images/kersting2012matrix.png},
  	Author = {Kristian Kersting and Christian Bauckhage and Christian Thurau and Mirwaes Wahabzada},
  	Booktitle = {Proceedings of the European Conference on Machine Learning and Principles of Knowledge Discovery in Databases (ECML PKDD)},
  	Keywords = {Interpreational Matrix Factorization, Achetypal Analysis, Simplex Volume Maximization, Search},
  	Note = {Simplex Volume Maximization (SiVM) exploits distance geometry for effciently factorizing gigantic matrices. It was proven successful in game, social media, and plant mining. Here, we review the distance geometry approach and argue that it generally suggests to factorize gigantic matrices using search-based instead of optimization techniques.},
  	Pages = {850--853},
  	Publisher = {Springer},
  	Title = {Matrix factorization as search},
  	Url = {http://www.cs.bris.ac.uk/~flach/ECMLPKDD2012papers/1125818.pdf},
  	Year = {2012},
  	Bdsk-Url-1 = {http://www.cs.bris.ac.uk/~flach/ECMLPKDD2012papers/1125818.pdf}}

  @inproceedings{neumann2012efficient,
  	Anote = {./images/propKernel_mlg2016.png},
  	Author = {Marion Neumann and Novi Patricia and Roman Garnett and Kristian Kersting},
  	Booktitle = {Proceedings of the European Conference on Machine Learning and Principles of Knowledge Discovery in Databases (ECML PKDD)},
  	Keywords = {Graph Kernels, Information Propagation, Random Walks, Weisfeiler Lehmann, Symmtries},
  	Note = {Learning from complex data is becoming increasingly important, and graph kernels have recently evolved into a rapidly developing branch of learning on structured data. However, previously proposed kernels rely on having discrete node label information. In this paper, we explore the power of continuous node-level features for propagation-based graph kernels. Specifically, propagation kernels exploit node label distributions from propagation schemes like label propagation, which naturally enables the construction of graph kernels for partially labeled graphs. In order to effciently extract graph features from continuous node label distributions, and in general from continuous vector-valued node attributes, we utilize randomized techniques, which easily allow for deriving similarity measures based on propagated information. We show that propagation kernels utilizing locality-sensitive hashing reduce the runtime of existing graph kernels by several orders of magnitude. We evaluate the performance of various propagation kernels on real-world bioinformatics and image benchmark datasets.},
  	Pages = {378--393},
  	Publisher = {Springer},
  	Title = {Efficient graph kernels by randomization},
  	Url = {http://www.cs.bris.ac.uk/~flach/ECMLPKDD2012papers/1125542.pdf},
  	Year = {2012},
  	Bdsk-Url-1 = {http://www.cs.bris.ac.uk/~flach/ECMLPKDD2012papers/1125542.pdf}}

  @inproceedings{bauckhage2012players,
  	Anote = {./images/bauckhage2012players.png},
  	Author = {Christian Bauckhage and Kristian Kersting and Rafet Sifa and Christian Thurau and Anders Drachen and Alessandro Canossa},
  	Booktitle = {Proceedings of the IEEE Conference on Computational Intelligence and Games (CIG)},
  	Keywords = {Collective Attention, Weilbull, Computer Games},
  	Note = {Analyzing telemetry data of player behavior in computer games is a topic of increasing interest for industry and research, alike. When applied to game telemetry data, pattern recognition and statistical analysis provide valuable business intelligence tools for game development. An important problem in this area is to characterize how player engagement in a game evolves over time. Reliable models are of pivotal interest since they allow for assessing the long-term success of game products and can provide estimates of how long players may be expected to keep actively playing a game. In this paper, we introduce methods from random process theory into game data mining in order to draw inferences about player engagement. Given large samples (over 250,000 players) of behavioral telemetry data from five different action-adventure and shooter games, we extract information as to how long individual players have played these games and apply techniques from lifetime analysis to identify common patterns. In all five cases, we find that the Weibull distribution gives a good account of the statistics of total playing times. This implies that an average player's interest in playing one of the games considered evolves according to a non-homogeneous Poisson process. Therefore, given data on the initial playtime behavior of the players of a game, it becomes possible to predict when they stop playing.},
  	Organization = {IEEE},
  	Pages = {139--146},
  	Title = {How players lose interest in playing a game: An empirical study based on distributions of total playing times},
  	Url = {./papers/bauckhage2012players.pdf},
  	Year = {2012},
  	Bdsk-Url-1 = {./papers/bauckhage2012players.pdf}}

  @inproceedings{ahmadi2012lifted,
  	Anote = {./images/ahmadi2012lifted.png},
  	Author = {Babak Ahmadi and Kristian Kersting and Sriraam Natarajan},
  	Booktitle = {Proceedings of the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD)},
  	Date-Modified = {2017-04-30 06:59:10 +0000},
  	Keywords = {Statistical Relational Learning, Symmetries, Stochastic Gradients, Relational Domain},
  	Note = {Lifted inference approaches have rendered large, previously intractable probabilistic inference problems quickly solvable by employing symmetries to handle whole sets of indistinguishable random variables. Still, in many if not most situations training relational models will not benefit from lifting: symmetries within models easily break since variables become correlated by virtue of depending asymmetrically on evidence. An appealing idea for such situations is to train and recombine local models. This breaks long-range dependencies and allows to exploit lifting within and across the local training tasks. Moreover, it naturally paves the way for online training for relational models. Specifically, we develop the first lifted stochastic gradient optimization method with gain vector adaptation, which processes each lifted piece one after the other. On several datasets, the resulting optimizer converges to the same quality solution over an order of magnitude faster, simply because unlike batch training it starts optimizing long before having seen the entire mega-example even once.},
  	Organization = {Springer},
  	Title = {Lifted Online Training of Relational Models with Stochastic Gradients},
  	Url = {./papers/ahmadi2012lifted.pdf},
  	Year = {2012},
  	Bdsk-Url-1 = {./papers/ahmadi2012lifted.pdf}}

  @inproceedings{kersting2012lifted,
  	Anote = {./images/kersting2012lifted.png},
  	Author = {Kristian Kersting},
  	Booktitle = {Proceedings of the 20th European conference on Artificial Intelligence (ECAI)},
  	Keywords = {Overview, Lifted Inference, Statistical Relational AI},
  	Note = {Many AI problems arising in a wide variety of fields such as machine learning, semantic web, network communication, computer vision, and robotics can elegantly be encoded and solved using probabilistic graphical models. Often, however, we are facing inference problems with symmetries and redundancies only implicitly captured in the graph structure and, hence, not exploitable by ef- ficient inference approaches. A prominent example are probabilistic logical models that tackle a long standing goal of AI, namely unifying first-order logic --- capturing regularities and symmetries --- and probability --- capturing uncertainty. Although they often encode large, complex models using few rules only and, hence, symmetries and redundancies abound, inference in them was originally still at the propositional representation level and did not exploit symmetries. This paper is intended to give a (not necessarily complete) overview and invitation to the emerging field of lifted probabilistic inference, inference techniques that exploit these symmetries in graphical models in order to speed up inference, ultimately orders of magnitude.},
  	Pages = {33--38},
  	Publisher = {IOS Press},
  	Title = {Lifted probabilistic inference},
  	Url = {./papers/kersting2012lifted.pdf},
  	Year = {2012},
  	Bdsk-Url-1 = {./papers/kersting2012lifted.pdf}}

  @inproceedings{fierens2012pairwise,
  	Anote = {./images/fierens2012pairwise.png},
  	Author = {Daan Fierens and Kristian Kersting and Jesse Davis and Jian Chen and Martin Mladenov},
  	Booktitle = {Proceedings of the 22nd International Conference on Inductive Logic Programming (ILP)},
  	Keywords = {Statistical Relational Learning, Statistical Relational AI, Markov Logic Networks, Pairwise},
  	Note = {We introduce pairwise Markov Logic, a subset of Markov Logic where each formula contains at most two atoms. We show that every non-pairwise Markov Logic Network (MLN) can be transformed or `reduced' to a pairwise MLN. Thus, existing, highly efficient probabilistic inference methods can be employed for pairwise MLNs without the overhead of devising or implementing high-order variants. Experiments on two relational datasets confirm the usefulness of this reduction approach.},
  	Organization = {Springer},
  	Pages = {58--73},
  	Title = {Pairwise Markov Logic},
  	Url = {./papers/fierens2012pairwise.pdf},
  	Year = {2012},
  	Bdsk-Url-1 = {./papers/fierens2012pairwise.pdf}}

  @inproceedings{zamani2012symbolic,
  	Anote = {./images/zamani2012symbolic.png},
  	Author = {Zahra Zamani and Scott Sanner and Pascal Poupart and Kristian Kersting},
  	Booktitle = {Proceedings of Neural Information Processing Systems (NIPS)},
  	Keywords = {Statistical Relational AI, Relational Reinforcement Learning, Lifted Inference, Symboic Dynamic Programming, Continous States, Continuous Actions},
  	Note = {Point-based value iteration (PBVI) methods have proven extremely effective for finding (approximately) optimal dynamic programming solutions to partiallyobservable Markov decision processes (POMDPs) when a set of initial belief states is known. However, no PBVI work has provided exact point-based backups for both continuous state and observation spaces, which we tackle in this paper. Our key insight is that while there may be an infinite number of observations, there are only a finite number of continuous observation partitionings that are relevant for optimal decision-making when a finite, fixed set of reachable belief states is considered. To this end, we make two important contributions: (1) we show how previous exact symbolic dynamic programming solutions for continuous state MDPs can be generalized to continuous state POMDPs with discrete observations, and (2) we show how recently developed symbolic integration methods allow this solution to be extended to PBVI for continuous state and observation POMDPs with potentially correlated, multivariate continuous observation spaces.},
  	Pages = {1394--1402},
  	Title = {Symbolic dynamic programming for continuous state and observation POMDPs},
  	Url = {https://papers.nips.cc/paper/4756-symbolic-dynamic-programming-for-continuous-state-and-observation-pomdps.pdf},
  	Year = {2012},
  	Bdsk-Url-1 = {https://papers.nips.cc/paper/4756-symbolic-dynamic-programming-for-continuous-state-and-observation-pomdps.pdf}}

  @inproceedings{natarajan2012machine,
  	Anote = {./images/natarajan2012machine.png},
  	Author = {Sriraam Natarajan and Saket Joshi and Baidya Nath Saha and Adam Edwards and Tushar Khot and Elizabeth Moody and Kristian Kersting and Christopher T Whitlow and Joseph A Maldjian},
  	Booktitle = {Proceedings of the 11th International Conference on Machine Learning and Applications (ICMLA)},
  	Keywords = {Alzheimer, Medicine, Statistical Relational Learning, Functional Gradient Boosting, Relational Grids},
  	Note = {Magnetic resonance imaging (MRI) has emerged as an important tool to identify intermediate biomarkers of Alzheimer's disease (AD) due to its ability to measure regional changes in the brain that are thought to reflect disease severity and progression. In this paper, we set out a novel pipeline that uses volumetric MRI data collected from different subjects as input and classifies them into one of three classes: AD, mild cognitive impairment (MCI) and cognitively normal (CN). Our pipeline consists of three stages -- (1) a segmentation layer where brain MRI data is divided into clinically relevant regions; (2) a classification layer that uses relational learning algorithms to make pairwise predictions between the three classes; and (3) a combination layer that combines the results of the different classes to obtain the final classification. One of the key features of our proposed approach is that it allows for domain expert's knowledge to guide the learning in all the layers. We evaluate our pipeline on 397 patients acquired from the Alzheimer's Disease Neuroimaging Initiative and demonstrate that it obtains state-oft-he-art performance with minimal feature engineering.},
  	Organization = {IEEE},
  	Pages = {203--208},
  	Title = {A machine learning pipeline for three-way classification of alzheimer patients from structural magnetic resonance images of the brain},
  	Url = {./papers/natarajan2012machine.pdf},
  	Volume = {1},
  	Year = {2012},
  	Bdsk-Url-1 = {./papers/natarajan2012machine.pdf}}

  @inproceedings{xu2012efficient,
  	Anote = {./images/xu2012efficient.png},
  	Author = {Zhao Xu and Kristian Kersting and Christian Bauckhage},
  	Booktitle = {Proceedings of the IEEE 12th International Conference on Data Mining (ICDM)},
  	Keywords = {Learning to Hash, Proportional Data, Simplex Volume Maximization, Ultra-Fast, Approximate Nearest-Neighbour},
  	Note = {Spectral hashing (SH) seeks compact binary codes of data points so that Hamming distances between codes correlate with data similarity. Quickly learning such codes typically boils down to principle component analysis (PCA). However, this is only justified for normally distributed data. For proportional data (normalized histograms), this is not the case. Due to the sum-to-unity constraint, features that are as independent as possible will not all be uncorrelated. In this paper, we show that a linear-time transformation efficiently copes with sum-to-unity constraints: first, we select a small number K of diverse data points by maximizing the volume of the simplex spanned by these prototypes; second, we represent each data point by means of its cosine similarities to the K selected prototypes. This maximum volume hashing is sensible since each dimension in the transformed space is likely to follow a von Mises (vM) distribution, and, in very high dimensions, the vM distribution closely resembles a Gaussian distribution. This justifies to employ PCA on the transformed data. Our extensive experiments validate this: maximum volume hashing outperforms spectral hashing and other state of the art techniques.},
  	Organization = {IEEE},
  	Pages = {735--744},
  	Title = {Efficient learning for hashing proportional data},
  	Url = {./papers/xu2012efficient.pdf},
  	Year = {2012},
  	Bdsk-Url-1 = {./papers/xu2012efficient.pdf}}

  @inproceedings{wahabzada2012uai,
  	Anote = {./images/wahabzada2012latent.png},
  	Author = {Mirwaes Wahabzada and Kristian Kersting and Christian Bauckhage and Christoph R√∂mer and Agim Ballvora and Francisco Pinto and Uwe Rascher and Jens Leon and Lutz Pl√ºmer},
  	Booktitle = {Proceedings of the 28th Conference on Uncertainty in Artificial Intelligence (UAI)},
  	Keywords = {Plant Phenotyping, Sustainability, Latent Dirichlet Allocation, Online, Variational},
  	Note = {Understanding the adaptation process of plants to drought stress is essential in improving management practices, breeding strategies as well as engineering viable crops for a sustainable agriculture in the coming decades. Hyper-spectral imaging provides a particularly promising approach to gain such understanding since it allows to discover non-destructively spectral characteristics of plants governed primarily by scattering and absorption characteristics of the leaf internal structure and biochemical constituents. Several drought stress indices have been derived using hyper-spectral imaging. However, they are typically based on few hyper-spectral images only, rely on interpretations of experts, and consider few wavelengths only. In this study, we present the first data-driven approach to discovering spectral drought stress indices, treating it as an unsupervised labeling problem at massive scale. To make use of short range dependencies of spectral wavelengths, we develop an online variational Bayes algorithm for latent Dirichlet allocation with convolved Dirichlet regularizer. This approach scales to massive datasets and, hence, provides a more objective complement to plant physiological practices. The spectral topics found conform to plant physiological knowledge and can be computed in a fraction of the time compared to existing LDA approaches.},
  	Title = {Latent dirichlet allocation uncovers spectral characteristics of drought stressed plants},
  	Url = {https://arxiv.org/pdf/1210.4919.pdf},
  	Year = {2012},
  	Bdsk-Url-1 = {https://arxiv.org/pdf/1210.4919.pdf}}

  @incollection{khot2012structure,
  	Author = {Tushar Khot and Sriraam Natarajan and Kristian Kersting and Jude Shavlik},
  	Booktitle = {Working Notes of the ICML Workshop on Statistical Relational Learning (SRL)},
  	Keywords = {Statistical Relational Learning, Functional Gradient Boosting, Missing Data, Markov Logic Networks},
  	Note = {Recent years have seen a surge of interest in learning the structure of Statistical Relational Learning (SRL) models, which combine logic with probabilities. Most of these models apply the closed-world assumption i.e., whatever is not observed is false in the world. We consider the problem of learning the structure of SRL models in the presence of hidden data, i.e. we open the closed-world assumption. We develop a functional-gradient boosting algorithm based on EM to learn the structure and parameters of the models simultaneously and apply it to learn two kinds of models -- Relational Dependency Networks (RDNs) and Markov Logic Networks (MLNs). Our results in two testbeds demonstrate that the algorithms can effectively learn with missing data.},
  	Title = {Structure Learning with Hidden Data in Relational Domains},
  	Url = {./papers/khot2012structure.pdf},
  	Year = {2012},
  	Bdsk-Url-1 = {./papers/khot2012structure.pdf}}

  @incollection{neumann2012propagation,
  	Author = {Marion Neumann and Roman Garnett and Plinio Moreno and Novi Patricia and Kristian Kersting},
  	Booktitle = {Working Notes of the ICML Workshop on Mining and Learning with Graphs (MLG)},
  	Keywords = {GraphKernels, Weisfeiler Lehmann, Random Walks, Label Propagation},
  	Note = {Learning from complex data is becoming increasingly important, and graph kernels have recently evolved into a rapidly developing branch of learning on structured data. However, previously proposed kernels rely on having discrete node label information. Propagation kernels leverage the power of continuous node label distributions as graph features and hence, enhance traditional graph kernels to efficiently handle partially labeled graphs in a principled manner. Utilizing localitysensitive hashing, propagation kernels are able to outperform state-of-the-art graph kernels in terms of runtime without loss in prediction accuracy. This paper investigates the power of propagation kernels to classify partially labeled images and to tackle the challenging problem of retrieving similar object views in robotic grasping},
  	Title = {Propagation kernels for partially labeled graphs},
  	Year = {2012}}

  @incollection{natarajan2012early,
  	Author = {Sriraam Natarajan and Kristian Kersting and Saket Joshi and Santiago Saldana and Edward Ip and D Jacobs and Jeffrey Carr},
  	Booktitle = {Working Notes of the ICML Workshop on Machine Learning for Clinical Data Analysis},
  	Keywords = {Statistical Relational Learning, Coronary Artery Calcification, Medicine, Longitudinal Study, Funcational Gradient Boosting},
  	Note = {Our investigation demonstrates that a statistical relational analysis of longitudinal data can easily uncover complex interactions of risks factors and actually predict future coronary artery calcification (CAC) levels --- an indicator of the risk of CHD present subclinically in an individual --- significantly better than traditional non-relational approaches.},
  	Pages = {30},
  	Title = {Early Prediction of Coronary Artery Calcification Levels Using Statistical Relational Learning},
  	Year = {2012}}

  @incollection{ahmadi2012lifted_ws,
  	Author = {Babak Ahmadi and Kristian Kersting and Sriraam Natarajan},
  	Booktitle = {Working Notes of the ICML-2012 Workshop on Statistical Relational Learning(SRL)},
  	Keywords = {Statistical Relational Learning, Symmetries, Parameter Estimation, Markov Logic Networks},
  	Note = {Lifted inference approaches have rendered large, previously intractable probabilistic inference problems quickly solvable by employing symmetries to handle whole sets of indistinguishable random variables. Still, in many if not most situations training relational models will not benefit from lifting: symmetries within models easily break since variables become correlated by virtue of depending asymmetrically on evidence. An appealing idea for such situations is to train and recombine local models. This breaks longrange dependencies and allows to exploit lifting within and across the local training tasks. Moreover, it naturally paves the way for online training for relational models. Specifically, we develop the first lifted stochastic gradient optimization method with gain vector adaptation, processing each lifted piece one after the other.},
  	Title = {Lifted Parameter Learning in Relational Models},
  	Year = {2012}}

  @incollection{poole2012aggregation,
  	Author = {David Poole and David Buchman and Sriraam Natarajan and Kristian Kersting},
  	Booktitle = {Working Notes of the UAI Workshop on Statistical Relational AI (StarAI)},
  	Keywords = {Statistical Relational Learning, Population Size, Relational Logistic Regression},
  	Note = {This paper considers how relational probabilistic models adapt to population size. First we show that what are arbitrary choices for nonrelational domains become a commitment to how a relational model adapts to population change. We show how this manifests in a directed model where the conditional probabilities are represented using the logistic function, and show why it needs to be extended to a relational logistic function. Second we prove that directed aggregation models cannot be represented by Markov Logic without clauses that involve multiple individuals. Third we show how these models change as a function of population size.},
  	Title = {Aggregation and population growth: The relational logistic regression and Markov logic cases},
  	Year = {2012}}

  @book{kersting2012stairs,
  	Anote = {./images/kersting2012stairs.jpg},
  	Author = {Kristian Kersting and Marc Toussaint},
  	Keywords = {Proceedings},
  	Note = {The field of Artificial Intelligence is one in which novel ideas and new and original perspectives are of more than usual importance. The Starting AI Researchers Symposium (STAIRS) is an international meeting which supports AI researchers from all countries at the beginning of their career, PhD students and those who have held a PhD for less than one year. It offers doctoral students and young post-doctoral AI fellows a unique and valuable opportunity to gain experience in presenting their work in a supportive scientific environment, where they can obtain constructive feedback on the technical content of their work, as well as advice on how to present it, and where they can also establish contacts with the broader European AI research community. This book presents revised versions of peer-reviewed papers presented at the Sixth STAIRS, which took place in Montpellier, France, in conjunction with the 20th European Conference on Artificial Intelligence (ECAI) and the Seventh Conference on Prestigious Applications of Intelligent Systems (PAIS) in August 2012. The topics covered in the book range over a broad spectrum of subjects in the field of AI: machine learning and data mining, constraint satisfaction problems and belief propagation, logic and reasoning, dialogue and multiagent systems, and games and planning. Offering a fascinating opportunity to glimpse the current work of the AI researchers of the future, this book will be of interest to anyone whose work involves the use of artificial intelligence and intelligent systems.},
  	Publisher = {IOS Press},
  	Title = {Proceedings of the Sixth Starting AI Researchers' Symposium (STAIRS)},
  	Url = {http://www.iospress.nl/book/stairs-2012/},
  	Year = {2012},
  	Bdsk-Url-1 = {http://www.iospress.nl/book/stairs-2012/}}

  @article{natarajan2012gradient,
  	Anote = {./images/natarajan2012gradient.png},
  	Author = {Sriraam Natarajan and Tushar Khot and Kristian Kersting and Bernd Gutmann and Jude Shavlik},
  	Journal = {Machine Learning (MLJ)},
  	Keywords = {Statistical Relational Learning, Functional Gradient Boosting, Dependency Networks, Relational Domains},
  	Note = {Dependency networks approximate a joint probability distribution over multiple random variables as a product of conditional distributions. Relational Dependency Networks (RDNs) are graphical models that extend dependency networks to relational domains. This higher expressivity, however, comes at the expense of a more complex model-selection problem: an unbounded number of relational abstraction levels might need to be explored. Whereas current learning approaches for RDNs learn a single probability tree per random variable, we propose to turn the problem into a series of relational function-approximation problems using gradient-based boosting. In doing so, one can easily induce highly complex features over several iterations and in turn estimate quickly a very expressive model. Our experimental results in several different data sets show that this boosting method results in efficient learning of RDNs when compared to state-of-the-art statistical relational learning approaches.},
  	Number = {1},
  	Pages = {25--56},
  	Publisher = {Springer},
  	Title = {Gradient-based boosting for statistical relational learning: The relational dependency network case},
  	Url = {https://link.springer.com/content/pdf/10.1007%2Fs10994-011-5244-9.pdf},
  	Volume = {86},
  	Year = {2012},
  	Bdsk-Url-1 = {https://link.springer.com/content/pdf/10.1007%2Fs10994-011-5244-9.pdf}}

  @article{thurau2012descriptive,
  	Anote = {./images/thurau2012descriptive.png},
  	Author = {Christian Thurau and Kristian Kersting and Mirwaes Wahabzada and Christian Bauckhage},
  	Journal = {Data Mining and Knowledge Discovery (DAMI)},
  	Keywords = {Sustainability, Interpretable Matrix Factorization, Linear Time, Simplex Volume Maximization},
  	Note = {We discuss a new technique for factorizing data matrices. The basic idea is to represent a set of data by means of convex combinations of extreme data points. This often accommodates human cognition. In contrast to established factorization methods, the approach presented in this paper can also determine over-complete bases. At the same time, convex combinations allow for highly efficient matrix factorization. Based on techniques adopted from the field of distance geometry, we derive a linear time algorithm to determine suitable basis vectors for factorization. By means of the example of several environmental and developmental data sets we discuss the performance and characteristics of the proposed approach and validate that significant efficiency gains are obtainable without performance decreases compared to existing convexity constrained approaches.},
  	Number = {2},
  	Pages = {325--354},
  	Publisher = {Springer},
  	Title = {Descriptive matrix factorization for sustainability adopting the principle of opposites},
  	Url = {http://static.springer.com/sgw/documents/1387633/application/pdf/10-5.pdf},
  	Volume = {24},
  	Year = {2012},
  	Bdsk-Url-1 = {http://static.springer.com/sgw/documents/1387633/application/pdf/10-5.pdf}}

  @article{romer2012early,
  	Anote = {./images/fpb2012sivm.png},
  	Author = {Christoph R√∂mer and Mirwaes Wahabzada and Agim Ballvora and Francisco Pinto and Micol Rossini and Cinzia Panigada and Jan Behmann and Jens Leon and Christian Thurau and Christian Bauckhage and Kristian Kersting and Uwe Rascher and Lutz Pl√ºmer},
  	Journal = {Functional Plant Biology},
  	Keywords = {Sustainability, Plant Phenotyping, Simplex Volume maximization, Achetypal Analysis, Drougth Stress, Interpretable Matrix Factorization},
  	Note = {Early water stress recognition is of great relevance in precision plant breeding and production. Hyperspectral imaging sensors can be a valuable tool for early stress detection with high spatio-temporal resolution. They gather large, high dimensional data cubes posing a significant challenge to data analysis. Classical supervised learning algorithms often fail in applied plant sciences due to their need of labelled datasets, which are difficult to obtain. Therefore, new approaches for unsupervised learning of relevant patterns are needed. We apply for the first time a recent matrix factorisation technique, simplex volume maximisation (SiVM), to hyperspectral data. It is an unsupervised classification approach, optimised for fast computation of massive datasets. It allows calculation of how similar each spectrum is to observed typical spectra. This provides the means to express how likely it is that one plant is suffering from stress. The method was tested for drought stress, applied to potted barley plants in a controlled rain-out shelter experiment and to agricultural corn plots subjected to a two factorial field setup altering water and nutrient availability. Both experiments were conducted on the canopy level. SiVM was significantly better than using a combination of established vegetation indices. In the corn plots, SiVM clearly separated the different treatments, even though the effects on leaf and canopy traits were subtle.},
  	Number = {11},
  	Pages = {878--890},
  	Publisher = {CSIRO PUBLISHING},
  	Title = {Early drought stress detection in cereals: simplex volume maximisation for hyperspectral image analysis},
  	Url = {https://ai2-s2-pdfs.s3.amazonaws.com/2a48/cd6a560752cb72299ea3d1f76370b26a7b2c.pdf},
  	Volume = {39},
  	Year = {2012},
  	Bdsk-Url-1 = {https://ai2-s2-pdfs.s3.amazonaws.com/2a48/cd6a560752cb72299ea3d1f76370b26a7b2c.pdf}}

  @article{bauckhage2012agriculture,
  	Author = {Christian Bauckhage and Kristian Kersting and Albrecht Schmidt},
  	Journal = {IEEE Pervasive Computing},
  	Keywords = {Overview, Agriculture, Sustainability},
  	Note = {Agriculture, one of the oldest economic sectors of humankind, is getting a technological makeover. Pervasive computing puts the vision of a sustainable agriculture within reach for anyone, any time, anywhere.},
  	Number = {2},
  	Pages = {4--7},
  	Publisher = {IEEE Press},
  	Title = {Agriculture's Technological Makeover},
  	Url = {https://www.researchgate.net/profile/Christian_Bauckhage/publication/254058744_Agriculture%27s_Technological_Makeover/links/56001b2a08aec948c4fa6590/Agricultures-Technological-Makeover.pdf},
  	Volume = {11},
  	Year = {2012},
  	Bdsk-Url-1 = {https://www.researchgate.net/profile/Christian_Bauckhage/publication/254058744_Agriculture%27s_Technological_Makeover/links/56001b2a08aec948c4fa6590/Agricultures-Technological-Makeover.pdf}}

  @article{lang2012exploration,
  	Anote = {./images/jmlr2012exploration.png},
  	Author = {Tobias Lang and Marc Toussaint and Kristian Kersting},
  	Journal = {Journal of Machine Learning Research (JMLR)},
  	Keywords = {Optimal Actions, Exploration vs. Exploitation, Statistical Relational AI, Statstical Relational Learning, Relational Reinforcement Learning, Markov Decision processes},
  	Note = {A fundamental problem in reinforcement learning is balancing exploration and exploitation. We address this problem in the context of model-based reinforcement learning in large stochastic relational domains by developing relational extensions of the concepts of the E3 and R-MAX algorithms. Efficient exploration in exponentially large state spaces needs to exploit the generalization of the learned model: what in a propositional setting would be considered a novel situation and worth exploration may in the relational setting be a well-known context in which exploitation is promising. To address this we introduce relational count functions which generalize the classical notion of state and action visitation counts. We provide guarantees on the exploration efficiency of our framework using count functions under the assumption that we had a relational KWIK learner and a near-optimal planner. We propose a concrete exploration algorithm which integrates a practically efficient probabilistic rule learner and a relational planner (for which there are no guarantees, however) and employs the contexts of learned relational rules as features to model the novelty of states and actions. Our results in noisy 3D simulated robot manipulation problems and in domains of the international planning competition demonstrate that our approach is more effective than existing propositional and factored exploration techniques.},
  	Number = {Dec},
  	Pages = {3725--3768},
  	Title = {Exploration in relational domains for model-based reinforcement learning},
  	Url = {http://www.jmlr.org/papers/volume13/lang12a/lang12a.pdf},
  	Volume = {13},
  	Year = {2012},
  	Bdsk-Url-1 = {http://www.jmlr.org/papers/volume13/lang12a/lang12a.pdf}}

  @inproceedings{ahmadi2011multi,
  	Anote = {./images/ahmadi2011multi.png},
  	Author = {Babak Ahmadi and Kristian Kersting and Scott Sanner},
  	Booktitle = {Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI)},
  	Keywords = {Statistical Relational AI, Lifted Inference, Symmetries, Kalman Filter, Page Rank, Multiple Evidence},
  	Note = {Lifted message passing algorithms exploit repeated structure within a given graphical model to answer queries efficiently. Given evidence, they construct a lifted network of supernodes and superpotentials corresponding to sets of nodes and potentials that are indistinguishable given the evidence. Recently, efficient algorithms were presented for updating the structure of an existing lifted network with incremental changes to the evidence. In the inference stage, however, current algorithms need to construct a separate lifted network for each evidence case and run a modified message passing algorithm on each lifted network separately. Consequently, symmetries across the inference tasks are not exploited. In this paper, we present a novel lifted message passing technique that exploits symmetries across multiple evidence cases. The benefits of this multi-evidence lifted inference are shown for several important AI tasks such as computing personalized PageRanks and Kalman filters via multievidence lifted Gaussian belief propagation.},
  	Pages = {1152},
  	Title = {Multi-evidence lifted message passing, with application to pagerank and the kalman filter},
  	Url = {./papers/ahmadi2011multi.pdf},
  	Year = {2011},
  	Bdsk-Url-1 = {./papers/ahmadi2011multi.pdf}}

  @inproceedings{wahabzada2011more,
  	Anote = {./images/wahabzada2011more.png},
  	Author = {Mirwaes Wahabzada and Kristian Kersting and Anja Pilz and Christian Bauckhage},
  	Booktitle = {Proceedings of the 20th ACM international conference on Information and knowledge management (CIKM)},
  	Keywords = {Stochastic Gradient, Probabilsitic Topics Models, Latent Dirichlet Allocation, Active Learning, Mini-Batch Scheduling},
  	Note = {There have recently been considerable advances in fast inference for (online) latent Dirichlet allocation (LDA). While it is widely recognized that the scheduling of documents in stochastic optimization and in turn in LDA may have significant consequences, this issue remains largely unexplored. Instead, practitioners schedule documents essentially uniformly at random, due perhaps to ease of implementation, and to the lack of clear guidelines on scheduling the documents. In this work, we address this issue and propose to schedule documents for an update that exert a disproportionately large influence on the topics of the corpus before less influential ones. More precisely, we justify to sample documents randomly biased towards those ones with higher norms to form mini-batches. On several real-world datasets, including 3M articles from Wikipedia and 8M from PubMed, we demonstrate that the resulting influence scheduled LDA can handily analyze massive document collections and find topic models as good or better than those found with online LDA, often at a fraction of time.},
  	Organization = {ACM},
  	Pages = {2273--2276},
  	Title = {More influence means less work: fast latent Dirichlet allocation by influence scheduling},
  	Url = {./papers/wahabzada2011more.pdf},
  	Year = {2011},
  	Bdsk-Url-1 = {./papers/wahabzada2011more.pdf}}

  @inproceedings{wahabzada2011larger,
  	Anote = {./images/wahabzada2011larger.png},
  	Author = {Mirwaes Wahabzada and Kristian Kersting},
  	Booktitle = {Proceedings of the European Conference on Machine Learning and Principles of Knowledge Discovery in Databases (ECML PKDD)},
  	Keywords = {Probabilistic Topic Models, Latent Dirichlet Allocation, Active Learning, Stochastic Gradient, Informed Sampling},
  	Note = {Recently, there have been considerable advances in fast inference for latent Dirichlet allocation (LDA). In particular, stochastic optimization of the variational Bayes (VB) objective function with a natural gradient step was proved to converge and able to process massive document collections. To reduce noise in the gradient estimation, it considers multiple documents chosen uniformly at random. While it is widely recognized that the scheduling of documents in stochastic optimization may have significant consequences, this issue remains largely unexplored. In this work, we address this issue. Specifically, we propose residual LDA, a novel, easy-to-implement, LDA approach that schedules documents in an informed way. Intuitively, in each iteration, residual LDA actively selects documents that exert a disproportionately large influence on the current residual to compute the next update. On several real-world datasets, including 3M articles from Wikipedia, we demonstrate that residual LDA can handily analyze massive document collections and find topic models as good or better than those found with batch VB and randomly scheduled VB, and significantly faster.},
  	Organization = {Springer},
  	Pages = {475--490},
  	Title = {Larger residuals, less work: Active document scheduling for latent Dirichlet allocation},
  	Url = {./papers/wahabzada2011larger.pdf},
  	Year = {2011},
  	Bdsk-Url-1 = {./papers/wahabzada2011larger.pdf}}

  @inproceedings{jawad2011traffic,
  	Anote = {./images/jawad2011traffic.png},
  	Author = {Ahmed Jawad and Kristian Kersting and Natalia Andrienko},
  	Booktitle = {Proceedings of the 19th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems (GIS)},
  	Keywords = {Sequence Logo, Traffic Data},
  	Note = {Traffic and mobility mining are fascinating and fast growing areas of data mining and geographical information systems that impact the lives of billions of people every day. Another well-known scientific field that impacts lives of billions is biological sequence analysis. It has experienced an incredible evolution in the recent decade, especially since the Human Genome project. Although, a very first link between both fields has been established already in the early 90ies, many recent papers on mobility mining seem to be unaware of it. We therefore revisit the link and show that many unexplored and novel mobility mining methods fall naturally out of it. Specifically, using advanced discretization techniques for stay-point detection and map matching, we turn traffic sequences into a biological ones. Then, we introduce a novel distance function that enables us to directly apply the rich toolbox for biological sequence analysis to it. For instance, by just looking at co mplex traffic data through the biological glasses of sequence logos we get a novel, easy-to-grasp visualization of data, called Traffic Logos. For clustering and prediction tasks, our empirical evaluation on three real-world data sets demonstrates that revisiting the link can yield performance as good as state-of-the-art data mining techniques.},
  	Organization = {ACM},
  	Pages = {357--360},
  	Title = {Where traffic meets dna: mobility mining using biological sequence analysis revisited},
  	Url = {./papers/jawad2011traffic.pdf},
  	Year = {2011},
    Key = {Best Poster Award at GIS 2011},
  	Bdsk-Url-1 = {./papers/jawad2011traffic.pdf}}

  @inproceedings{neumann2011markov,
  	Anote = {./images/neumann2011markov.png},
  	Author = {Marion Neumann and Babak Ahmadi and Kristian Kersting},
  	Booktitle = {Proceedings of the 25th AAAI Conference on Artificial Intelligence (AAAI)},
  	Keywords = {Statistical Relatonal Leanring, Clustering on Demand, Information Retrieval},
  	Note = {Inspired by ``GoogleTM Sets'' and Bayesian sets, we consider the problem of retrieving complex objects and relations among them, i.e., ground atoms from a logical concept, given a query consisting of a few atoms from that concept. We formulate this as a within-network relational learning problem using few labels only and describe an algorithm that ranks atoms using a score based on random walks with restart (RWR): the probability that a random surfer hits an atom starting from the query atoms. Specifically, we compute an initial ranking using personalized PageRank. Then, we find paths of atoms that are connected via their arguments, variablize the ground atoms in each path, in order to create features for the query. These features are used to re-personalize the original RWR and to finally compute the set completion, based on Label Propagation. Moreover, we exploit that RWR techniques can naturally be lifted and show that lifted inference for label propagation is possible. We evaluate our algorithm on a realworld relational dataset by finding completions of sets of objects describing the Roman city of Pompeii. We compare to Bayesian sets and show that our approach gives very reasonable set completions.},
  	Title = {Markov Logic Sets: Towards Lifted Information Retrieval Using PageRank and Label Propagation},
  	Url = {./papers/neumann2011markov.pdf},
  	Year = {2011},
  	Bdsk-Url-1 = {./papers/neumann2011markov.pdf}}

  @inproceedings{hadiji2011efficient,
  	Anote = {./images/hadiji2011efficient.png},
  	Author = {Fabian Hadiji and Babak Ahmadi and Kristian Kersting},
  	Booktitle = {Annual German Conference on Artificial Intelligence (KI)},
  	Keywords = {Statistical Relational AI, Lifted Inference, High-order Marginals, Sequental Clamping, Sampling},
  	Note = {Lifted message passing approaches can be extremely fast at computing approximate marginal probability distributions over single variables and neighboring ones in the underlying graphical model. They do, however, not prescribe a way to solve more complex inference tasks such as computing joint marginals for k-tuples of distant random variables or satisfying assignments of CNFs. A popular solution in these cases is the idea of turning the complex inference task into a sequence of simpler ones by selecting and clamping variables one at a time and running lifted message passing again after each selection. This naive solution, however, recomputes the lifted network in each step from scratch, therefore often canceling the benefits of lifted inference. We show how to avoid this by efficiently computing the lifted network for each conditioning directly from the one already known for the single node marginals. Our experiments show that significant efficiency gains are possible for lifted message passing guided decimation for SAT and sampling.},
  	Organization = {Springer Berlin Heidelberg},
  	Pages = {122--133},
  	Title = {Efficient sequential clamping for lifted message passing},
  	Url = {./papers/hadiji2011efficient.pdf},
  	Year = {2011},
  	Bdsk-Url-1 = {./papers/hadiji2011efficient.pdf}}

  @inproceedings{natarajan2011imitation,
  	Anote = {./images/natarajan2011imitation.png},
  	Author = {Sriraam Natarajan and Saket Joshi and Prasad Tadepalli and Kristian Kersting and Jude Shavlik},
  	Booktitle = {Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI)},
  	Keywords = {Statistical Relational Learning, Functional Gradient Boosting, Imitation Learning, Relational Domains},
  	Note = {Imitation learning refers to the problem of learning how to behave by observing a teacher in action. We consider imitation learning in relational domains, in which there is a varying number of objects and relations among them. In prior work, simple relational policies are learned by viewing imitation learning as supervised learning of a function from states to actions. For propositional worlds, functional gradient methods have been proved to be beneficial. They are simpler to implement than most existing methods, more efficient, more naturally satisfy common constraints on the cost function, and better represent our prior beliefs about the form of the function. Building on recent generalizations of functional gradient boosting to relational representations, we implement a functional gradient boosting approach to imitation learning in relational domains. In particular, given a set of traces from the human teacher, our system learns a policy in the form of a set of relational regression trees that additively approximate the functional gradients. The use of multiple additive trees combined with relational representation allows for learning more expressive policies than what has been done before. We demonstrate the usefulness of our approach in several different domains.},
  	Number = {1},
  	Title = {Imitation learning in relational domains: A functional-gradient boosting approach},
  	Url = {./papers/natarajan2011imitation.pdf},
  	Year = {2011},
  	Bdsk-Url-1 = {./papers/natarajan2011imitation.pdf}}

  @inproceedings{xu2011multi,
  	Anote = {./images/xu2011multi.png},
  	Author = {Zhao Xu and Kristian Kersting},
  	Booktitle = {Proceedings of the 11th IEEE International Conference on Data Mining (ICDM)},
  	Keywords = {Statistical Relational Learning, Gaussian Processes, Multi-Task Learning},
  	Note = {Multi-task and relational learning with Gaussian processes are two active but also orthogonal areas of research. So far, there has been few attempt at exploring relational information within multi-task Gaussian processes. While existing relational Gaussian process methods have focused on relations among entities and in turn could be employed within an individual task, we develop a class of Gaussian process models which incorporates relational information across multiple tasks. As we will show, inference and learning within the resulting class of models, called relational multi-task Gaussian processes, can be realized via a variational EM algorithm. Experimental results on synthetic and real-world datasets verify the usefulness of this approach: The observed relational knowledge at the level of tasks can indeed reveal additional pair wise correlations between tasks of interest and, in turn, improve prediction performance.},
  	Organization = {IEEE},
  	Pages = {884--893},
  	Title = {Multi-task learning with task relations},
  	Url = {./papers/xu2011multi.pdf},
  	Year = {2011},
  	Bdsk-Url-1 = {./papers/xu2011multi.pdf}}

  @inproceedings{khot2011learning,
  	Anote = {./images/mlj2015boosting.png},
  	Author = {Tushar Khot and Sriraam Natarajan and Kristian Kersting and Jude Shavlik},
  	Booktitle = {Proceedings of the 11th IEEE International Conference on Data Mining (ICDM)},
  	Keywords = {Statistical Relational Learning, Funcational Gradient Boosting, Markov Logic Networks, Structure Learning},
  	Note = {Recent years have seen a surge of interest in Statistical Relational Learning (SRL) models that combine logic with probabilities. One prominent example is Markov Logic Networks (MLNs). While MLNs are indeed highly expressive, this expressiveness comes at a cost. Learning MLNs is a hard problem and therefore has attracted much interest in the SRL community. Current methods for learning MLNs follow a twostep approach: first, perform a search through the space of possible clauses and then learn appropriate weights for these clauses. We propose to take a different approach, namely to learn both the weights and the structure of the MLN simultaneously. Our approach is based on functional gradient boosting where the problem of learning MLNs is turned into a series of relational functional approximation problems. We use two kinds of representations for the gradients: clausebased and tree-based. Our experimental evaluation on several benchmark data sets demonstrates that our new approach can learn MLNs as good or better than those found with state-ofthe-art methods, but often in a fraction of the time.},
  	Organization = {IEEE},
  	Pages = {320--329},
  	Title = {Learning markov logic networks via functional gradient boosting},
  	Url = {./papers/khot2011learning.pdf},
  	Year = {2011},
  	Bdsk-Url-1 = {./papers/khot2011learning.pdf}}

  @incollection{sanner2011symbolic,
  	Author = {Scott Sanner and Kristian Kersting},
  	Booktitle = {Encyclopedia of machine learning},
  	Keywords = {Introduction, Symbolic Dynamic Programming, Optimal Actions, Relational Markov Decision Processes},
  	Pages = {946--954},
  	Publisher = {Springer US},
  	Title = {Symbolic dynamic programming},
  	Year = {2011}}

  @incollection{de2011statistical,
  	Author = {Luc De Raedt and Kristian Kersting},
  	Booktitle = {Encyclopedia of Machine Learning},
  	Keywords = {Intoduction, Statistical Relational Learning},
  	Pages = {916--924},
  	Publisher = {Springer US},
  	Title = {Statistical relational learning},
  	Year = {2011}}

  @incollection{ahmadi2011scientist,
  	Anote = {./images/ahmadi2011scientist.png},
  	Author = {Babak Ahmadi and Salah Zayakh and Fabian Hadiji and Kristian Kersting},
  	Booktitle = {Working Notes of the Lernen, Wissen, Adaptivit{\"a}t (LWA) Workshop},
  	Keywords = {DBLP, Affiliation Propagation, Geo-tags},
  	Note = {Today, electronic scholarly articles are available freely at the point of use. Moreover, bibliographic systems such as DBLP, ACM's Digital Libraries, Google's Scholar, and Microsoft's AcademicSearch provide means to search and analyze bibliographic information. However, one important information is typically incomplete, wrong, or even missing: the affiliation of authors. This type of information can be valuable not only for finding and tracking scientists using map interfaces but also for automatic detection of con- flict of interests and, in aggregate form, for helping to understand topics and trends in science at global scale. In this work-in-progress report, we consider the problem of retrieving affiliations from few observed affiliations only. Specifically, we crawl ACM's Digital Libraries for affiliations of authors listed in DBLP. Then, we employ multi-label propagation to propagate the few observed affiliations through out a network induced by a Markov logic network on DBLP entries. We use the propagated affiliations to create a visualization tool, PubMap, that can help expose the affiliations, using a map interface to display the propagated affiliations. Furthermore, we motivate how the information about affiliations can be used in publication summarization.},
  	Pages = {133--136},
  	Title = {O Scientist, Where Art Thou? Affiliation Propagation for Geo-Referencing Scientific Publications},
  	Url = {./papers/ahmadi2011scientist.pdf},
  	Year = {2011},
  	Bdsk-Url-1 = {./papers/ahmadi2011scientist.pdf}}

  @incollection{ahmadi2011lifted,
  	Author = {Babak Ahmadi and Martin Mladenov and Kristian Kersting and Scott Sanner},
  	Booktitle = {Working Notes of the Lernen, Wissen, Adaptivit{\"a}t (LWA) Workshop},
  	Keywords = {Statistical Relational AI, Lifted Inference, Page Rank, Kalman Filter},
  	Note = {Lifted message passing algorithms exploit repeated structure within a given graphical model to answer queries efficiently. Given evidence, they construct a lifted network of supernodes and superpotentials corresponding to sets of nodes and potentials that are indistinguishable given the evidence. Recently, efficient algorithms were presented for updating the structure of an existing lifted network with incremental changes to the evidence. In the inference stage, however, current algorithms need to construct a separate lifted network for each evidence case and run a modified message passing algorithm on each lifted network separately. Consequently, symmetries across the inference tasks are not exploited. In this paper, we present a novel lifted message passing technique that exploits symmetries across multiple evidence cases. The benefits of this multi-evidence lifted inference are shown for several important AI tasks such as solving linear programs, computing personalized PageRanks and Kalman filters via multi-evidence lifted Gaussian belief propagation.},
  	Pages = {35--42},
  	Title = {On Lifted PageRank, Kalman Filter and Towards Lifted Linear Program Solving},
  	Year = {2011}}

  @incollection{kersting2011statistical,
  	Author = {Kristian Kersting and Sriraam Natarajan and David Poole},
  	Booktitle = {Proceedings of the 11th International Conference on Logic Programming and Nonmonotonic Reasoning (LPNMR)},
  	Keywords = {Statistical Relaitonal AI, Overview},
  	Note = {One of the key challenges in building intelligent agents is closing the gap between logical and statistical AI, so that we can have rich representations including objects, relations and uncertainty, that we can effectively learn and carry out inference with. Over the last 25 years there has been a considerable body of research into combinations of predicate logic and probability forming what has become known as statistical relational artifi- cial intelligence (StaR-AI). We overview the foundations of the area, give some research problems, proposed solutions, outstanding issues, and clear up some misconceptions that have arisen. We discuss representations, semantics, inference, learning and applications, and provide references to the literature.},
  	Pages = {1--9},
  	Title = {Statistical relational AI: logic, probability and computation},
  	Url = {./papers/kersting2011statistical.pdf},
  	Year = {2011},
  	Bdsk-Url-1 = {./papers/kersting2011statistical.pdf}}

  @article{thurau2011convex,
  	Anote = {./images/thurau2011convex.png},
  	Author = {Christian Thurau and Kristian Kersting and Mirwaes Wahabzada and Christian Bauckhage},
  	Journal = {Knowledge and information systems (KAIS)},
  	Keywords = {Interpretable Matrix Factorization, Convex, Massive Data, Non-Negative},
  	Note = {Non-negative matrix factorization (NMF) has become a standard tool in data mining, information retrieval, and signal processing. It is used to factorize a non-negative data matrix into two non-negative matrix factors that contain basis elements and linear coefficients, respectively. Often, the columns of the first resulting factor are interpreted as ``cluster centroids'' of the input data, and the columns of the second factor are understood to contain cluster membership indicators. When analyzing data such as collections of gene expressions, documents, or images, it is often beneficial to ensure that the resulting cluster centroids are meaningful, for instance, by restricting them to be convex combinations of data points. However, known approaches to convex-NMF suffer from high computational costs and therefore hardly apply to large-scale data analysis problems. This paper presents a new framework for convex-NMF that allows for an efficient factorization of data matrices of millions of data points. Triggered by the simple observation that each data point can be expressed as a convex combination of vertices of the data convex hull, we require the basic factors to be vertices of the data convex hull. The benefits of convex-hull NMF are twofold. First, for a growing number of data points the expected size of the convex hull, i.e. the number of its vertices, grows much slower than the dataset. Second, distance preserving low-dimensional embeddings allow us to efficiently sample the convex hull and hence to quickly determine candidate vertices. Our extensive experimental evaluation on large datasets shows that convex-hull NMF compares favorably to convex-NMF in terms of both speed and reconstruction quality. We demonstrate that our method can easily be applied to large-scale, real-world datasets, in our case consisting of 750,000 DBLP entries, 4,000,000 digital images, and 150,000,000 votes on World of Warcraft {\textregistered}guilds, respectively.},
  	Number = {2},
  	Pages = {457--478},
  	Publisher = {Springer},
  	Title = {Convex non-negative matrix factorization for massive datasets},
  	Url = {./papers/thurau2011convex.pdf},
  	Volume = {29},
  	Year = {2011},
  	Bdsk-Url-1 = {./papers/thurau2011convex.pdf}}

  @article{joshi2011decision,
  	Anote = {./images/joshi2011decision.png},
  	Author = {Saket Joshi and Kristian Kersting and Roni Khardon},
  	Journal = {Artificial Intelligence (AIJ)},
  	Keywords = {Statistical Relational AI, Lifted Inference, First Oder Decision Diagrams, Optimal Actions, Relational Markov Decision Processes},
  	Note = {Many tasks in AI require representation and manipulation of complex functions. First order decision diagrams (FODD) are a compact knowledge representation expressing functions over relational structures. They represent numerical functions that, when constrained to the Boolean range, use only existential quantification. Previous work has developed a set of operations for composition and for removing redundancies in FODDs, thus keeping them compact, and showed how to successfully employ FODDs for solving large-scale stochastic planning problems through the formalism of relational Markov decision processes (RMDP). In this paper, we introduce several new ideas enhancing the applicability of FODDs. More specifically, we first introduce Generalized FODDs (GFODD) and composition operations for them, generalizing FODDs to arbitrary quantification. Second, we develop a novel approach for reducing (G)FODDs using model checking. This yields -- for the first time -- a reduction that maximally reduces the diagram for the FODD case and provides a sound reduction procedure for GFODDs. Finally we show how GFODDs can be used in principle to solve RMDPs with arbitrary quantification, and develop a complete solution for the case where the reward function is specified using an arbitrary number of existential quantifiers followed by an arbitrary number of universal quantifiers.},
  	Number = {18},
  	Pages = {2198--2222},
  	Publisher = {Elsevier},
  	Title = {Decision-theoretic planning with generalized first-order decision diagrams},
  	Url = {./papers/joshi2011decision.pdf},
  	Volume = {175},
  	Year = {2011},
  	Bdsk-Url-1 = {./papers/joshi2011decision.pdf}}

  @article{schmidt2011perception,
  	Author = {Albrecht Schmidt and Marc Langheinrich and Kristian Kersting},
  	Journal = {IEEE Computer},
  	Keywords = {Position Statement, Cyber-Physical Systems},
  	Note = {Emerging sensor-equipped computing devices are overcoming longstanding temporal and spatial boundaries to human perception.},
  	Number = {2},
  	Pages = {86--88},
  	Publisher = {IEEE Computer Society},
  	Title = {Perception beyond the Here and Now},
  	Ural = {./papers/schmidt2011perception.pdf},
  	Volume = {44},
  	Year = {2011}}

  @inproceedings{sanner2010symbolic,
  	Anote = {./images/sanner2010symbolic.png},
  	Author = {Scott Sanner and Kristian Kersting},
  	Booktitle = {Proceedings of the 24th AAAI Conference on Artificial Intelligence (AAAI)},
  	Keywords = {Statistical Relational AI, Symbolic Dynamic Programming, Relational Markov Decision Processes, Partially Observed},
  	Note = {Partially-observable Markov decision processes (POMDPs) provide a powerful model for sequential decision-making problems with partially-observed state and are known to have (approximately) optimal dynamic programming solutions. Much work in recent years has focused on improving the effi- ciency of these dynamic programming algorithms by exploiting symmetries and factored or relational representations. In this work, we show that it is also possible to exploit the full expressive power of first-order quantification to achieve state, action, and observation abstraction in a dynamic programming solution to relationally specified POMDPs. Among the advantages of this approach are the ability to maintain compact value function representations, abstract over the space of potentially optimal actions, and automatically derive compact conditional policy trees that minimally partition relational observation spaces according to distinctions that have an impact on policy values. This is the first lifted relational POMDP solution that can optimally accommodate actions with a potentially infinite relational space of observation outcomes.},
  	Pages = {1140--1146},
  	Title = {Symbolic Dynamic Programming for First-order POMDPs},
  	Url = {./papers/sanner2010symbolic.pdf},
  	Volume = {10},
  	Year = {2010},
  	Bdsk-Url-1 = {./papers/sanner2010symbolic.pdf}}

  @inproceedings{joshi2010self,
  	Anote = {./images/joshi2010self.png},
  	Author = {Saket Joshi and Kristian Kersting and Roni Khardon},
  	Booktitle = {Proceedings of the Twentieth International Conference on Automated Planning and Scheduling (ICAPS)},
  	Keywords = {Statistical Relational AI, Relational Markov Decison Processes, First Order Decision Diagrams, Sampling, Reduction},
  	Note = {We present a new paradigm for planning by learning, where the planner is given a model of the world and a small set of states of interest, but no indication of optimal actions in these states. The additional information can help focus the planner on regions of the state space that are of interest and lead to improved performance. We demonstrate this idea by introducing novel model-checking reduction operations for First Order Decision Diagrams (FODD), a representation that has been used to implement decision-theoretic planning with Relational Markov Decision Processes (RMDP). Intuitively, these reductions modify the construction of the value function by removing any complex specifications that are irrelevant to the set of training examples, thereby focusing on the region of interest. We show that such training examples can be constructed on the fly from a description of the planning problem thus we can bootstrap to get a self-taught planning system. Additionally, we provide a new heuristic to embed universal and conjunctive goals within the framework of RMDP planners, expanding the scope and applicability of such systems. We show that these ideas lead to significant improvements in performance in terms of both speed and coverage of the planner, yielding state of the art planning performance on problems from the International Planning Competition.},
  	Pages = {89--96},
  	Title = {Self-Taught Decision Theoretic Planning with First Order Decision Diagrams},
  	Url = {./papers/joshi2010self.pdf},
  	Year = {2010},
  	Bdsk-Url-1 = {./papers/joshi2010self.pdf}}

  @inproceedings{natarajan2010boosting,
  	Author = {Sriraam Natarajan and Tushar Khot and Kristian Kersting and Bernd Gutmann and Jude Shavlik},
  	Booktitle = {Proceedings of the 20th International Conference on Inductive Logic Programming (ILP)},
  	Keywords = {Statistical Relational Learning, Functional Gradient Boosting, Dependency Networks, Relational Domains},
  	Note = {Relational Dependency Networks (RDNs) are graphical models that extend dependency networks to relational domains where the joint probability distribution over the variables is approximated as a product of conditional distributions. The current learning algorithms for RDNs use pseudolikelihood techniques to learn probability trees for each variable in order to represent the conditional distribution. We propose the use of gradient tree boosting as applied by Dietterich et al. to approximate the gradient for each variable. The use of several regression trees, instead of just one, results in an expressive model. Our results in 3 different data sets show that this training method results in effi- cient learning of RDNs when compared to state-of-the-art approaches to Statistical Relational Learning.},
  	Pages = {1--8},
  	Title = {Boosting Relational Dependency Networks},
  	Url = {./papers/natarajan2010boosting.pdf},
  	Year = {2010},
  	Bdsk-Url-1 = {./papers/natarajan2010boosting.pdf}}

  @inproceedings{lang2010exploration,
  	Anote = {./images/lang2010exploration.png},
  	Author = {Tobias Lang and Marc Toussaint and Kristian Kersting},
  	Booktitle = {European Conference on Machine Learning and Knowledge Discovery in Databases (ECML PKDD)},
  	Keywords = {Statistical Relational AI, Exploration vs. Exploitation, Optimal Actions, Relational Domains},
  	Note = {One of the key problems in model-based reinforcement learning is balancing exploration and exploitation. Another is learning and acting in large relational domains, in which there is a varying number of objects and relations between them. We provide one of the first solutions to exploring large relational Markov decision processes by developing relational extensions of the concepts of the Explicit Explore or Exploit (E3) algorithm. A key insight is that the inherent generalization of learnt knowledge in the relational representation has profound implications also on the exploration strategy: what in a propositional setting would be considered a novel situation and worth exploration may in the relational setting be an instance of a well-known context in which exploitation is promising. Our experimental evaluation shows the effectiveness and benefit of relational exploration over several propositional benchmark approaches on noisy 3D simulated robot manipulation problems.},
  	Organization = {Springer},
  	Pages = {178--194},
  	Title = {Exploration in relational worlds},
  	Url = {./papers/lang2010exploration.pdf},
  	Year = {2010},
  	Bdsk-Url-1 = {./papers/lang2010exploration.pdf}}

  @inproceedings{quadrianto2010beyond,
  	Anote = {./images/quadrianto2010beyond.png},
  	Author = {Novi Quadrianto and Kristian Kersting and Tinne Tuytelaars and Wray L Buntine},
  	Booktitle = {Proceedings of the international conference on Multimedia Information Retrieval (MIR)},
  	Keywords = {Image Browsing, Kernelized Sorting},
  	Note = {Ideally, one would like to perform image search using an intuitive and friendly approach. Many existing image search engines, however, present users with sets of images arranged in some default order on the screen, typically the relevance to a query, only. While this certainly has its advantages, arguably, a more flexible and intuitive way would be to sort images into arbitrary structures such as grids, hierarchies, or spheres so that images that are visually or semantically alike are placed together. This paper focuses on designing such a navigation system for image browsers. This is a challenging task because arbitrary layout structure makes it difficult -- if not impossible -- to compute cross-similarities between images and structure coordinates, the main ingredient of traditional layouting approaches. For this reason, we resort to a recently developed machine learning technique: kernelized sorting. It is a general technique for matching pairs of objects from different domains without requiring cross-domain similarity measures and hence elegantly allows sorting images into arbitrary structures. Moreover, we extend it so that some images can be preselected for instance forming the tip of the hierarchy allowing to subsequently navigate through the search results in the lower levels in an intuitive way.},
  	Organization = {ACM},
  	Pages = {339--348},
  	Title = {Beyond 2D-grids: a dependence maximization view on image browsing},
  	Url = {./papers/quadrianto2010beyond.pdf},
  	Year = {2010},
  	Bdsk-Url-1 = {./papers/quadrianto2010beyond.pdf}}

  @inproceedings{xu2010fast,
  	Anote = {./images/xu2010fast.png},
  	Author = {Zhao Xu and Kristian Kersting and Thorsten Joachims},
  	Booktitle = {Proceedings of the European Conference on Machine Learning and Principles of Knowledge Discovery in Databases (ECML PKDD)},
  	Keywords = {Gaussian Processes, Statistical Relational Learning, Preference learning, Active Learning},
  	Note = {In preference learning, the algorithm observes pairwise relative judgments (preference) between items as training data for learning an ordering of all items. This is an important learning problem for applications where absolute feedback is difficult to elicit, but pairwise judgments are readily available (e.g., via implicit feedback [13]). While it was already shown that active learning can effectively reduce the number of training pairs needed, the most successful existing algorithms cannot generalize over items or queries. Considering web search as an example, they would need to learn a separate relevance score for each document-query pair from scratch. To overcome this inefficiency, we propose a link-based active preference learning method based on Gaussian Processes (GPs) that incorporates dependency information from both feature-vector representations as well as relations. Specifically, to meet the requirement on computational efficiency of active exploration, we introduce a novel incremental update method that scales as well as the non-generalizing models. The proposed algorithm is evaluated on datasets for information retrieval, showing that it learns substantially faster than algorithms that cannot model dependencies.},
  	Pages = {499--514},
  	Publisher = {Springer},
  	Title = {Fast active exploration for link-based preference learning using gaussian processes},
  	Url = {./papers/xu2010fast.pdf},
  	Year = {2010},
  	Bdsk-Url-1 = {./papers/xu2010fast.pdf}}

  @inproceedings{ahmadi2010lifted,
  	Anote = {./images/ahmadi2010lifted.png},
  	Author = {Babak Ahmadi and Kristian Kersting and Fabian Hadiji},
  	Booktitle = {Proceedings of the European Workshop on Probabilistic Graphical Models (PGM)},
  	Keywords = {Statistical Relational AI, Lifted Inference, Symmetries, High-order Marginals, Loopy Belief Propagation},
  	Note = {Lifted belief propagation (LBP) can be extremely fast at computing approximate marginal probability distributions over single variables and neighboring ones in the underlying graphical model. It does, however, not prescribe a way to compute joint distributions over pairs, triples or k-tuples of distant random variables. In this paper, we present an algorithm, called conditioned LBP, for approximating these distributions. Essentially, we select variables one at a time for conditioning, running lifted belief propagation after each selection. This naive solution, however, recomputes the lifted network in each step from scratch, therefore often canceling the benefits of lifted inference. We show how to avoid this by efficiently computing the lifted network for each conditioning directly from the one already known for the single node marginals. Our experimental results validate that significant efficiency gains are possible and illustrate the potential for second-order parameter estimation of Markov logic networks.},
  	Pages = {9},
  	Title = {Lifted belief propagation: Pairwise marginals and beyond},
  	Url = {./papers/ahmadi2010lifted.pdf},
  	Year = {2010},
  	Bdsk-Url-1 = {./papers/ahmadi2010lifted.pdf}}

  @inproceedings{antanas2010combining,
  	Anote = {./images/antanas2010combining.png},
  	Author = {Laura Antanas and Bernd Gutmann and Ingo Thon and Kristian Kersting and Luc De Raedt},
  	Booktitle = {Proceedings of the Annual Belgian-Dutch Conference on Machine Learning (BeneLearn)},
  	Keywords = {Statistical Relational Learning, Video, Card Games},
  	Note = {The key to make computer games more compelling and interesting is to create intelligent artificial game agents. A first step is teaching them the protocols to play a game. To the best of our knowledge, most systems which train AI agents are used in virtual environments. In this work we train a computer system in a real world environment by video streams. First, we demonstrate a way to bridge the gap between low-level video data and high-level symbolic data. Second, using the high-level, yet noisy data, we show that state-of-the-art statistical relational learning systems are able to capture underlying concepts in video streams. We evaluate the selected methods on the task of detecting fraudulent behavior in card games},
  	Pages = {1--6},
  	Title = {Combining video and sequential statistical relational techniques to monitor card games},
  	Url = {./papers/antanas2010combining.pdf},
  	Year = {2010},
  	Bdsk-Url-1 = {./papers/antanas2010combining.pdf}}

  @inproceedings{natarajan2010exploiting,
  	Anote = {./images/natarajan2010exploiting.png},
  	Author = {Sriraam Natarajan and Tushar Khot and Daniel Lowd and Prasad Tadepalli and Kristian Kersting and Jude Shavlik},
  	Booktitle = {Proceedings of the European Conference on Machine Learning and Principles of Knowledge Discovery in Databases (ECML PKDD)},
  	Keywords = {Combining Rules, Causal Independencies, Statistical Relational Learning, Markov Logic Networs},
  	Note = {A new method is proposed for compiling causal independencies into Markov logic networks (MLNs). An MLN can be viewed as compactly representing a factorization of a joint probability into the product of a set of factors guided by logical formulas. We present a notion of causal independence that enables one to further factorize the factors into a combination of even smaller factors and consequently obtain a finer-grain factorization of the joint probability. The causal independence lets us specify the factor in terms of weighted, directed clauses and operators, such as ``or'', ``sum'' or ``max'', on the contribution of the variables involved in the factors, hence combining both undirected and directed knowledge. Our experimental evaluations shows that making use of the finer-grain factorization provided by causal independence can improve quality of parameter learning in MLNs.},
  	Pages = {434--450},
  	Publisher = {Springer},
  	Title = {Exploiting causal independence in Markov logic networks: Combining undirected and directed models},
  	Url = {./papers/natarajan2010exploiting.pdf},
  	Year = {2010},
  	Bdsk-Url-1 = {./papers/natarajan2010exploiting.pdf}}

  @inproceedings{behley2010learning,
  	Anote = {./images/behley2010learning.png},
  	Author = {Jens Behley and Kristian Kersting and Dirk Schulz and Volker Steinhage and Armin B Cremers},
  	Booktitle = {Proceedings of the IEEE International Conference on Intelligent Robots and Systems (IROS)},
  	Keywords = {Robotics, Logistic Regression, 3D, Laser Range,Point Classification, Hashing, Ultra-Fast},
  	Note = {Segmenting range data into semantic categories has become a more and more active field of research in robotics. In this paper, we advocate to view this task as a problem of fast, large-scale retrieval. Intuitively, given a dataset of millions of labeled scan points and their neighborhoods, we simply search for similar points in the datasets and use the labels of the retrieved ones to predict the labels of a novel point using some local prediction model such as majority vote or logistic regression. However, actually carrying this out requires highly efficient ways of (1) storing millions of scan points in memory and (2) quickly finding similar scan points to a target scan point. In this paper, we propose to address both issues by employing Weiss et al.'s recent spectral hashing. It represents each item in a database by a compact binary code that is constructed so that similar items will have similar binary code words. In turn, similar neighbors have codes within a small Hamming distance of the code for the query. Then, we learn a logistic regression model locally over all points with the same binary code word. Our experiments on real world 3D scans show that the resulting approach, called spectrally hashed logistic regression, can be ultra fast at prediction time and outperforms state-of-the art approaches such as logistic regression and nearest neighbor.},
  	Organization = {IEEE},
  	Pages = {5960--5965},
  	Title = {Learning to hash logistic regression for fast 3D scan point classification},
  	Url = {./papers/behley2010learning.pdf},
  	Year = {2010},
  	Bdsk-Url-1 = {./papers/behley2010learning.pdf}}

  @inproceedings{kersting2010hierarchical,
  	Anote = {./images/kersting2010hierarchical.png},
  	Author = {Kristian Kersting and Mirwaes Wahabzada and Christian Thurau and Christian Bauckhage},
  	Booktitle = {Proceedings of the Asian Conference on Machine Learning (ACML)},
  	Keywords = {Matrix Factorization, Interpretable, Convex, Tree Decomposition, Massive Scale},
  	Note = {We present an extension of convex-hull non-negative matrix factorization (CH-NMF) which was recently proposed as a large scale variant of convex non-negative matrix factorization or Archetypal Analysis. CH-NMF factorizes a non-negative data matrix V into two nonnegative matrix factors V ‚âà W H such that the columns of W are convex combinations of certain data points so that they are readily interpretable to data analysts. There is, however, no free lunch: imposing convexity constraints on W typically prevents adaptation to intrinsic, low dimensional structures in the data. Alas, in cases where the data is distributed in a non-convex manner or consists of mixtures of lower dimensional convex distributions, the cluster representatives obtained from CH-NMF will be less meaningful. In this paper, we present a hierarchical CH-NMF that automatically adapts to internal structures of a dataset, hence it yields meaningful and interpretable clusters for non-convex datasets. This is also confirmed by our extensive evaluation on DBLP publication records of 760,000 authors, 4,000,000 images harvested from the web, and 150,000,000 votes on World of Warcraft guilds.},
  	Pages = {253--268},
  	Title = {Hierarchical Convex NMF for Clustering Massive Data},
  	Url = {http://proceedings.mlr.press/v13/kersting10a/kersting10a.pdf},
  	Year = {2010},
  	Bdsk-Url-1 = {http://proceedings.mlr.press/v13/kersting10a/kersting10a.pdf}}

  @inproceedings{jawad2010kernelized,
  	Anote = {./images/jawad2010kernelized.png},
  	Author = {Ahmed Jawad and Kristian Kersting},
  	Booktitle = {Proceedings of the 18th SIGSPATIAL International Conference on Advances in Geographic Information Systems (GIS)},
  	Keywords = {Map Matching, Kernels},
  	Note = {Map matching is a fundamental operation in many applications such as traffic analysis and location-aware services, the killer apps for ubiquitous computing. In past, several map matching approaches have been proposed. Roughly, they can be categorized into four groups: geometric, topological, probabilistic, and other advanced techniques. Surprisingly, kernel methods have not received attention yet although they are very popular in the machine learning community due to their solid mathematical foundation, tendency toward easy geometric interpretation, and strong empirical performance in a wide variety of domains. In this paper, we show how to employ kernels for map matching. Specifically, ignoring map constraints, we first maximize the consistency between the similarity measures captured by the kernel matrices of the trajectory and relevant part of the street map. The resulting relaxed assignment is then ''rounded'' into a hard assignment fulfilling the map constraints. On synthetic and real-world trajectories, we show that kernels methods can be used for map matching and perform well compared to probabilistic methods such as HMMs.},
  	Organization = {ACM},
  	Pages = {454--457},
  	Title = {Kernelized map matching},
  	Url = {./papers/jawad2010kernelized.pdf},
  	Year = {2010},
  	Bdsk-Url-1 = {./papers/jawad2010kernelized.pdf}}

  @inproceedings{kersting2010informed,
  	Anote = {./images/kersting2010informed.png},
  	Author = {Kristian Kersting and Youssef El Massaoudi and Fabian Hadiji and Babak Ahmadi},
  	Booktitle = {Proceedings of the 24th AAAI Conference on Artificial Intelligence (AAAI)},
  	Keywords = {Statistical Relational AI, Lifted Inferences, Symmetries, Loopy Belief Propagation, On-the-fly},
  	Note = {Lifted inference, handling whole sets of indistinguishable objects together, is critical to the effective application of probabilistic relational models to realistic real world tasks. Recently, lifted belief propagation (LBP) has been proposed as an efficient approximate solution of this inference problem. It runs a modified BP on a lifted network where nodes have been grouped together if they have --- roughly speaking --- identical computation trees, the tree-structured unrolling of the underlying graph rooted at the nodes. In many situations, this purely syntactic criterion is too pessimistic: message errors decay along paths. Intuitively, for a long chain graph with weak edge potentials, distant nodes will send and receive identical messages yet their computation trees are quite different. To overcome this, we propose iLBP, a novel, easy-to-implement, informed LBP approach that interleaves lifting and modified BP iterations. In turn, we can efficiently monitor the true BP messages sent and received in each iteration and group nodes accordingly. As our experiments show, iLBP can yield significantly faster more lifted network while not degrading performance. Above all, we show that iLBP is faster than BP when solving the problem of distributing data to a large network, an important real-world application where BP is faster than uninformed LBP.},
  	Title = {Informed Lifting for Message-Passing},
  	Url = {https://www.researchgate.net/profile/Kristian_Kersting/publication/221604336_Informed_Lifting_for_Message-Passing/links/0912f512b40316d189000000/Informed-Lifting-for-Message-Passing.pdf},
  	Year = {2010},
  	Bdsk-Url-1 = {https://www.researchgate.net/profile/Kristian_Kersting/publication/221604336_Informed_Lifting_for_Message-Passing/links/0912f512b40316d189000000/Informed-Lifting-for-Message-Passing.pdf}}

  @inproceedings{natarajan2010multi,
  	Anote = {./images/natarajan2010multi.png},
  	Author = {Sriraam Natarajan and Gautam Kunapuli and Kshitij Judah and Prasad Tadepalli and Kristian Kersting and Jude Shavlik},
  	Booktitle = {Proceedings of the 9th International Conference on Machine Learning and Applications (ICMLA)},
  	Keywords = {Inverse Reinforcement Learning, Multi-Agents},
  	Note = {Learning the reward function of an agent by observing its behavior is termed inverse reinforcement learning and has applications in learning from demonstration or apprenticeship learning. We introduce the problem of multiagent inverse reinforcement learning, where reward functions of multiple agents are learned by observing their uncoordinated behavior. A centralized controller then learns to coordinate their behavior by optimizing a weighted sum of reward functions of all the agents. We evaluate our approach on a traffic-routing domain, in which a controller coordinates actions of multiple traffic signals to regulate traffic density. We show that the learner is not only able to match but even significantly outperform the expert.},
  	Organization = {IEEE},
  	Pages = {395--400},
  	Title = {Multi-agent inverse reinforcement learning},
  	Url = {./papers/natarajan2010multi.pdf},
  	Year = {2010},
  	Bdsk-Url-1 = {./papers/natarajan2010multi.pdf}}

  @inproceedings{ganzert2010identifying,
  	Anote = {./images/ganzert2010identifying.png},
  	Author = {Steven Ganzert and Knut M√∂ller and Stefan Kramer and Kristian Kersting and Josef Guttmann},
  	Booktitle = {Proceedings of the World Congress on Medical Physics and Biomedical Engineering (IFMBE)},
  	Keywords = {Process Model Induction, Differential Equation, Medicine, Lung},
  	Note = {Mechanical ventilation is the live-saving therapy in intensive care medicine by all means. Nevertheless, it can induce severe mechanical stress to the lung, which generally impairs the outcome of the therapy. To reduce the risk of a ventilator induced lung injury (VILI), lung protective ventilation is essential, especially for patients with a previous medical history like the adult respiratory distress syndrome (ARDS). The prerequisite for lung protective ventilation approaches is the knowledge about the physical behavior of the human lung under the condition of mechanical ventilation. This knowledge is commonly described by mathematical models. Diverse models have been introduced to represent particular aspects of mechanical characteristics of the lung. A commonly accepted general model is the equation of motion, which relates the airway pressure to the airflow and the volume applied by the ventilator and describes the influence of the distensibility and resistance of the respiratory system. Equation Discovery systems extract mathematical models from observed time series data. To reduce the vast search space associated with this task, the LAGRAMGE-system introduced the application of declarative bias in Equation Discovery, which furthermore allows the presentation of domain specific knowledge. We introduce a modification of this system and apply it to data obtained during mechanical ventilation of ARDS-patients. We experimentally validate the effectiveness of our approach and show that the equation of motion model can automatically be rediscovered from real-world data.},
  	Organization = {Springer},
  	Pages = {1524--1527},
  	Title = {Identifying mathematical models of the mechanically ventilated lung using equation discovery},
  	Url = {https://link.springer.com/chapter/10.1007/978-3-642-03882-2_404},
  	Year = {2010},
  	Bdsk-Url-1 = {https://link.springer.com/chapter/10.1007/978-3-642-03882-2_404}}

  @inproceedings{wahabzada2010topic,
  	Anote = {./images/wahabzada2010topic.png},
  	Author = {Mirwaes Wahabzada and Zhao Xu and Kristian Kersting},
  	Booktitle = {Proceedings of the European Conference on Machine Learning and Principles of Knowledge Discovery in Databases (ECML PKDD)},
  	Keywords = {Probabilistic Topics Model, Latent Dirichlet Allocation, Relational Domain, Statistical Relational learning},
  	Note = {Latent Dirichlet allocation is a fully generative statistical language model that has been proven to be successful in capturing both the content and the topics of a corpus of documents. Recently, it was even shown that relations among documents such as hyper-links or citations allow one to share information between documents and in turn to improve topic generation. Although fully generative, in many situations we are actually not interested in predicting relations among documents. In this paper, we therefore present a Dirichlet-multinomial nonparametric regression topic model that includes a Gaussian process prior on joint document and topic distributions that is a function of document relations. On networks of scientific abstracts and of Wikipedia documents we show that this approach meets or exceeds the performance of several baseline topic models.},
  	Organization = {Springer},
  	Pages = {402--417},
  	Title = {Topic models conditioned on relations},
  	Url = {./papers/wahabzada2010topic.pdf},
  	Year = {2010},
  	Bdsk-Url-1 = {./papers/wahabzada2010topic.pdf}}

  @inproceedings{thurau2010yes,
  	Anote = {./images/thurau2010yes.png},
  	Author = {Christian Thurau and Kristian Kersting and Christian Bauckhage},
  	Booktitle = {Proceedings of the 19th ACM international conference on Information and knowledge management (CIKM)},
  	Keywords = {Interpretable Matric Factorization, Simplex Volume Maximization, Web-Scale},
  	Note = {Matrix factorization methods are among the most common techniques for detecting latent components in data. Popular examples include the Singular Value Decomposition or Non-negative Matrix Factorization. Unfortunately, most methods suffer from high computational complexity and therefore do not scale to massive data. In this paper, we present a linear time algorithm for the factorization of gigantic matrices that iteratively yields latent components. We consider a constrained matrix factorization s.t.~the latent components form a simplex that encloses most of the remaining data. The algorithm maximizes the volume of that simplex and thereby reduces the displacement of data from the space spanned by the latent components. Hence, it also lowers the Frobenius norm, a common criterion for matrix factorization quality. Our algorithm is efficient, well-grounded in distance geometry, and easily applicable to matrices with billions of entries. In addition, the resulting factors allow for an intuitive interpretation of data: every data point can now be expressed as a convex combination of the most extreme and thereby often most descriptive instances in a collection of data. Extensive experimental validations on web-scale data, including 80 million images and 1.5 million twitter tweets, demonstrate superior performance compared to related factorization or clustering techniques.},
  	Organization = {ACM},
  	Pages = {1785--1788},
  	Title = {Yes we can: simplex volume maximization for descriptive web-scale matrix factorization},
  	Url = {./papers/thurau2010yes.pdf},
  	Year = {2010},
  	Bdsk-Url-1 = {./papers/thurau2010yes.pdf}}

  @incollection{xu2010social,
  	Anote = {./images/xu2010social.png},
  	Author = {Zhao Xu and Volker Tresp and Achim Rettinger and Kristian Kersting},
  	Booktitle = {Advances in social network mining and analysis},
  	Keywords = {Statistical Relational Learning, Non-Parametric, Social Networks, Infinite Hidden Relational Models},
  	Note = {Statistical relational learning (SRL) provides effective techniques to analyze social network data with rich collections of objects and complex networks. Infinite hidden relational models (IHRMs) introduce nonparametric mixture models into relational learning and have been successful in many relational applications. In this paper we explore the modeling and analysis of complex social networks with IHRMs for community detection, link prediction and product recommendation. In an IHRM-based social network model, each edge is associated with a random variable and the probabilistic dependencies between these random variables are specified by the model, based on the relational structure. The hidden variables, one for each object, are able to transport information such that non-local probabilistic dependencies can be obtained. The model can be used to predict entity attributes, to predict relationships between entities and it performs an interpretable cluster analysis. We demonstrate the performance of IHRMs with three social network applications. We perform community analysis on the Sampson's monastery data and perform link analysis on the Bernard & Killworth data. Finally we apply IHRMs to the MovieLens data for prediction of user preference on movies and for an analysis of user clusters and movie clusters.},
  	Pages = {77--96},
  	Publisher = {Springer Berlin Heidelberg},
  	Title = {Social network mining with nonparametric relational models},
  	Url = {./papers/xu2010social.pdf},
  	Year = {2010},
  	Bdsk-Url-1 = {./papers/xu2010social.pdf}}

  @incollection{hadiji2010lifted,
  	Author = {Fabian Hadiji and Kristian Kersting and Babak Ahmadi},
  	Booktitle = {Working Notes of the International Workshop on Statistical Relational Artificial Intelligence (StarAI)},
  	Keywords = {Statistical Relational AI, Lifted Inference, SAT, Symmetries},
  	Title = {Lifted Message Passing for Satisfiability},
  	Year = {2010}}

  @incollection{jawad2010kernelized,
  	Anote = {./images/kdml2010ahmed.png},
  	Author = {Ahmed Jawad and Kristian Kersting},
  	Booktitle = {Working Notes of the Lernen, Wissen, Adaptivit{\"a}t (LWA) Workshop},
  	Keywords = {Kernels, Map Matching},
  	Note = {Map matching is a fundamental operation in many applications such as traffic analysis and location-aware services, the killer apps for ubiquitous computing. In this paper, we show how to employ kernels for map matching. Specifically, ignoring map constraints, we first maximize the consistency between the similarity measures captured by the kernel matrices of the trajectory and relevant part of the street map.},
  	Pages = {89--96},
  	Title = {Kernelized Map Matching for noisy trajectories},
  	Url = {./papers/kdml10jawad.pdf},
  	Year = {2010},
  	Bdsk-Url-1 = {./papers/kdml10jawad.pdf}}

  @incollection{ahmadi2010lifted,
  	Author = {Babak Ahmadi and Kristian Kersting and Fabian Hadiji},
  	Booktitle = {Working Notes of the Lernen, Wissen, Adaptivit{\"a}t (LWA) Workshop},
  	Keywords = {Statistical Relational AI, Lifted Inference, High-order Marginal, Belief Propagation},
  	Pages = {13--18},
  	Title = {Lifted Conditioning for Pairwise Marginals and Beyond},
  	Year = {2010}}

  @incollection{kersting2010convex,
  	Author = {Kristian Kersting and Mirwaes Wahabzada and Christian Thurau and Christian Bauckhage},
  	Booktitle = {Working Notes of the Lernen, Wissen, Adaptivit{\"a}t (LWA) Workshop},
  	Keywords = {Convex Matrix Factorization, Interpretable, Massive Data},
  	Pages = {97--104},
  	Title = {Convex NMF on Non-Convex Massiv Data},
  	Year = {2010}}

  @incollection{quadrianto2010gaussian,
  	Author = {Novi Quadrianto and Kristian Kersting and Zhao Xu},
  	Booktitle = {Encyclopedia of Machine Learning},
  	Keywords = {Intoduction, Gaussian Processes, Short},
  	Pages = {428--439},
  	Publisher = {Springer},
  	Title = {Gaussian process},
  	Year = {2010}}

  @inbook{dereadt2010probabilistic,
  	Anote = {./images/dereadt2010probabilistic.png},
  	Author = {Luc {De Raedt} and Angelika Kimmig and Bernd Gutmann and Kristian Kersting and V{\'\i}tor Santos Costa and Hannu Toivonen},
  	Booktitle = {Inductive Databases and Constraint-Based Data Mining},
  	Keywords = {Statistical Relational Learning, Problog, Inference},
  	Note = {We study how probabilistic reasoning and inductive querying can be combined within ProbLog, a recent probabilistic extension of Prolog. ProbLog can be regarded as a database system that supports both probabilistic and inductive reasoning through a variety of querying mechanisms. After a short introduction to ProbLog, we provide a survey of the different types of inductive queries that ProbLog supports, and show how it can be applied to the mining of large biological networks.},
  	Pages = {229--262},
  	Publisher = {Springer},
  	Title = {Probabilistic inductive querying using ProbLog},
  	Url = {./papers/dereadt2010probabilistic.pdf},
  	Year = {2010},
  	Bdsk-Url-1 = {./papers/dereadt2010probabilistic.pdf}}

  @inproceedings{kersting2009counting,
  	Anote = {./images/kersting2009counting.png},
  	Author = {Kristian Kersting and Babak Ahmadi and Sriraam Natarajan},
  	Booktitle = {Proceedings of the 25th Conference on Uncertainty in Artificial Intelligence (UAI)},
  	Keywords = {Statistical Relational Learning, Lifted Inference, Symmetries, Color Passing, Loopy Belief Propagation},
  	Note = {A major benefit of graphical models is that most knowledge is captured in the model structure. Many models, however, produce inference problems with a lot of symmetries not reflected in the graphical structure and hence not exploitable by efficient inference techniques such as belief propagation (BP). In this paper, we present a new and simple BP algorithm, called counting BP, that exploits such additional symmetries. Starting from a given factor graph, counting BP first constructs a compressed factor graph of clusternodes and clusterfactors, corresponding to sets of nodes and factors that are indistinguishable given the evidence. Then it runs a modified BP algorithm on the compressed graph that is equivalent to running BP on the original factor graph. Our experiments show that counting BP is applicable to a variety of important AI tasks such as (dynamic) relational models and boolean model counting, and that significant efficiency gains are obtainable, often by orders of magnitude.},
  	Organization = {AUAI Press},
  	Pages = {277--284},
  	Title = {Counting belief propagation},
  	Url = {./papers/kersting2009counting.pdf},
  	Year = {2009},
  	Bdsk-Url-1 = {./papers/kersting2009counting.pdf}}

  @inproceedings{xu2009multi,
  	Anote = {./images/xu2009multi.png},
  	Author = {Zhao Xu and Kristian Kersting and Volker Tresp},
  	Booktitle = {Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI)},
  	Keywords = {Statistical Relational Learning, Gaussian Processes},
  	Note = {Due to their flexible nonparametric nature, Gaussian process models are very effective at solving hard machine learning problems. While existing Gaussian process models focus on modeling one single relation, we present a generalized GP model, named multi-relational Gaussian process model, that is able to deal with an arbitrary number of relations in a domain of interest. The proposed model is analyzed in the context of bipartite, directed, and undirected univariate relations. Experimental results on real-world datasets show that exploiting the correlations among different entity types and relations can indeed improve prediction performance.},
  	Pages = {1309--1314},
  	Title = {Multi-Relational Learning with Gaussian Processes.},
  	Url = {./paper/xu2009multi.pdf},
  	Volume = {9},
  	Year = {2009},
  	Bdsk-Url-1 = {./paper/xu2009multi.pdf}}

  @inproceedings{joshi2009generalized,
  	Anote = {./images/joshi2009generalized.png},
  	Author = {Saket Joshi and Kristian Kersting and Roni Khardon},
  	Booktitle = {Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI)},
  	Keywords = {Statistical Relational AI, Relational Markov Decision Processes, First Order Decision Diagrams, Optimal Actions},
  	Note = {First order decision diagrams (FODD) were recently introduced as a compact knowledge representation expressing functions over relational structures. FODDs represent numerical functions that, when constrained to the Boolean range, use only existential quantification. Previous work developed a set of operations over FODDs, showed how they can be used to solve relational Markov decision processes (RMDP) using dynamic programming algorithms, and demonstrated their success in solving stochastic planning problems from the International Planning Competition in the system FODD-Planner. A crucial ingredient of this scheme is a set of operations to remove redundancy in decision diagrams, thus keeping them compact. This paper makes three contributions. First, we introduce Generalized FODDs (GFODD) and combination algorithms for them, generalizing FODDs to arbitrary quantification. Second, we show how GFODDs can be used in principle to solve RMDPs with arbitrary quantification, and develop a particularly promising case where an arbitrary number of existential quantifiers is followed by an arbitrary number of universal quantifiers. Third, we develop a new approach to reduce FODDs and GFODDs using model checking. This yields a reduction that is complete for FODDs and provides a sound reduction procedure for GFODDs.},
  	Pages = {1916--1921},
  	Title = {Generalized First Order Decision Diagrams for First Order Markov Decision Processes},
  	Url = {./papers/joshi2009generalized.pdf},
  	Year = {2009},
  	Bdsk-Url-1 = {./papers/joshi2009generalized.pdf}}

  @inproceedings{thurau2009convex,
  	Author = {Christian Thurau and Kristian Kersting and Christian Bauckhage},
  	Booktitle = {Proceedings of the 9th IEEE International Conference on Data Mining (ICDM)},
  	Keywords = {Interpretable, Matrix Factorization, Convex},
  	Note = {Non-negative matrix factorization (NMF) has become a standard tool in data mining, information retrieval, and signal processing. It is used to factorize a non-negative data matrix into two non-negative matrix factors that contain basis elements and linear coefficients, respectively. Often, the columns of the first resulting factor are interpreted as ``cluster centroids'' of the input data, and the columns of the second factor are understood to contain cluster membership indicators. When analyzing data such as collections of gene expressions, documents, or images, it is often beneficial to ensure that the resulting cluster centroids are meaningful, for instance, by restricting them to be convex combinations of data points. However, known approaches to convex-NMF suffer from high computational costs and therefore hardly apply to large-scale data analysis problems. This paper presents a new framework for convex-NMF that allows for an efficient factorization of data matrices of millions of data points. Triggered by the simple observation that each data point can be expressed as a convex combination of vertices of the data convex hull, we require the basic factors to be vertices of the data convex hull. The benefits of convex-hull NMF are twofold. First, for a growing number of data points the expected size of the convex hull, i.e. the number of its vertices, grows much slower than the dataset. Second, distance preserving low-dimensional embeddings allow us to efficiently sample the convex hull and hence to quickly determine candidate vertices. Our extensive experimental evaluation on large datasets shows that convex-hull NMF compares favorably to convex-NMF in terms of both speed and reconstruction quality. We demonstrate that our method can easily be applied to large-scale, real-world datasets, in our case consisting of 750,000 DBLP entries, 4,000,000 digital images, and 150,000,000 votes on World of Warcraft {\textregistered}guilds, respectively.},
  	Organization = {IEEE},
  	Pages = {523--532},
  	Title = {Convex non-negative matrix factorization in the wild},
  	Year = {2009}}

  @inproceedings{kersting2009learning,
  	Anote = {./images/kersting2009learning.png},
  	Author = {Kristian Kersting and Zhao Xu},
  	Booktitle = {European Conference on Machine Learning and Knowledge Discovery in Databases (ECML PKDD)},
  	Keywords = {Gaussian Processes, Preference Learning, Statistical Relational learning, Hidden Common Cause Relations},
  	Note = {Gaussian processes have successfully been used to learn preferences among entities as they provide nonparametric Bayesian approaches for model selection and probabilistic inference. For many entities encountered in real-world applications, however, there are complex relations between them. In this paper, we present a preference model which incorporates information on relations among entities. Specifically, we propose a probabilistic relational kernel model for preference learning based on Silva et al.'s mixed graph Gaussian processes: a new prior distribution, enhanced with relational graph kernels, is proposed to capture the correlations between preferences. Empirical analysis on the LETOR datasets demonstrates that relational information can improve the performance of preference learning.},
  	Pages = {676--691},
  	Publisher = {Springer},
  	Title = {Learning preferences with hidden common cause relations},
  	Url = {./papers/kersting2009learning.pdf},
  	Year = {2009},
  	Bdsk-Url-1 = {./papers/kersting2009learning.pdf}}

  @inproceedings{quadrianto2009kernel,
  	Anote = {./images/quadrianto2009kernel.png},
  	Author = {Novi Quadrianto and Kristian Kersting and Mark Reid and Tiberio Caetano and Wray Buntine},
  	Booktitle = {Proceedings of the 9th International Conference on Data Mining (ICDM)},
  	Keywords = {Gaussian Processes, Quantile Esimtation, Reduction},
  	Note = {Quantile regression refers to the process of estimating the quantiles of a conditional distribution and has many important applications within econometrics and data mining, among other domains. In this paper, we show how to estimate these quantile functions within a Bayes risk minimization framework using a Gaussian process prior. The resulting non-parametric probabilistic model is easy to implement and allows non-crossing quantile functions to be enforced. Moreover, it can directly be used in combination with tools and extensions of standard Gaussian Processes such as principled hyperparameter estimation, sparsification, and quantile regression with input-dependent noise rates. No existing approach enjoys all of these desirable properties. Experiments on benchmark datasets show that our method is competitive with state-of-the-art approaches.},
  	Organization = {IEEE},
  	Pages = {938--943},
  	Title = {Kernel conditional quantile estimation via reduction revisited},
  	Url = {./papers/quadrianto2009kernel.pdf},
  	Year = {2009},
  	Bdsk-Url-1 = {./papers/quadrianto2009kernel.pdf}}

  @inproceedings{neumann2009stacked,
  	Anote = {./images/neumann2009stacked.png},
  	Author = {Marion Neumann and Kristian Kersting and Zhao Xu and Daniel Schulz},
  	Booktitle = {Proceedings of the 9th IEEE International Conference on Data Mining( ICDM)},
  	Keywords = {Gaussian Processes, Stacked Learning, Relational Domain},
  	Note = {Triggered by a market relevant application that involves making joint predictions of pedestrian and public transit flows in urban areas, we address the question of how to utilize hidden common cause relations among variables of interest in order to improve performance in the two related regression tasks. Specifically, we propose stacked Gaussian process learning, a meta-learning scheme in which a base Gaus- sian process is enhanced by adding the posterior covariance functions of other related tasks to its covariance function in a stage-wise optimization. The idea is that the stacked posterior covariances encode the hidden common causes among variables of interest that are shared across the related regression tasks. Stacked Gaussian process learning is efficient, capable of capturing shared common causes, and can be implemented with any kind of standard Gaussian process regression model such as sparse approximations and relational variants. Our experimental results on real-world data from the market relevant application show that stacked Gaussian processes learning can significantly improve prediction performance of a standard Gaussian process.},
  	Organization = {IEEE},
  	Pages = {387--396},
  	Title = {Stacked Gaussian process learning},
  	Url = {./papers/neumann2009stacked.pdf},
  	Year = {2009},
  	Bdsk-Url-1 = {./papers/neumann2009stacked.pdf}}

  @inproceedings{schulz2009ilp,
  	Anote = {./images/schulz2009ilp.png},
  	Author = {Hannes Schulz and Kristian Kersting and Andreas Karwath},
  	Booktitle = {Proceedings of the 19th International Conference on Inductive Logic Programming (ILP)},
  	Keywords = {Euclidean Embedding, Relational Learning, Statistical Relational Learning, Inductive Logic Programming},
  	Note = {Relational data is complex. This complexity makes one of the basic steps of ILP difficult: understanding the data and results. If the user cannot easily understand it, he draws incomplete conclusions. The situation is very much as in the parable of the blind men and the elephant that appears in many cultures. In this tale the blind work independently and with quite different pieces of information, thereby drawing very different conclusions about the nature of the beast. In contrast, visual representations make it easy to shift from one perspective to another while exploring and analyzing data. This paper describes a method for embedding interpretations and queries into a single, common Euclidean space based on their co-proven statistics. We demonstrate our method on real-world datasets showing that ILP results can indeed be captured at a glance.},
  	Organization = {Springer},
  	Pages = {209--216},
  	Title = {ILP, the blind, and the elephant: Euclidean embedding of co-proven queries},
  	Url = {./papers/schulz2009ilp.pdf},
  	Year = {2009},
  	Bdsk-Url-1 = {./papers/schulz2009ilp.pdf}}

  @article{plagemann2009bayesian,
  	Anote = {./images/jfr09-cp.png},
  	Author = {Christian Plagemann and Sebastian Mischke and Sam Prentice and Kristian Kersting and Nicholas Roy and Wolfram Burgard},
  	Journal = {Journal of Field Robotics},
  	Keywords = {Robotics, Gaussian Processes, Terrain Mapping},
  	Note = {We deal with the problem of learning probabilistic models of terrain surfaces from sparse and noisy elevation measurements. The key idea is to formalize this as a regression problem and to derive a solution based on nonstationary Gaussian processes. We describe how to achieve a sparse approximation of the model, which makes the model applicable to real-world data sets. The main benefits of our model are that (1) it does not require a discretization of space, (2) it also provides the uncertainty for its predictions, and (3) it adapts its covariance function to the observed data, allowing more accurate inference of terrain elevation at points that have not been observed directly. As a second contribution, we describe how a legged robot equipped with a laser range finder can utilize the developed terrain model to plan and execute a path over rough terrain. We show how a motion planner can use the learned terrain model to plan a path to a goal location, using a terrain-specific cost model to accept or reject candidate footholds. To the best of our knowledge, this was the first legged robotics system to autonomously sense, plan, and traverse a terrain surface of the given complexity.},
  	Number = {10},
  	Pages = {789--811},
  	Publisher = {Wiley Online Library},
  	Title = {A Bayesian regression approach to terrain mapping and an application to legged robot locomotion},
  	Url = {https://people.csail.mit.edu/prentice/papers/jfr09-cp.pdf},
  	Volume = {26},
  	Year = {2009},
  	Bdsk-Url-1 = {https://people.csail.mit.edu/prentice/papers/jfr09-cp.pdf}}

  @misc{deraedt200807161,
  	Author = {Luc De Raedt and Thomas Dietterich and Lise Getoor and Kristian Kersting and Stephen H Muggleton},
  	Howpublished = {Dagstuhl Seminar Proceedings},
  	Organization = {Schloss Dagstuhl-Leibniz-Zentrum f√ºr Informatik},
  	Title = {07161 Abstracts Collection--Probabilistic, Logical and Relational Learning-A Further Synthesis},
  	Year = {2008}}

  @inproceedings{gutmann2008parameter,
  	Anote = {./images/gutmann2008parameter.png},
  	Author = {Bernd Gutmann and Angelika Kimmig and Kristian Kersting and Luc De Raedt},
  	Booktitle = {Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases (ECML PKDD)},
  	Keywords = {Statistical Relational Learning, Least Squares, Parameter Estimation},
  	Note = {We introduce the problem of learning the parameters of the probabilistic database ProbLog. Given the observed success probabilities of a set of queries, we compute the probabilities attached to facts that have a low approximation error on the training examples as well as on unseen examples. Assuming Gaussian error terms on the observed success probabilities, this naturally leads to a least squares optimization problem. Our approach, called LeProbLog, is able to learn both from queries and from proofs and even from both simultaneously. This makes it flexible and allows faster training in domains where the proofs are available. Experiments on real world data show the usefulness and effectiveness of this least squares calibration of probabilistic databases.},
  	Organization = {Springer Berlin Heidelberg},
  	Pages = {473--488},
  	Title = {Parameter learning in probabilistic databases: A least squares approach},
  	Url = {./papers/gutmann2008parameter.pdf},
  	Year = {2008},
  	Bdsk-Url-1 = {./papers/gutmann2008parameter.pdf}}

  @inproceedings{plagemann2008learning,
  	Anote = {./images/plagemann08iros.png},
  	Author = {Christian Plagemann and Sebastian Mischke and Sam Prentice and Kristian Kersting and Nicholas Roy and Wolfram Burgard},
  	Booktitle = {Proceedings of the IEEE International Conference on Intelligent Robots and Systems (IROS)},
  	Keywords = {Robotics, Gaussian Processes, Terrain Model, Predictive Uncertainty},
  	Note = {Legged robots require accurate models of their environment in order to plan and execute paths. We present a probabilistic technique based on Gaussian processes that allows terrain models to be learned and updated efficiently using sparse approximation techniques. The major benefit of our terrain model is its ability to predict elevations at unseen locations more reliably than alternative approaches, while it also yields estimates of the uncertainty in the prediction. In particular, our nonstationary Gaussian process model adapts its covariance to the situation at hand, allowing more accurate inference of terrain height at points that have not been observed directly. We show how a conventional motion planner can use the learned terrain model to plan a path to a goal location, using a terrain-specific cost model to accept or reject candidate footholds. In experiments with a real quadruped robot equipped with a laser range finder, we demonstrate the usefulness of our approach and discuss its benefits compared to simpler terrain models such as elevations grids.},
  	Organization = {IEEE},
  	Pages = {3545--3552},
  	Title = {Learning predictive terrain models for legged robot locomotion},
  	Url = {./paper/plagemann08iros.pdf},
  	Year = {2008},
  	Bdsk-Url-1 = {./paper/plagemann08iros.pdf}}

  @inproceedings{kersting2008non,
  	Anote = {./images/kersting2008non.png},
  	Author = {Kristian Kersting and Kurt Driessens},
  	Booktitle = {Proceedings of the 25th international conference on Machine learning (ICML)},
  	Keywords = {Statistical Relational Learning, Relational Reinforcement Learning, Policy Gradient, Functional Gradient Boosting},
  	Note = {Policy gradient approaches are a powerful instrument for learning how to interact with the environment. Existing approaches have focused on propositional and continuous domains only. Without extensive feature engineering, it is difficult -- if not impossible -- to apply them within structured domains, in which e.g. there is a varying number of objects and relations among them. In this paper, we describe a non-parametric policy gradient approach -- called NPPG -- that overcomes this limitation. The key idea is to apply Friedmann's gradient boosting: policies are represented as a weighted sum of regression models grown in an stage-wise optimization. Employing off-the-shelf regression learners, NPPG can deal with propositional, continuous, and relational domains in a uni- fied way. Our experimental results show that it can even improve on established results.},
  	Organization = {ACM},
  	Pages = {456--463},
  	Title = {Non-parametric policy gradients: A unified treatment of propositional and relational domains},
  	Url = {./papers/kersting2008non.pdf},
  	Year = {2008},
  	Bdsk-Url-1 = {./papers/kersting2008non.pdf}}

  @inproceedings{karwath2008boosting,
  	Anote = {./images/karwath2008boosting.png},
  	Author = {Andreas Karwath and Kristian Kersting and Niels Landwehr},
  	Booktitle = {Proceedings of the 8th IEEE International Conference on Data Mining (ICDM)},
  	Keywords = {Statistical Relational Learning, Functional Gradient Boosting, Sequence Alignment},
  	Note = {The task of aligning sequences arises in many applications. Classical dynamic programming approaches require the explicit state enumeration in the reward model. This is often impractical: the number of states grows very quickly with the number of domain objects and relations among these objects. Relational sequence alignment aims at exploiting symbolic structure to avoid the full enumeration. This comes at the expense of a more complex reward model selection problem: virtually infinitely many abstraction levels have to be explored. In this paper, we apply gradientbased boosting to leverage this problem. Specifically, we show how to reduce the learning problem to a series of relational regressions problems. The main benefit of this is that interactions between states variables are introduced only as needed, so that the potentially infinite search space is not explicitly considered. As our experimental results show, this boosting approach can significantly improve upon established results in challenging applications.},
  	Organization = {IEEE},
  	Pages = {857--862},
  	Title = {Boosting relational sequence alignments},
  	Url = {./papers/karwath2008boosting.pdf},
  	Year = {2008},
  	Bdsk-Url-1 = {./papers/karwath2008boosting.pdf}}

  @inproceedings{plagemann2008nonstationary,
  	Anote = {./images/plagemann2008nonstationary.png},
  	Author = {Christian Plagemann and Kristian Kersting and Wolfram Burgard},
  	Booktitle = {European Conference on Machine Learning and Knowledge Discovery in Databases (ECML PKDD)},
  	Keywords = {Gaussian Processes, Bayesian Regression, Non-Stationary},
  	Note = {Gaussian processes using nonstationary covariance functions are a powerful tool for Bayesian regression with input-dependent smoothness. A common approach is to model the local smoothness by a latent process that is integrated over using Markov chain Monte Carlo approaches. In this paper, we demonstrate that an approximation that uses the estimated mean of the local smoothness yields good results and allows one to employ efficient gradient-based optimization techniques for jointly learning the parameters of the latent and the observed processes. Extensive experiments on both synthetic and real-world data, including challenging problems in robotics, show the relevance and feasibility of our approach.},
  	Pages = {204--219},
  	Publisher = {Springer},
  	Title = {Nonstationary Gaussian process regression using point estimates of local smoothness},
  	Url = {./papers/plagemann2008nonstationary.pdf},
  	Year = {2008},
  	Bdsk-Url-1 = {./papers/plagemann2008nonstationary.pdf}}

  @inproceedings{natarajan2008logical,
  	Anote = {./images/natarajan2008logical.png},
  	Author = {Sriraam Natarajan and Hung Bui and Prasad Tadepalli and Kristian Kersting and Weng-Keen Wong},
  	Booktitle = {Proceedings of the 18th International Conference on Inductive Logic Programming (ILP)},
  	Keywords = {Statistical Relational Learning, Logical Hidden Markov Models, User Modeling},
  	Note = {Hidden Markov Models (HMM) have been successfully used in applications such as speech recognition, activity recognition, bioinformatics etc. There have been previous attempts such as Hierarchical HMMs and Abstract HMMs to elegantly extend HMMs at multiple levels of temporal abstraction (for example to represent the user's activities). Similarly, there has been previous work such as Logical HMMs on extending HMMs to domains with relational structure. In this work we develop a representation that naturally combines the power of both relational and hierarchical models in the form of Logical Hierarchical Hidden Markov Models (LoHiHMMs). LoHiHMMs inherit the compactness of representation from Logical HMMs and the tractability of inference from Hierarchical HMMs. We outline two inference algorithms: one based on grounding the LoHiHMM to a propositional HMM and the other based on particle filtering adapted for this setting. We present the results of our experiments with the model in two simulated domains.},
  	Organization = {Springer},
  	Pages = {192--209},
  	Title = {Logical hierarchical hidden markov models for modeling user activities},
  	Url = {./papers/natarajan2008logical.pdf},
  	Year = {2008},
  	Bdsk-Url-1 = {./papers/natarajan2008logical.pdf}}

  @inproceedings{katz2008modeling,
  	Anote = {./images/katz2008modeling.png},
  	Author = {Yarden Katz and Noah D Goodman and Kristian Kersting and Charles Kemp and Joshua B Tenenbaum},
  	Booktitle = {Proceedings of the Cognitive Science Society (CogSci)},
  	Keywords = {Cognitive Science, Statistical Relational Learning, Logical Dimensionality Reduction},
  	Note = {Semantic knowledge is often expressed in the form of intuitive theories, which organize, predict and explain our observations of the world. How are these powerful knowledge structures represented and acquired? We present a framework, logical dimensionality reduction, that treats theories as compressive probabilistic models, attempting to express observed data as a sample from the logical consequences of the theory's underlying laws and a small number of core facts. By performing Bayesian learning and inference on these models we combine important features of more familiar connectionist and symbolic approaches to semantic cognition: an ability to handle graded, uncertain inferences, together with systematicity and compositionality that support appropriate inferences from sparse observations in novel contexts.},
  	Number = {30},
  	Title = {Modeling semantic cognition as logical dimensionality reduction},
  	Url = {./papers/katz2008modeling.pdf},
  	Volume = {30},
  	Year = {2008},
  	Bdsk-Url-1 = {./papers/katz2008modeling.pdf}}

  @inproceedings{milchZKHK08,
  	Anote = {./images/milchZKHK08.png},
  	Author = {Brian Milch and Luke S. Zettlemoyer and Kristian Kersting and Michael Haimes and Leslie Pack Kaelbling},
  	Booktitle = {Proceedings of the 23th {AAAI} Conference on Artificial Intelligence, (AAAI)},
  	Keywords = {Statistical Relational AI, Lifted Inference, Exact, C-FOVE},
  	Note = {Lifted inference algorithms exploit repeated structure in probabilistic models to answer queries efficiently. Previous work such as de Salvo Braz et al.'s first-order variable elimination (FOVE) has focused on the sharing of potentials across interchangeable random variables. In this paper, we also exploit interchangeability within individual potentials by introducing counting formulas, which indicate how many of the random variables in a set have each possible value. We present a new lifted inference algorithm, C-FOVE, that not only handles counting formulas in its input, but also creates counting formulas for use in intermediate potentials. C-FOVE can be described succinctly in terms of six operators, along with heuristics for when to apply them. Because counting formulas capture dependencies among large numbers of variables compactly, C-FOVE achieves asymptotic speed improvements compared to FOVE.},
  	Pages = {1062--1068},
  	Title = {Lifted Probabilistic Inference with Counting Formulas},
  	Url = {http://www.aaai.org/Papers/AAAI/2008/AAAI08-168.pdf},
  	Year = {2008},
  	Bdsk-Url-1 = {http://www.aaai.org/Papers/AAAI/2008/AAAI08-168.pdf}}

  @incollection{missura2008towards,
  	Author = {Olana Missura and Kristian Kersting and Thomas G{\"a}rtner},
  	Booktitle = {Wokring Notes of the Workshop on Artificial Intelligence in Games},
  	Keywords = {Automatic difficulty scaling, Markov Decision Processes},
  	Pages = {55},
  	Title = {Towards Engaging MDPs},
  	Year = {2008}}

  @incollection{xu2008gaussian,
  	Anote = {./images/xu2008gaussian.png},
  	Author = {Zhao Xu and Kristian Kersting and Volker Tresp},
  	Booktitle = {Working Notes of the NIPS Workshop on Analyzing Graphs},
  	Keywords = {Statistical Relational Learning, Gaussian Processes},
  	Note = {Many real-world domains can naturally be represented as complex graphs, i.e., in terms of entities (nodes) and relations (edges) among them. In domains with multiple relations, represented as colored graphs, we may improve the quality of a model by exploiting the correlations among relations of different types. To this end, we develop a multi-relational Gaussian process (MRGP) model. The MRGP model introduces multiple GPs for each type of entities. Each random variable drawn from a GP represents profile/preference of an entity in some aspect, which is the function value of entity features at the aspect. These GPs are then coupled together via relations between entities. The MRGP model can be used for relation prediction and (semi-) supervised learning. We give an analysis of the MRGP model for bipartite, directed and undirected univariate relations.},
  	Title = {Gaussian Process Models for Colored Graphs},
  	Url = {./papers/xu2008gaussian.pdf},
  	Year = {2008},
  	Bdsk-Url-1 = {./papers/xu2008gaussian.pdf}}

  @inbook{kersting2008basic,
  	Author = {Kristian Kersting and Luc De Raedt},
  	Booktitle = {Probabilistic Inductive Logic Programming},
  	Keywords = {Statistical Relational Learning, Bayesian Logic Programs, Bayesian Networks, Logic Programming},
  	Note = {Bayesian logic programs tightly integrate definite logic programs with Bayesian networks in order to incorporate the notions of objects and relations into Bayesian networks. They establish a one-to-one mapping between ground atoms and random variables, and between the immediate consequence operator and the directly influenced by relation. In doing so, they nicely separate the qualitative (i.e. logical) component from the quantitative (i.e. the probabilistic) one providing a natural framework to describe general, probabilistic dependencies among sets of random variables. In this chapter, we present results on combining Inductive Logic Programming with Bayesian networks to learn both the qualitative and the quantitative components of Bayesian logic programs from data. More precisely, we show how the qualitative components can be learned by combining the inductive logic programming setting learning from interpretations with score-based techniques for learning Bayesian networks. The estimation of the quantitative components is reduced to the corresponding problem of (dynamic) Bayesian networks.},
  	Pages = {189--221},
  	Publisher = {Springer},
  	Title = {Basic principles of learning Bayesian logic programs},
  	Url = {https://link.springer.com/chapter/10.1007/978-3-540-78652-8_7},
  	Year = {2008},
  	Bdsk-Url-1 = {https://link.springer.com/chapter/10.1007/978-3-540-78652-8_7}}

  @inbook{kersting2008relational,
  	Author = {Kristian Kersting and Luc De Raedt and Bernd Gutmann and Andreas Karwath and Niels Landwehr},
  	Booktitle = {Probabilistic inductive logic programming},
  	Keywords = {Statistical Relational Learning, Complex Sequences, Sequence Labeling, Reinforcement Learning},
  	Note = {Sequential behavior and sequence learning are essential to intelligence. Often the elements of sequences exhibit an internal structure that can elegantly be represented using relational atoms. Applying traditional sequential learning techniques to such relational sequences requires one either to ignore the internal structure or to live with a combinatorial explosion of the model complexity. This chapter briefly reviews relational sequence learning and describes several techniques tailored towards realizing this, such as local pattern mining techniques, (hidden) Markov models, conditional random fields, dynamic programming and reinforcement learning.},
  	Pages = {28--55},
  	Publisher = {Springer},
  	Title = {Relational sequence learning},
  	Url = {https://link.springer.com/chapter/10.1007/978-3-540-78652-8_2},
  	Year = {2008},
  	Bdsk-Url-1 = {https://link.springer.com/chapter/10.1007/978-3-540-78652-8_2}}

  @book{deraedt2008probabilistic,
  	Anote = {./images/pilp2.jpg},
  	Author = {Luc {De Raedt} and Paolo Frasconi and Kristian Kersting and Stephen Muggleton},
  	Keywords = {Statistical Relational Learning, Inductive Logic Programming, Probabilistic Inductive Logic Programming},
  	Note = {This editorial book provides an introduction to statistical relational learning with an emphasis on those methods based on logic programming principles. The question of how to combine probability and logic with learning is getting an increased attention as there is an explosive growth in the amount of heterogeneous data that is being collected in the business and scientific world. The structures encountered can be as simple as sequences and trees or as complex as citation graphs, the World Wide Web, and relational databases},
  	Publisher = {Springer},
  	Series = {Lecture Notes in Computer Science},
  	Title = {Probabilistic Inductive Logic Programming-Theory and Applications},
  	Url = {https://link.springer.com/book/10.1007%2F978-3-540-78652-8},
  	Volume = {4911},
  	Year = {2008},
  	Bdsk-Url-1 = {https://link.springer.com/book/10.1007%2F978-3-540-78652-8}}

  @article{deraedt2008compressing,
  	Anote = {./images/deraedt2008compressing.png},
  	Author = {Luc De Raedt and Kristian Kersting and Angelika Kimmig and Kate Revoredo and Hannu Toivonen},
  	Journal = {Machine Learning (MLJ)},
  	Keywords = {Statistical Relational Learning, Structure Learning, ProbLog, Compression},
  	Note = {A ProbLog program defines a distribution over logic programs by specifying for each clause the probability that it belongs to a randomly sampled program, and these probabilities are mutually independent. The semantics of ProbLog is then defined by the success probability of a query in a randomly sampled program. This paper introduces the theory compression task for ProbLog, which consists of selecting that subset of clauses of a given ProbLog program that maximizes the likelihood w.r.t. a set of positive and negative examples. Experiments in the context of discovering links in real biological networks demonstrate the practical applicability of the approach.},
  	Number = {2},
  	Pages = {151--168},
  	Publisher = {Springer},
  	Title = {Compressing probabilistic Prolog programs},
  	Url = {./papers/deraedt2008compressing.pdf},
  	Volume = {70},
  	Year = {2008},
  	Bdsk-Url-1 = {./papers/deraedt2008compressing.pdf}}

  @inproceedings{plagemann2007gaussian,
  	Anote = {./images/plagemann2007gaussian.png},
  	Author = {Christian Plagemann and Kristian Kersting and Patrick Pfaff and Wolfram Burgard},
  	Booktitle = {Proceedings of Robotics: Science and Systems (RSS)},
  	Keywords = {Gaussian Processes, Laser Range Finders, Robotics},
  	Note = {In probabilistic mobile robotics, the development of measurement models plays a crucial role as it directly influences the efficiency and the robustness of the robot's performance in a great variety of tasks including localization, tracking, and map building. In this paper, we present a novel probabilistic measurement model for range finders, called Gaussian beam processes, which treats the measurement modeling task as a nonparametric Bayesian regression problem and solves it using Gaussian processes. The major benefit of our approach is its ability to generalize over entire range scans directly. This way, we can learn the distributions of range measurements for whole regions of the robot's configuration space from only few recorded or simulated range scans. Especially in approximative approaches to state estimation like particle filtering or histogram filtering, this leads to a better approximation of the true likelihood function. Experiments on real world and synthetic data show that Gaussian beam processes combine the advantages of two popular measurement models.},
  	Title = {Gaussian Beam Processes: A Nonparametric Bayesian Measurement Model for Range Finders},
  	Url = {http://www.roboticsproceedings.org/rss03/p18.pdf},
  	Year = {2007},
  	Bdsk-Url-1 = {http://www.roboticsproceedings.org/rss03/p18.pdf}}

  @inproceedings{kerstingPPB07,
  	Anote = {./images/kerstingPPB07.png},
  	Author = {Kristian Kersting and Christian Plagemann and Patrick Pfaff and Wolfram Burgard},
  	Booktitle = {Proceedings of the 24th International Conference Machine Learning (ICML)},
  	Keywords = {Gaussian Processes, Heteroscedastic, Input-Dependent Noise, Sampling},
  	Note = {This paper presents a novel Gaussian process (GP) approach to regression with inputdependent noise rates. We follow Goldberg et al.'s approach and model the noise variance using a second GP in addition to the GP governing the noise-free output value. In contrast to Goldberg et al., however, we do not use a Markov chain Monte Carlo method to approximate the posterior noise variance but a most likely noise approach. The resulting model is easy to implement and can directly be used in combination with various existing extensions of the standard GPs such as sparse approximations. Extensive experiments on both synthetic and real-world data, including a challenging perception problem in robotics, show the effectiveness of most likely heteroscedastic GP regression.},
  	Pages = {393--400},
  	Title = {Most likely heteroscedastic Gaussian process regression},
  	Url = {http://www.machinelearning.org/proceedings/icml2007/papers/326.pdf},
  	Year = {2007},
  	Bdsk-Url-1 = {http://www.machinelearning.org/proceedings/icml2007/papers/326.pdf}}

  @incollection{kersting2007reasoning,
  	Anote = {./images/kersting2007reasoning.png},
  	Author = {Kristian Kersting and Brian Milch and Luke Zettlemoyer and Michael Haimes and Leslie Pack Kaelbling},
  	Booktitle = {Working Notes of the NIPS Workshop on Statistical Models of Networks},
  	Keywords = {Statistical Relational AI, Lifted Inference, Exact, C-FOVE},
  	Note = {We use a concrete problem in the context of planning meetings to show how lifted probabilistic inference can dramatically speed up reasoning. We also extend lifted inference to deal with cardinality potentials, and examine how to deal with background knowledge about a social network.},
  	Title = {Reasoning about large populations with lifted probabilistic inference},
  	Url = {./papers/kersting2007reasoning.pdf},
  	Year = {2007},
  	Bdsk-Url-1 = {./papers/kersting2007reasoning.pdf}}

  @incollection{gutmann2007stratified,
  	Anote = {./images/gutmann2007stratified.png},
  	Author = {Bernd Gutmann and Kristian Kersting},
  	Booktitle = {Wokring Notes of the 6th international workshop on multi-relational data mining (MRDM)},
  	Keywords = {Functional Gradient Boosting, Imbalanced Data, Stratified Sampling, Conditional Random Fields},
  	Note = {Boosting has recently been shown to be a promising approach for training conditional random fields (CRFs) as it allows to effi- ciently induce conjunctive (even relational) features. The potentials are represented as weighted sums of regression trees that are induced using gradient tree boosting. Its large scale application such as in relational domains, however, suffers from two drawbacks: induced trees can spoil previous maximizations and the number of generated regression examples can become quite large. In this paper, we propose to tackle the latter problem by injecting randomness into the regression estimation procedure by subsampling regression examples. Experiments on a real-world data set show that this sampling approach is comparable with more sophisticated boosting algorithms in early iterations and, hence, provides an interesting alternative as it is much simpler to implement.},
  	Pages = {56--68},
  	Title = {Stratified gradient boosting for fast training of conditional random fields},
  	Url = {./papers/gutmann2007stratified.pdf},
  	Year = {2007},
  	Bdsk-Url-1 = {./papers/gutmann2007stratified.pdf}}

  @incollection{thon2007distributed,
  	Anote = {./images/thon2007distributed.png},
  	Author = {Ingo Thon and Kristian Kersting},
  	Booktitle = {Working Notes of the 6th International Workshop on Multirelational Data Mining (MRDM)},
  	Keywords = {Statistical Relational Learning, Factorial, Logical Hidden Markov Models},
  	Note = {Several promising variants of hidden Markov models (HMMs) have recently been developed to efficiently deal with large state and observation spaces and relational structure. Many application domains, however, have an apriori componential structure such as parts in musical scores. In this case, exact inference within relational HMMs still grows exponentially in the number of components. In this paper, we propose to approximate the complex joint relational HMM with a simpler, distributed one: k relational hidden chains over n states, one for each component. Then, we iteratively perform inference for each chain given fixed values for the other chains until convergence. Due to this structured mean field approximation, the effective size of the hidden state space collapses from exponential to linear.},
  	Pages = {129--140},
  	Title = {Distributed relational state representations for complex stochastic processes (extended abstract)},
  	Url = {./papers/thon2007distributed.pdf},
  	Year = {2007},
  	Bdsk-Url-1 = {./papers/thon2007distributed.pdf}}

  @incollection{ganzert2007equation,
  	Anote = {./images/ganzert2010identifying.png},
  	Author = {Steven Ganzert and Knut M√∂ller and Kristian Kersting and Luc De Raedt and Josef Guttmann},
  	Booktitle = {Working notes of the ICML workshop on the Induction of Process Models (IPM)},
  	Keywords = {Process Model Induction, Differential Equations, Medicine},
  	Note = {Lung protective ventilation considerably improves the outcome of mechanically ventilated and critically ill patients as it avoids extensive mechanical stress of the lung tissue and hence its irreversible damage. A valid analysis of respiratory mechanics is a prerequisite for lung protective ventilation. This analysis is always based on mathematical models. The equation of motion defines a generally accepted model of the respiratory system. It relates the airway pressure to the ventilator induced airflow and volume application influenced by distensibility and resistance of the respiratory system. We present a novel equation discovery system which combines the technique of using declarative bias for the reduction of the vast search space known from the LAGRAMGE-system with a greedy, randomized search strategy according to GSAT. We experimentally validate the effectiveness of our approach and show that the equation of motion model can automatically be rediscovered from real-world data.},
  	Title = {Equation discovery for model identification in respiratory mechanics under conditions of mechanical ventilation},
  	Url = {./papers/ganzert2007equation.pdf},
  	Year = {2007},
  	Bdsk-Url-1 = {./papers/ganzert2007equation.pdf}}

  @incollection{gutmann2007mlg,
  	Author = {Bernd Gutmann and Kristian Kersting},
  	Booktitle = {Working Notes of the 5th International Workshop on Mining and Learning with Graphs (MLG)},
  	Keywords = {Statistical Relational Learning, Funcational Gradient Boosting, Conditional Random Fields, Imbalanced Data},
  	Note = {Boosting has recently been shown to be a promising approach for training conditional random fields (CRFs) as it allows to efficiently induce conjunctive (even relational) features. The potentials are represented as weighted sums of regression trees that are induced using gradient tree boosting. Its large scale application, however, suffers from two drawbacks: induced trees can spoil previous maximizations and the number of generated regression examples can become quite large. In this paper, we propose to tackle the latter problem by injecting randomness in the regression estimation procedure due to subsampling regression examples.},
  	Pages = {131--134},
  	Title = {Stratified conjugate gradient boosting for fast training of conditional random fields (extended abstract)},
  	Url = {./images/gutmann2007mlg.pdf},
  	Year = {2007},
  	Bdsk-Url-1 = {./images/gutmann2007mlg.pdf}}

    @inproceedings{landwehrKR05,
    author    = {Niels Landwehr and
                 Kristian Kersting and
                 Luc De Raedt},
    Anote = {./images/landwehr2007integrating.png},
    Keywords = {Statistical Relational Learning, FOIL, Naive Bayes, Rule Learning},
    title     = {nFOIL: Integrating Naive Bayes and {FOIL}},
    booktitle = {Proceedings of the Twentieth National Conference on Artificial Intelligence
                 (AAAI))},
    pages     = {795--800},
    year      = {2005}
  }

  @article{landwehr2007integrating,
  	Anote = {./images/landwehr2007integrating.png},
  	Author = {Niels Landwehr and Kristian Kersting and Luc De Raedt},
  	Journal = {Journal of Machine Learning Research (JMLR)},
  	Keywords = {Statistical Relational Learning, FOIL, Naive Bayes, Rule Learning},
  	Note = {A novel relational learning approach that tightly integrates the na¬®ƒ±ve Bayes learning scheme with the inductive logic programming rule-learner FOIL is presented. In contrast to previous combinations that have employed na¬®ƒ±ve Bayes only for post-processing the rule sets, the presented approach employs the na¬®ƒ±ve Bayes criterion to guide its search directly.
    The proposed technique is implemented in the NFOIL and TFOIL systems, which employ standard na¬®ƒ±ve Bayes and tree augmented na¬®ƒ±ve Bayes models respectively. We show that these integrated approaches to probabilistic model and rule learning outperform post-processing approaches. They also yield significantly more accurate models than simple rule learning and are competitive with more sophisticated ILP systems.},
  	Number = {Mar},
  	Pages = {481--507},
  	Title = {Integrating naive bayes and foil},
  	Url = {./papers/landwehr2007integrating.pdf},
  	Volume = {8},
  	Year = {2007},
  	Bdsk-Url-1 = {./papers/landwehr2007integrating.pdf}}

  @article{kersting2007learning,
  	Anote = {./images/kersting2007learning.png},
  	Author = {Kristian Kersting and Christian Plagemann and Alexandru Cocora and Wolfram Burgard and Luc De Raedt},
  	Journal = {Advanced Robotics},
  	Keywords = {Statistical Relational Learning, Robotics, Policy Learning, Relational Domains},
  	Note = {Autonomous agents that act in the real world utilizing sensory input greatly rely on the ability to plan their actions and to transfer these skills across tasks. The majority of path-planning approaches for mobile robots, however, solve the current navigation problem from scratch given the current and goal configuration of the robot. Consequently, these approaches yield highly efficient plans for the specific situation, but the computed policies typically do not transfer to other, similar tasks. In this paper, we propose to apply techniques from statistical relational learning to the pathplanning problem. More precisely, we propose to learn relational decision trees as abstract navigation strategies from example paths. Relational abstraction has several interesting and important properties. First, it allows a mobile robot to imitate navigation behavior shown by users or by optimal policies. Second, it yields comprehensible models of behavior. Finally, a navigation policy learned in one environment naturally transfers to unknown environments. In several experiments with real robots and in simulated runs, we demonstrate that our approach yields efficient navigation plans. We show that our system is robust against observation noise and can outperform hand-crafted policies.},
  	Number = {13},
  	Pages = {1565--1582},
  	Publisher = {Taylor \& Francis Group},
  	Title = {Learning to transfer optimal navigation policies},
  	Url = {./papers/kersting2007learning.pdf},
  	Volume = {21},
  	Year = {2007},
  	Bdsk-Url-1 = {./papers/kersting2007learning.pdf}}

  @inproceedings{gutmann2006tildecrf,
  	Anote = {./images/gutmann2006tildecrf.png},
  	Author = {Bernd Gutmann and Kristian Kersting},
  	Booktitle = {Proceedings of the European Conference on Machine Learning (ECML)},
  	Keywords = {Statistical Relational Learning, Sequence Labeleing, Conditional Random Fields, Functional Gradient Boosting},
  	Note = {Conditional Random Fields (CRFs) provide a powerful instrument for labeling sequences. So far, however, CRFs have only been considered for labeling sequences over flat alphabets. In this paper, we describe TildeCRF, the first method for training CRFs on logical sequences, i.e., sequences over an alphabet of logical atoms. TildeCRF's key idea is to use relational regression trees in Dietterich et al.'s gradient tree boosting approach. Thus, the CRF potential functions are represented as weighted sums of relational regression trees. Experiments show a significant improvement over established results achieved with hidden Markov models and Fisher kernels for logical sequences.},
  	Organization = {Springer},
  	Pages = {174--185},
  	Title = {TildeCRF: Conditional random fields for logical sequences},
  	Url = {./papers/gutmann2006tildecrf.pdf},
  	Year = {2006},
    Key = {Best Student Paper Award at ECML 2006},
  	Bdsk-Url-1 = {./papers/gutmann2006tildecrf.pdf}}

  @inproceedings{triebel2006robust,
  	Anote = {./images/triebel2006robust.png},
  	Author = {Rudolph Triebel and Kristian Kersting and Wolfram Burgard},
  	Booktitle = {Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)},
  	Keywords = {3D, Associative Markov Networks, Computer Vision, Laser Range Finders},
  	Note = {In this paper we present an efficient technique to learn Associative Markov Networks (AMNs) for the segmentation of 3D scan data. Our technique is an extension of the work recently presented by Anguelov et al. [1], in which AMNs are applied and the learning is done using max-margin optimization. In this paper we show that by adaptively reducing the training data, the training process can be performed much more efficiently while still achieving good classification results. The reduction is obtained by utilizing kd-trees and pruning them appropriately. Our algorithm does not require any additional parameters and yields an abstraction of the training data. In experiments with real data collected from a mobile outdoor robot we demonstrate that our approach yields accurate segmentations.},
  	Organization = {IEEE},
  	Pages = {2603--2608},
  	Title = {Robust 3D scan point classification using associative Markov networks},
  	Url = {./papers/triebel2006robust.pdf},
  	Year = {2006},
  	Bdsk-Url-1 = {./papers/triebel2006robust.pdf}}

  @inproceedings{cocora2006learning,
  	Anote = {./images/cocora2006learning.png},
  	Author = {Alexandru Cocora and Kristian Kersting and Christian Plagemann and Wolfram Burgard and Luc De Raedt},
  	Booktitle = {Proceedings of the IEEE International Conference on Intelligent Robots and Systems (IROS)},
  	Keywords = {Statistical Relational Learning, Relational Reinforcement Learning, Robotics, Optimal Actions, Policy Learning},
  	Note = {Navigation is one of the fundamental tasks for a mobile robot. The majority of path planning approaches has been designed to entirely solve the given problem from scratch given the current and goal configurations of the robot. Although these approaches yield highly efficient plans, the computed policies typically do not transfer to other, similar tasks. We propose to learn relational decision trees as abstract navigation strategies from example paths. Relational abstraction has several interesting and important properties. First, it allows a mobile robot to generalize navigation plans from specific examples provided by users or exploration. Second, the navigation policy learned in one environment can be transferred to unknown environments. In several experiments with real robots in a real environment and in simulated runs, we demonstrate the usefulness of our approach.},
  	Organization = {IEEE},
  	Pages = {2792--2797},
  	Title = {Learning relational navigation policies},
  	Url = {./papers/cocora2006learning.pdf},
  	Year = {2006},
  	Bdsk-Url-1 = {./papers/cocora2006learning.pdf}}

  @inproceedings{kersting2006fisher,
  	Anote = {./images/kersting2006fisher.png},
  	Author = {Uwe Dick and Kristian Kersting},
  	Booktitle = {Proceedings of the European Conference on Machine Learning (ECML)},
  	Keywords = {Statistical Relational Learning, Fisher Kernels, Classification, Relational Data},
  	Note = {Combining statistical and relational learning receives currently a lot of attention. The majority of statistical relational learning approaches focus on density estimation. For classification, however, it is well-known that the performance of such generative models is often lower than that of discriminative classifiers. One approach to improve the performance of generative models is to combine them with discriminative algorithms. Fisher kernels were developed to combine them with kernel methods, and have shown promising results for the combinations of support vector machines with (logical) hidden Markov models and Bayesian networks. So far, however, Fisher kernels have not been considered for relational data, i.e., data consisting of a collection of objects and relational among these objects. In this paper, we develop Fisher kernels for relational data and empirically show that they can significantly improve over the results achieved without Fisher kernels.},
  	Organization = {Springer},
  	Pages = {114-125},
  	Title = {Fisher kernels for relational data},
  	Url = {./papers/kersting2006fisher.pdf},
  	Year = {2006},
  	Bdsk-Url-1 = {./papers/kersting2006fisher.pdf}}

  @inproceedings{karwath2006relational,
  	Anote = {./images/karwath2006relational.png},
  	Author = {Andreas Karwath and Kristian Kersting},
  	Booktitle = {Proceedings of the 16th International Conference on Inductive Logic Programming (ILP)},
  	Keywords = {Statistical Relational Learning, Complex Sequences, Alignements, Logos},
  	Note = {The need to measure sequence similarity arises in many applicitation domains and often coincides with sequence alignment: the more similar two sequences are, the better they can be aligned. Aligning sequences not only shows how similar sequences are, it also shows where there are differences and correspondences between the sequences. Traditionally, the alignment has been considered for sequences of flat symbols only. Many real world sequences such as natural language sentences and protein secondary structures, however, exhibit rich internal structures. This is akin to the problem of dealing with structured examples studied in the field of inductive logic programming (ILP). In this paper, we introduce Real, which is a powerful, yet simple approach to align sequence of structured symbols using well-established ILP distance measures within traditional alignment methods. Although straight-forward, experiments on protein data and Medline abstracts show that this approach works well in practice, that the resulting alignments can indeed provide more information than flat ones, and that they are meaningful to experts when represented graphically.},
  	Organization = {Springer},
  	Pages = {290--304},
  	Title = {Relational sequence alignments and logos},
  	Url = {./papaer/karwath2006relational.pdf},
  	Year = {2006},
  	Bdsk-Url-1 = {./papaer/karwath2006relational.pdf}}

  @inproceedings{deraedt2006revising,
  	Author = {Luc De Raedt and Kristian Kersting and Angelika Kimmig and Kate Revoredo and Hannu Toivonen},
  	Booktitle = {Proceedings of the 16th International Conference on Inductive Logic Programming (ILP)},
  	Keywords = {Statistical Relational Learning, Theory Revision, ProbLog},
  	Note = {In a recently submitted paper [1], the ProbLog (probabilistic prolog) language has been introduced and various algorithms have been developed for solving and approximating ProbLog queries. Here, we define and study the problem of revising ProbLog theories from examples.},
  	Organization = {Springer},
  	Pages = {30--33},
  	Title = {Revising probabilistic prolog programs},
  	Url = {./papers/deraedt2006revising.pdf},
  	Year = {2006},
  	Bdsk-Url-1 = {./papers/deraedt2006revising.pdf}}

  @incollection{jaeger2006expressivity,
  	Author = {Manfred Jaeger and Kristian Kersting and Luc De Raedt},
  	Booktitle = {Working Notes of the ICML Workshop Open Problems in Statistial Relational Learning (SRL)},
  	Keywords = {Statistical RelationaL Learning, Expressivity},
  	Note = {We propose a framework for analyzing the expressivity of probabilistic logical languages.},
  	Title = {Expressivity analysis for pl-languages},
  	Url = {./papers/jaeger2006expressivity.pdf},
  	Year = {2006},
  	Bdsk-Url-1 = {./papers/jaeger2006expressivity.pdf}}

  @incollection{kersting2006unbiased,
  	Author = {Kristian Kersting and Bernd Gutmann},
  	Booktitle = {Proceedings of the International Workshop on Mining and Learning with Graphs (MLG)},
  	Keywords = {Conditional Random Fields, Functional Gradient Boosting, Conjuate Gradients, Selection Bias},
  	Note = {Conditional Random Fields (CRFs) currently receive a lot of attention for labeling sequences. To train CRFs, Dietterich et al. proposed a functional gradient optimization approach: the potential functions are represented as weighted sums of regression trees that are induced using Friedman's gradient tree boosting method. In this paper, we improve upon this approach in two ways. First, we identify an expectation selection bias implicitly imposed and compensate for it. Second, we employ a more sophisticated boosting algorithm based on conjugate gradients in function space. Initial experiments show performance gains over the basic functional gradient approach.},
  	Pages = {157--164},
  	Title = {Unbiased conjugate direction boosting for conditional random fields},
  	Url = {./papers/kersting2006unbiased.pdf},
  	Year = {2006},
  	Bdsk-Url-1 = {./papers/kersting2006unbiased.pdf}}

  @incollection{karwath2006relational,
  	Author = {Andreas Karwath and Kristian Kersting},
  	Booktitle = {Working Notes of the International Workshop on Mining and Learning with Graphs (MLG)},
  	Keywords = {Statistical Relational Learning, Complex Sequences, Alignment, Sequence Logos},
  	Note = {The need to measure sequence similarity arises in information extraction, music mining, biological sequence analysis, and other domains, and often coincides with sequence alignment: the more similar two sequences are, the better they can be aligned. Aligning sequences not only shows how similar sequences are, it also shows where there are differences and correspondences between the sequences. Traditionally, the alignment has been considered for sequences of flat symbols only. Many real world sequences such as protein secondary structures, however, exhibit a rich internal structures. This is akin to the problem of dealing with structured examples studied in the field of inductive logic programming (ILP). In this paper, we propose to use wellestablished ILP distance measures within alignment methods. Although straight-forward, our initial experimental results show that this approach performs well in practice and is worth to be explored.},
  	Title = {Relational Sequence Alignment},
  	Url = {./papers/karwath2006relational.pdf},
  	Year = {2006},
  	Bdsk-Url-1 = {./papers/karwath2006relational.pdf}}

  @book{kersting20016,
  	Anote = {./images/diss.jpg},
  	Author = {Kristian Kersting},
  	Keywords = {Statistical Relational Learning, Ph.D. Thesis, Inductive Logic Programming, Relational Fisher Kernels, Bayesian Logic Programs, Logical Hidden Markov Models},
  	Note = {This books addresses Probabilistic Inductive Logic Programming. The new area is closely tied to, though strictly subsumes, a new field known as `Statistical Relational Learning' which has in the last few years gained major prominence in the AI community. The book makes several contributions, including the introduction of a series of definitions which circumscribe the new area formed by extending Inductive Logic Programming to the case in which clauses are annotated with probability values. Also, it introduces Bayesian logic programs and investigates the approach of Learning from proofs and the issue of upgrading Fisher Kernels to Relational Fisher kernels.},
  	Publisher = {IOS Press},
  	Series = {Frontiers in Artificial Intelligence and Applications},
  	Title = {An Inductive Logic Programming Approach to Statistical Relational Learning},
  	Url = {http://www.iospress.nl/book/an-inductive-logic-programming-approach-to-statistical-relational-learning/},
  	Volume = {148},
  	Year = {2006},
    Key = {EurAI (formerly ECCAI) Dissertation Award 2006},
  	Bdsk-Url-1 = {http://www.iospress.nl/book/an-inductive-logic-programming-approach-to-statistical-relational-learning/}}

  @article{deraedt2006logical,
  	Anote = {./images/deraedt2006logical.png},
  	Author = {Luc De Raedt and Kristian Kersting and Tapani Raiko},
  	Journal = {Journal of Artificial Intelligence Research (JAIR)},
  	Keywords = {Statistical Relational Learning, Complex Sequences, Logical Hidden Markov Models},
  	Note = {Logical hidden Markov models (LOHMMs) upgrade traditional hidden Markov models to deal with sequences of structured symbols in the form of logical atoms, rather than flat characters. This note formally introduces LOHMMs and presents solutions to the three central inference problems for LOHMMs: evaluation, most likely hidden state sequence and parameter estimation. The resulting representation and algorithms are experimentally evaluated on problems from the domain of bioinformatics.},
  	Title = {Logical Hidden Markov Models},
  	Url = {http://www.jair.org/media/1675/live-1675-2623-jair.pdf},
  	Year = {2006},
  	Bdsk-Url-1 = {http://www.jair.org/media/1675/live-1675-2623-jair.pdf}}

  @inproceedings{kersting2005say,
  	Anote = {./images/kersting2012say.png},
  	Author = {Kristian Kersting and Tapani Raiko},
  	Booktitle = {Proceedings of the 21st Conference in Uncertainty in Artificial Intelligence (UAI)},
  	Keywords = {Statistical Relational Learning, Structural EM, Structure Learning, Complex Sequences},
  	Note = {Many real world sequences such as protein secondary structures or shell logs exhibit a rich internal structures. Traditional probabilistic models of sequences, however, consider sequences of flat symbols only. Logical hidden Markov models have been proposed as one solution. They deal with logical sequences, i.e., sequences over an alphabet of logical atoms. This comes at the expense of a more complex model selection problem. Indeed, different abstraction levels have to be explored. In this paper, we propose a novel method for selecting logical hidden Markov models from data called SAGEM. SAGEM combines generalized expectation maximization, which optimizes parameters, with structure search for model selection using inductive logic programming refinement operators. We provide convergence and experimental results that show SAGEM's effectiveness.},
  	Title = {'Say EM'for selecting probabilistic models for logical sequences},
  	Url = {https://arxiv.org/pdf/1207.1353.pdf},
  	Year = {2005},
  	Bdsk-Url-1 = {https://arxiv.org/pdf/1207.1353.pdf}}

  @inproceedings{deraedtKT05,
  	Anote = {./images/deraedtKT05.png},
  	Author = {Luc De Raedt and Kristian Kersting and Sunna Torge},
  	Booktitle = {Proceedings of the 20th National Conference on Artificial Intelligence (AAAI)},
  	Keywords = {Statistical Relational Learning, Stochastic Logic Programs, Structure Learning, Proof-Banks, Compression},
  	Note = {Stochastic logic programs combine ideas from probabilistic grammars with the expressive power of definite clause logic; as such they can be considered as an extension of probabilistic context-free grammars. Motivated by an analogy with learning tree-bank grammars, we study how to learn stochastic logic programs from proof-trees. Using proof-trees as examples imposes strong logical constraints on the structure of the target stochastic logic program. These constraints can be integrated in the least general generalization (lgg) operator, which is employed to traverse the search space. Our implementation employs a greedy search guided by the maximum likelihood principle and failure-adjusted maximization. We also report on a number of simple experiments that show the promise of the approach.},
  	Pages = {752--757},
  	Title = {Towards Learning Stochastic Logic Programs from Proof-Banks},
  	Url = {http://www.aaai.org/Papers/AAAI/2005/AAAI05-118.pdf},
  	Year = {2005},
  	Bdsk-Url-1 = {http://www.aaai.org/Papers/AAAI/2005/AAAI05-118.pdf}}

  @incollection{gurel2005trade,
  	Author = {Tayfun G√ºrel and Kristian Kersting},
  	Booktitle = {Working Notes of the 3rd International ECML/PKDD Workshop on Mining Graphs, Trees, and Sequences},
  	Keywords = {Collective Classification, Iterative Classification},
  	Note = {There have been two major approaches for classification of networked (linked) data. Local approaches (iterative classification) learn a model locally without considering unlabeled data and apply the model iteratively to classify unlabeled data. Global approaches (collective classification), on the other hand, exploit unlabeled data and the links occurring between labeled and unlabeled data for learning. Naturally, global approaches are computationally more demanding than local ones. Moreover, for large data sets, approximate inference has to be performed to make computations feasible. In the present work, we investigate the benefits of collective classification based on global probabilistic models over local approaches. Our experimental results show that global approaches do not always outperform local approaches with respect to the classification accuracy. More precisely, the results suggest that global approaches considerably outperform local approaches only for low ratios of labeled data.},
  	Title = {On the trade-off between iterative classification and collective classification: First experimental results},
  	Url = {./papers/gurel2005trade.pdf},
  	Year = {2005},
  	Bdsk-Url-1 = {./papers/gurel2005trade.pdf}}

  @inproceedings{dereadt2004probabilistic,
  	Author = {Luc {De Raedt} and Kristian Kersting},
  	Booktitle = {International Conference on Algorithmic Learning Theory (ALT)},
  	Keywords = {Statistical Relational Learning, Inductive Logic Programming, Probabilistic Inductive Logic Programming},
  	Note = {Probabilistic inductive logic programming, sometimes also called statistical relational learning, addresses one of the central questions of artificial intelligence: the integration of probabilistic reasoning with first order logic representations and machine learning. A rich variety of different formalisms and learning techniques have been developed. In the present paper, we start from inductive logic programming and sketch how it can be extended with probabilistic methods. More precisely, we outline three classical settings for inductive logic programming, namely learning from entailment, learning from interpretations, and learning from proofs or traces, and show how they can be used to learn different types of probabilistic representations.},
  	Organization = {Springer},
  	Pages = {19--36},
  	Title = {Probabilistic inductive logic programming},
  	Url = {./papers/dereadt2004probabilistic.pdf},
  	Year = {2004},
  	Bdsk-Url-1 = {./papers/dereadt2004probabilistic.pdf}}

  @incollection{kersting2004logical,
  	Anote = {./images/kersting2004logical.png},
  	Author = {Kristian Kersting and Luc De Raedt},
  	Booktitle = {Proceedings of the 14th International Conference on Inductive Logic Programming (ILP)},
  	Keywords = {Statistical Relational Learning, Relational Reinforcement Learning, Temporal Difference Learning, Optimal Actions},
  	Note = {Recent developments in the area of relational reinforcement learning (RRL) have resulted in a number of new algorithms. A theory, however, that explains why RRL works, seems to be lacking. In this paper, we provide some initial results on a theory of RRL. To realize this, we introduce a novel representation formalism, called logical Markov decision programs (LOMDPs), that integrates Markov Decision Processes (MDPs) with Logic Programs. Using LOMDPs one can compactly and declaratively represent complex MDPs. Within this framework we then devise a relational upgrade of TD(Œª) called logical TD(Œª) and prove convergence. Experiments validate our approach.},
  	Organization = {Springer},
  	Pages = {180--197},
  	Title = {Logical Markov decision programs and the convergence of logical TD(Œª)},
  	Url = {./papers/kersting2004logical.pdf},
  	Year = {2004},
  	Bdsk-Url-1 = {./papers/kersting2004logical.pdf}}

  @inproceedings{kersting2004fisher,
  	Anote = {./images/kersting2004fisher.png},
  	Author = {Kristian Kersting and Thomas G{\"a}rtner},
  	Booktitle = {Proceedings of the European Conference on Machine Learning (ECML)},
  	Keywords = {Statistical Relational Learning, Bayesian Logic Programs, Fisher Kernels},
  	Note = {One approach to improve the accuracy of classifications based on generative models is to combine them with successful discriminative algorithms. Fisher kernels were developed to combine generative models with a currently very popular class of learning algorithms, kernel methods. Empirically, the combination of hidden Markov models with support vector machines has shown promising results. So far, however, Fisher kernels have only been considered for sequences over flat alphabets. This is mostly due to the lack of a method for computing the gradient of a generative model over structured sequences. In this paper, we show how to compute the gradient of logical hidden Markov models, which allow for the modelling of logical sequences, i.e., sequences over an alphabet of logical atoms. Experiments show a considerable improvement over results achieved without Fisher kernels for logical sequences.},
  	Organization = {Springer},
  	Pages = {205--216},
  	Title = {Fisher kernels for logical sequences},
  	Url = {./papers/kersting2004fisher.pdf},
  	Year = {2004},
  	Bdsk-Url-1 = {./papers/kersting2004fisher.pdf}}

  @inproceedings{kersting2004balios,
  	Anote = {./images/kersting2004balios.png},
  	Author = {Kristian Kersting and Uwe Dick},
  	Booktitle = {Proceedings of the 8th European Conference on Principles and Practice of Knowledge Discovery in Databases (PKDD)},
  	Keywords = {Statistical Relational Learning, Bayesian Logic Programms, Graphical Represenation},
  	Note = {Balios is an inference engine for Bayesian logic programs (BLPs). BLPs combine BNs with definite clause logic. The basic idea is to view logical atoms as sets of random variables which are similar to each other.},
  	Organization = {Springer},
  	Pages = {549--551},
  	Title = {Balios--the engine for Bayesian logic programs},
  	Url = {./papers/kersting2004balios.pdf},
  	Year = {2004},
  	Bdsk-Url-1 = {./papers/kersting2004balios.pdf}}

  @inproceedings{kerstingOR04,
  	Anote = {./images/kerstingOR04.png},
  	Author = {Kristian Kersting and Martijn {van Otterlo} and Luc {De Raedt}},
  	Booktitle = {Proceedings of the Twenty-first International Conference on Machine Learning (ICML)},
  	Keywords = {Statistical Relational AI, Statistical Relational Learning, Optimal Actions, Relational Markov Decision Processes, Value Iteration},
  	Note = {Motivated by the interest in relational reinforcement learning, we introduce a novel relational Bellman update operator called ReBel. It employs a constraint logic programming language to compactly represent Markov decision processes over relational domains. Using ReBel, a novel value iteration algorithm is developed in which abstraction (over states and actions) plays a major role. This framework provides new insights into relational reinforcement learning. Convergence results as well as experiments are presented.},
  	Title = {Bellman goes relational},
  	Url = {./papers/kerstingOR04.pdf},
  	Year = {2004},
  	Bdsk-Url-1 = {./papers/kerstingOR04.pdf}}

  @incollection{otterlo2004challenges,
  	Author = {Martijn Otterlo and Kristian Kersting},
  	Booktitle = {Working Notes of the ICML Workshop on Relational Reinforcement Learning (RRL)},
  	Keywords = {Statistical Relational Learning, Relational Reinforcment Learning},
  	Note = {We present a perspective and challenges for Relational Reinforcement Learning (RRL). We first survey existing work and distinguish a number of main directions. We then highlight some research problems that are intrinsically involved in abstracting over relational Markov Decision Processes. These are the challenges of RRL. In addition, we describe a number of issues that will be important for further research into RRL. These are the challenges for RRL and deal with newly arising issues because of relational abstraction.},
  	Pages = {74--80},
  	Title = {Challenges for relational reinforcement learning},
  	Url = {./papers/otterlo2004challenges.pdf},
  	Year = {2004},
  	Bdsk-Url-1 = {./papers/otterlo2004challenges.pdf}}

  @inbook{kersting2004scaled,
  	Author = {Kristian Kersting and Niels Landwehr},
  	Booktitle = {Advances in Bayesian Networks},
  	Keywords = {Graphical Models, Parameter Estimation, Scaled Conjugate Gradient},
  	Note = {To learn Bayesian networks, one must estimate the parameters of the network from the data. EM (Expectation-Maximization) and gradient-based algorithms are the two best known techniques to estimate these parameters. Although the theoretical properties of these two frameworks are well-studied, it remains an open question as to when and whether EM is to be preferred over gradients. We will answer this question empirically. More specifically, we first adapt scaled conjugate gradients well-known from neural network learning. This accelerated conjugate gradient avoids the time consuming line search of more traditional methods. Secondly, we empirically compare scaled conjugate gradients with EM. The experiments show that accelerated conjugate gradients are competitive with EM. Although, in general EM is the domain independent method of choice, gradient-based methods can be superior.},
  	Pages = {235--254},
  	Publisher = {Springer},
  	Title = {Scaled conjugate gradients for maximum likelihood: An empirical comparison with the EM algorithm},
  	Url = {./papers/kersting2004scaled.pdf},
  	Year = {2004},
  	Bdsk-Url-1 = {./papers/kersting2004scaled.pdf}}

  @inproceedings{kersting2003towards,
  	Author = {Kristian Kersting and Tapani Raiko and Stefan Kramer and Luc De Raedt},
  	Booktitle = {Proceedings of the Pacific Symposium on Biocomputing (PSB)},
  	Keywords = {Statistical Relational Learning, Protein Folding, Logical Hidden Markov Models, Complex Sequences},
  	Note = {With the growing number of determined protein structures and the availability of classification schemes, it becomes increasingly important to develop computer methods that automatically extract structural signatures for classes of proteins. In this paper, we introduce and apply a new Machine Learning technique, Logical Hidden Markov Models (LOHMMs), to the task of finding structural signatures of folds according to the classification scheme SCOP. Our results indicate that LOHMMs are applicable to this task and possess several advantages over other approaches.},
  	Pages = {192--203},
  	Title = {Towards discovering structural signatures of protein folds based on logical hidden Markov models},
  	Url = {./papers/kersting2003towards.pdf},
  	Volume = {8},
  	Year = {2003},
  	Bdsk-Url-1 = {./papers/kersting2003towards.pdf}}

  @inproceedings{kersting2003logical,
  	Author = {Kristian Kersting and Luc De Raedt},
  	Booktitle = {Proceedings of the IJCAI Workshop on Learning Statistical Models of Relational Data (SRL)},
  	Keywords = {Statistical Relational Learning, Complex Decision, Markov Decision Processes, Optimal Actions},
  	Note = {Motivated by the interest in relational reinforcement learning, we introduce a novel representation formalism, called logical Markov decision programs (LOMDPs), that integrates Markov Decision Processes with Logic Programs. Using LOMDPs one can compactly and declaratively represent complex relational Markov decision processes. Within this framework we then develop a theory of reinforcement learning in which abstraction (of states and actions) plays a major role. The framework presented should provide a basis for further developments in relational reinforcement learning.},
  	Pages = {63--70},
  	Title = {Logical markov decision programs},
  	Url = {./papers/kersting2003logical.pdf},
  	Year = {2003},
  	Bdsk-Url-1 = {./papers/kersting2003logical.pdf}}

  @inproceedings{fischer2003scaled,
  	Author = {J√∂rg Fischer and Kristian Kersting},
  	Booktitle = {Proceedings of the European Conference on Machine Learning (ECML)},
  	Keywords = {Graphical Models, Parameter Estimation, Expectation-Maximization, Scaled Conjugate EM},
  	Note = {The EM algorithm is a popular method for maximum likelihood estimation of Bayesian networks in the presence of missing data. Its simplicity and general convergence properties make it very attractive. However, it sometimes converges slowly. Several accelerated EM methods based on gradient-based optimization techniques have been proposed. In principle, they all employ a line search involving several NP-hard likelihood evaluations. We propose a novel acceleration called SCGEM based on scaled conjugate gradients (SCGs) well-known from learning neural networks. SCGEM avoids the line search by adopting the scaling mechanism of SCGs applied to the expected information matrix. This guarantees a single likelihood evaluation per iteration. We empirically compare SCGEM with EM and conventional conjugate gradient accelerated EM. The experiments show that SCGEM can significantly accelerate both of them and is equal in quality.},
  	Organization = {Springer},
  	Pages = {133--144},
  	Title = {Scaled CGEM: A fast accelerated EM},
  	Url = {./papers/fischer2003scaled.pdf},
  	Year = {2003},
  	Bdsk-Url-1 = {./papers/fischer2003scaled.pdf}}

  @incollection{kersting2003representational,
  	Author = {Kristian Kersting},
  	Booktitle = {In IJCAI Workshop on Learning Statistical Models from Relational Data (SRL)},
  	Keywords = {Statistical Relational Learning, Representional Power},
  	Note = {There is a diversity of probabilistic-logical models (PLM). No clear understanding of the relative advantages and limitations of different formalisms and their language concepts has yet emerged. To overcome this, we propose to downgrade highly expressive PLMs. This method has several advantages: one can profit from existing research on PLMs and inherit unique semantics, and inference and learning algorithms. Moreover, there is a clear relationship between the new PLM and its more expressive counterpart. No single existing approach is devalued.},
  	Title = {Representational power of probabilistic-logical models: From upgrading to downgrading},
  	Url = {./papers/kersting2003representational.pdf},
  	Year = {2003},
  	Bdsk-Url-1 = {./papers/kersting2003representational.pdf}}

  @incollection{kersting2003structural,
  	Author = {Kristian Kersting and Tapani Raiko and Luc {De Raedt}},
  	Booktitle = {Working Notes of the Second KDD Workshop on Multi-Relational Data Mining (MRDM)},
  	Keywords = {Complex Sequences, Statistical Relational Learning, Hidden markov Models, Structural EM},
  	Note = {The compactness of Logical HMMS (LOHHMS) comes at the expense of a more complex structure learning problem. We proposed to adapt Driedman's structural EM. The method combines generalized expectation maximization for parameter estimation with structure search for model selection using inductiv logic programming.},
  	Title = {A structural GEM for learning logical hidden markov models},
  	Url = {./papers/kersting2003structural.pdf},
  	Year = {2003},
  	Bdsk-Url-1 = {./papers/kersting2003structural.pdf}}

  @article{deraedt2003probabilistic,
  	Anote = {./images/deraedt2003probabilistic.png},
  	Author = {Luc {De Raedt} and Kristian Kersting},
  	Journal = {ACM SIGKDD Explorations Newsletter},
  	Keywords = {Overview, Introduction, Statistical Relational Learning, Inductive Logic Programming},
  	Note = {The past few years have witnessed an significant interest in probabilistic logic learning, i.e. in research lying at the intersection of probabilistic reasoning, logical representations, and machine learning. A rich variety of different formalisms and learning techniques have been developed. This paper provides an introductory survey and overview of the stateof-the-art in probabilistic logic learning through the identi- fication of a number of important probabilistic, logical and learning concepts.},
  	Number = {1},
  	Pages = {31--48},
  	Publisher = {ACM},
  	Title = {Probabilistic logic learning},
  	Url = {./papers/deraedt2003probabilistic.pdf},
  	Volume = {5},
  	Year = {2003},
  	Bdsk-Url-1 = {./papers/deraedt2003probabilistic.pdf}}

  @inproceedings{raiko2002bayesian,
  	Author = {Tapani Raiko and Kristian Kersting and Juha Karhunen and Luc {De Raedt}},
  	Booktitle = {Proceedings of the Finnish Artificial Intelligence Conference (STeP)},
  	Keywords = {Statistical Relational Learning, Logical Hidden Markov Models, Priors, Complex Sequences},
  	Note = {Logial hidden Markov models (LOHMMs) are a generalisation of hidden Markov models to analyze sequenes of logial atoms. Transitions are fatorized into two steps, seleting an atom and instantiating the variables. Unification is used to share information among states, and between states and observations. In this paper, we show how LOHMMs an be learned using Bayesian methods. Some estimators are ompared and parameter estimation is tested with syntheti data.},
  	Pages = {64--71},
  	Title = {Bayesian learning of logical hidden markov models},
  	Url = {./papers/raiko2002bayesian.pdf},
  	Year = {2002},
  	Bdsk-Url-1 = {./papers/raiko2002bayesian.pdf}}

  @incollection{kersting2002scaled,
  	Author = {Kristian Kersting and Niels Landwehr},
  	Booktitle = {Proceedings of the First European Workshop on Probabilistic Graphical Models (PGM)},
  	Keywords = {Graphical Models, Parameter Estimation, Scaled Conjugate Gradient},
  	Note = {To learn Bayesian networks, one must estimate the parameters of the network from the data. EM (Expectation-Maximization) and gradient-based algorithms are the two best known techniques to estimate these parameters. Although the theoretical properties of these two frameworks are well-studied, it remains an open question as to when and whether EM is to be preferred over gradients. We will answer this question empirically. More specifically, we first adapt scaled conjugate gradients well-known from neural network learning. This accelerated conjugate gradient avoids the time consuming line search of more traditional methods. Secondly, we empirically compare scaled conjugate gradients with EM. The experiments show that accelerated conjugate gradients are competitive with EM. Although, in general EM is the domain independent method of choice, gradient-based methods can be superior.},
  	Title = {Scaled Conjugate Gradients for Maximum Likelihood: An Empirical Comparison with the {EM} Algorithm},
  	Url = {./papers/kersting2002scaled.pdf},
  	Year = {2002},
  	Bdsk-Url-1 = {./papers/kersting2002scaled.pdf}}

  @incollection{kersting2002fisher,
  	Author = {Kristian Kersting and Thomas G{\"a}rtner},
  	Booktitle = {Working Notes of the NIPS Workshop on Machine Learning Techniques for Bioinformatics},
  	Keywords = {Statistical Relational learning, Complex Sequences, Fisher Kernels},
  	Note = {The present work investigates whether the predictive accuracy of LOHMMs can be im- proved using Fisher kernels and support vector machines. For that, we devise a method to compute the gradient of the log likelihood with respect to the parameters of a LOHMM.},
  	Title = {Fisher Kernels and Logical Sequences with an Application to Protein Fold Recognition},
  	Url = {./papers/kersting2002fisher.pdf},
  	Year = {2002},
  	Bdsk-Url-1 = {./papers/kersting2002fisher.pdf}}

  @incollection{kersting2002lohmms,
  	Author = {Kristian Kersting and Tapani Raiko and Luc {De Raedt}},
  	Booktitle = {Proceedings of the First European Workshop on Graphical Models (PGM)},
  	Keywords = {Statistical Relational AI, Complex Sequences, Logical Hidden Markov Models},
  	Note = {Logical hidden Markov models (LOHMMS) are a generalization of hidden Markov models (HMMs) to analyze sequences of logical atoms. In LOHMMs, abstract states summarize aets of states and are represented by logical atoms. Transitions are defined between astract states to saummarize sets of transitions between states. Unification is used to share information among states, and betweens tates and observations.},
  	Pages = {99-107},
  	Title = {Logical Hidden Markov Models (Extended Abstract)},
  	Url = {./papers/kersting2002lohmms.pdf},
  	Year = {2002},
  	Bdsk-Url-1 = {./papers/kersting2002lohmms.pdf}}

  @article{ganzert2002analysis,
  	Anote = {./images/ganzert2002analysis.png},
  	Author = {Steven Ganzert and Josef Guttmann and Kristian Kersting and Ralf Kuhlen and Christian Putensen and Michael Sydow and Stefan Kramer},
  	Journal = {Artificial Intelligence in Medicine},
  	Keywords = {Intensive Care Medicine, Process Induction, Differential Equations},
  	Note = {We present a case study of machine learning and data mining in intensive care medicine. In the study, we compared different methods of measuring pressure--volume curves in artificially ventilated patients suffering from the adult respiratory distress syndrome (ARDS). Our aim was to show that inductive machine learning can be used to gain insights into differences and similarities among these methods. We defined two tasks: the first one was to recognize the measurement method producing a given pressure--volume curve. This was defined as the task of classifying pressure--volume curves (the classes being the measurement methods). The second was to model the curves themselves, that is, to predict the volume given the pressure, the measurement method and the patient data. Clearly, this can be defined as a regression task. For these two tasks, we applied C5.0 and CUBIST, two inductive machine learning tools, respectively. Apart from medical findings regarding the characteristics of the measurement methods, we found some evidence showing the value of an abstract representation for classifying curves: normalization and high-level descriptors from curve fitting played a crucial role in obtaining reasonably accurate models. Another useful feature of algorithms for inductive machine learning is the possibility of incorporating background knowledge. In our study, the incorporation of patient data helped to improve regression results dramatically, which might open the door for the individual respiratory treatment of patients in the future.},
  	Number = {1},
  	Pages = {69--86},
  	Publisher = {Elsevier},
  	Title = {Analysis of respiratory pressure--volume curves in intensive care medicine using inductive machine learning},
  	Url = {./papers/ganzert2002analysis.pdf},
  	Volume = {26},
  	Year = {2002},
  	Bdsk-Url-1 = {./papers/ganzert2002analysis.pdf}}

  @misc{kersting2001blps,
  	Author = {Kristian Kersting and Luc {De Raedt}},
  	Howpublished = {CoRR},
  	Keywords = {Statistical Relational Learning, Bayesian Logic Programs, Bayesian Networks, Logic Programming},
  	Note = {Bayesian networks provide an elegant formalism for representing and reasoning about uncertainty using probability theory. They are a probabilistic extension of propositional logic and, hence, inherit some of the limitations of propositional logic, such as the difficulties to represent objects and relations. We introduce a generalization of Bayesian networks, called Bayesian logic programs, to overcome these limitations. In order to represent objects and relations it combines Bayesian networks with definite clause logic by establishing a one-to-one mapping between ground atoms and random variables. We show that Bayesian logic programs combine the advantages of both definite clause logic and Bayesian networks. This includes the separation of quantitative and qualitative aspects of the model. Furthermore, Bayesian logic programs generalize both Bayesian networks as well as logic programs. So, many ideas developed in both areas carry over.},
  	Title = {Bayesian Logic Programs},
  	Url = {https://arxiv.org/pdf/cs/0111058.pdf},
  	Volume = {cs.AI/0111058},
  	Year = {2001},
  	Bdsk-Url-1 = {https://arxiv.org/pdf/cs/0111058.pdf}}

  @inproceedings{kerstingR01,
  	Author = {Kristian Kersting and Luc {De Raedt}},
  	Booktitle = {Proceedings of the 11th International Conference on Inductive Logic Programming (ILP)},
  	Keywords = {Statistical Relational Learning, Bayesian Logic Programs, Bayesian Networks, Logic Programming},
  	Note = {First order probabilistic logics combine a first order logic with a probabilistic knowledge representation. In this context, we introduce continuous Bayesian logic programs, which extend the recently introduced Bayesian logic programs to deal with continuous random variables. Bayesian logic programs tightly integrate definite logic programs with Bayesian networks. The resulting framework nicely seperates the qualitative (i.e. logical) component from the quantitative (i.e. the probabilistic) one. We also show how the quantitative component can be learned using a gradient-based maximum likelihood method.},
  	Pages = {104--117},
  	Title = {Adaptive Bayesian Logic Programs},
  	Url = {./papers/kerstingR01.pdf},
  	Year = {2001},
  	Bdsk-Url-1 = {./papers/kerstingR01.pdf}}

  @inproceedings{kersting2001towards,
  	Author = {Kristian Kersting and Luc De Raedt},
  	Booktitle = {Proceedings of the 11th International Conference on Inductive Logic Programming (ILP)},
  	Keywords = {Statistical Relational Learning, Bayesian Logic Programs, Bayesian Networks, Logic Programming, Inductive Logic Programming, Structure Learning},
  	Note = {Recently, new representation languages that integrate first order logic with Bayesian networks have been developed. Bayesian logic programs are one of these languages. In this paper, we present results on combining Inductive Logic Programming (ILP) with Bayesian networks to learn both the qualitative and the quantitative components of Bayesian logic programs. More precisely, we show how to combine the ILP setting learning from interpretations with score-based techniques for learning Bayesian networks. Thus, the paper positively answers Koller and Pfeffer's question, whether techniques from ILP could help to learn the logical component of first order probabilistic models.},
  	Organization = {Springer},
  	Pages = {118--131},
  	Title = {Towards combining inductive logic programming with Bayesian networks},
  	Url = {./papers/kersting2001towards.pdf},
  	Year = {2001},
  	Bdsk-Url-1 = {./papers/kersting2001towards.pdf}}

  @incollection{kersting2000interpreting,
  	Author = {Kristian Kersting and Luc De Raedt and Stefan Kramer},
  	Booktitle = {Working Notes of the AAAI workshop on learning statistical models from relational data (SRL)},
  	Keywords = {Statistical Relational Learning, Bayesian Logic Programs, Bayesian Networks, Logic Programming},
  	Note = {Various proposals for combining first order logic with Bayesian nets exist. We introduce the formalism of Bayesian logic programs, which is basically a simplification and reformulation of Ngo and Haddawys probabilistic logic programs. However, Bayesian logic programs are sufficiently powerful to represent essentially the same knowledge in a more elegant manner. The elegance is illustrated by the fact that they can represent both Bayesian nets and definite clause programs (as in pure Prolog) and that their kernel in Prolog is actually an adaptation of an usual Prolog meta-interpreter.},
  	Pages = {29--35},
  	Title = {Interpreting Bayesian logic programs},
  	Url = {./papers/kersting2000interpreting.pdf},
  	Year = {2000},
  	Bdsk-Url-1 = {./papers/kersting2000interpreting.pdf}}
