
@inproceedings{delfosse2024raRL,
    booktitle = {Proceedings of the International Conference on Representation Learning (ICLR) },
      title={Adaptive Rational Activations to Boost Deep Reinforcement Learning},
      author={Quentin Delfosse and Patrick Schramowski and Martin Mundt and Alejandro Molina and Kristian Kersting},
      year={2024},
      Keywords={Neural Plasticity, Deep Reinforcement Learning, Rational Activations},
      Anote={./images/delfosse2024ratRL.png},
      Note={Latest insights from biology show that intelligence not only emerges from the connections between neurons, but that individual neurons shoulder more computational responsibility than previously anticipated. Specifically, neural plasticity should be critical in the context of constantly changing reinforcement learning (RL) environments, yet current approaches still primarily employ static activation functions. In this work, we motivate the use of adaptable activation functions in RL and show that rational activation functions are particularly suitable for augmenting plasticity. Inspired by residual networks, we derive a condition under which rational units are closed under residual connections and formulate a naturally regularised version. The proposed joint-rational activation allows for desirable degrees of flexibility, yet regularises plasticity to an extent that avoids overfitting by leveraging a mutual set of activation function parameters across layers. We demonstrate that equipping popular algorithms with (joint) rational activations leads to consistent improvements on different games from the Atari Learning Environment benchmark, notably making DQN competitive to DDQN and Rainbow.},
      Url={https://openreview.net/pdf?id=g90ysX1sVs}
}

@inproceedings{struppek2024iclr,
    booktitle = {Proceedings of the International Conference on Representation Learning (ICLR) },
      title={Be Careful What You Smooth For: Label Smoothing Can Be a Privacy Shield but Also a Catalyst for Model Inversion Attacks},
      author={Lukas Struppek and Dominik Hintersdorf and Kristian Kersting},
      year={2024},
      Keywords={Label Smoothing, Privacy, Membership Inference Attack, Defence},
      Anote={./images/struppek2024iclr.png},
      Note={Label smoothing – using softened labels instead of hard ones – is a widely adopted regularization method for deep learning, showing diverse benefits such as enhanced generalization and calibration. Its implications for preserving model privacy, however, have remained unexplored. To fill this gap, we investigate the impact of label smoothing on model inversion attacks (MIAs), which aim to generate class-representative samples by exploiting the knowledge encoded in a classifier, thereby inferring sensitive information about its training data. Through extensive analyses, we uncover that traditional label smoothing fosters MIAs, thereby increasing a model's privacy leakage. Even more, we reveal that smoothing with negative factors counters this trend, impeding the extraction of class-related information and leading to privacy preservation, beating state-of-the-art defenses. This establishes a practical and powerful novel way for enhancing model resilience against MIAs.},
      Url={https://openreview.net/pdf?id=1SbkubNdbW}
}

@inproceedings{seng2024iclr,
    booktitle = {Proceedings of the International Conference on Representation Learning (ICLR) },
      title={Learning Large DAGs is Harder than you Think: Many Losses are Minimal for the Wrong DAG},
      author={Jonas Seng and Matej Zečević and Devendra Singh Dhami and Kristian Kersting},
      year={2024},
      Keywords={Structure Learning, DAG, Differentiable, Square-based Losses, Scale},
      Anote={./images/sseng2024iclr.png},
      Note={Structure learning is a crucial task in science, especially in fields such as medicine and biology, where the wrong identification of (in)dependencies among random variables can have significant implications. The primary objective of structure learning is to learn a Directed Acyclic Graph (DAG) that represents the underlying probability distribution of the data. Many prominent DAG learners rely on least square losses or log-likelihood losses for optimization. It is well-known from regression models that least square losses are heavily influenced by the scale of the variables. Recently it has been demonstrated that the scale of data also affects performance of structure learning algorithms, though with a strong focus on linear 2-node systems and simulated data. Moving beyond these results, we provide conditions under which square-based losses are minimal for wrong DAGs in
-dimensional cases. Furthermore, we also show that scale can impair performance of structure learners if relations among variables are non-linear for both square based and log-likelihood based losses. We confirm our theoretical findings through extensive experiments on synthetic and real-world data.},
      Url={https://openreview.net/pdf?id=gwbQ2YwLhD}
}



@article{thomas2024decisions,
  Anote = {./images/thomas2023decisions.png},
  title = {Modeling dataset bias in machine-learned theories of economic decision making},
  author={Tobias Thomas and Dominik Straub and Fabian Tatai and Megan Shene and Tümer Tosik and Kristian Kersting and Constantin Rothkopf},
  Journal = {Nature Human Behaviour},
  Note = {Normative and descriptive models have long vied for explaining and predicting human risky choices, such as those between goods or gambles. A recent study (Peterson et al., 2021, Science) reports the discovery of a new, more accurate model of human decision-making by training neural networks on a new online large-scale dataset, choices13k. Here, we systematically analyze the relationships between several models and datasets using machine learning methods and find evidence for dataset bias. Because participants’ choices in stochastically dominated gambles were consistently skewed towards equipreference in the choices13k dataset, we hypothesized that this reflected increased decision noise. Indeed, a probabilistic generative model adding structured decision noise to a neural network trained on data from a laboratory study transferred best, i.e. outperformed all models apart from those trained on choices13k. We conclude that a careful combination of theory and data analysis is still required to understand the complex interactions of machine learning models and data of human risky choices.},
  Keywords = {choices13k, economic decisions, deep learning, data-driven, no free lunch, model-driven, computational cognitive science, no end of theory},
  Publisher = {Nature Publishing Group},
  year={2024},
  volume={},
  pages={},
  issn={},
  doi={},
  url={}
}

@article{zecevic2024acml,
Anote={./images/zecevic2023acml.png},
author={Matej Zecevic and Devendra Singh Dhami and Kristian Kersting},
note = {The recent years have been marked by extended research on adversarial attacks, especially on deep neural networks. With this work we intend on posing and investigating the question of whether the phenomenon might be more general in nature, that is, adversarial-style attacks outside classical classification tasks. Specifically, we investigate optimization problems as they constitute a fundamental part of modern AI research. To this end, we consider the base class of optimizers namely Linear Programs (LPs). On our initial attempt of a naïve mapping between the formalism of adversarial examples and LPs, we quickly identify the key ingredients missing for making sense of a reasonable notion of adversarial examples for LPs. Intriguingly, the formalism of Pearl's notion to causality allows for the right description of adversarial like examples for LPs. Characteristically, we show the direct influence of the Structural Causal Model (SCM) onto the subsequent LP optimization, which ultimately exposes a notion of confounding in LPs (inherited by said SCM) that allows for adversarial-style attacks. We provide both the general proof formally alongside existential proofs of such intriguing LP-parameterizations based on SCM for three combinatorial problems, namely Linear Assignmen
